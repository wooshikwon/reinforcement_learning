# GCB6206 과제 1: 모방 학습 (Imitation Learning)

**[원시욱 (WOOSHIK WON)]**
**[2025961226]**

---

## 1. 서론 (Introduction)

이 과제의 목표는 다음 세 가지에 익숙해지도록 하는 것입니다: (1) 연세 AI 교육 클러스터 (VESSL AI), (2) 강화학습 환경, 그리고 (3) 행동 복제(Behavioral Cloning, BC)와 DAgger를 포함한 모방 학습 알고리즘입니다. 여기 Overleaf의 과제 보고서 템플릿 링크가 있습니다.

---

## 2. (선택사항) LaTeX

LaTeX에 익숙하지 않다면, Overleaf 튜토리얼 "Learn LaTeX in 30 minutes"를 읽어보는 것을 강력히 권장합니다. 더 자세한 내용은 Overleaf의 문서를 참조하세요.

이 과제 과제를 위해 다음과 같이 Overleaf를 사용해야 합니다:

1. Overleaf 계정을 생성합니다.
2. 왼쪽 상단의 "Menu"를 선택하고 "Copy Project"를 클릭하여 main.tex 파일을 수정할 수 있도록 합니다.
3. `\author{...}`의 플레이스홀더를 이름과 학번으로 바꿉니다 (37번째 줄과 38번째 줄).
4. 변경 후 "Recompile" 버튼을 눌러 오른쪽 PDF 패널에서 업데이트를 확인합니다.
5. 제목 아래에 이름과 학번이 제대로 표시되는지 확인합니다.
6. main.tex를 읽어보고 실험해보세요!

---

## 3. (선택사항) VESSL AI

자신의 GPU를 가지고 있거나 Google Colab을 사용하고 싶다면 이 섹션을 건너뛸 수 있습니다.

이 과제를 위해 GPU가 필요하다면, 연세 AI 학과의 강의용 GPU 클러스터인 VESSL AI (AI-LEC-1 그룹)를 제공합니다. 월 300시간 동안 RTX3090 GPU를 사용할 수 있습니다. 이 과제를 위한 워크스페이스(즉, GPU가 할당된 가상 머신)를 시작하고 사용하는 방법에 대한 지침이 여기 있습니다.

**경고:**
- 컴퓨팅 리소스를 절약하기 위해 사용하지 않을 때는 항상 워크스페이스를 중지하세요.
- VESSL 워크스페이스는 최대 72시간의 런타임을 제공합니다. 이 제한을 초과하면 워크스페이스가 자동으로 중지됩니다. 사용 가능한 시간을 관리하기 위해 정기적으로 사용량을 모니터링하고, 자동으로 중지된 경우 워크스페이스를 다시 시작하세요.

---

## 4. 강화학습 환경 실습

강화학습(RL) 환경에 익숙해지도록 돕기 위해 과제 자료 내에 튜토리얼(`GymTutorial.ipynb`)을 제공합니다. gym 환경과 상호작용하는 방법을 이해하기 위해 이 튜토리얼을 검토하고 실행하는 것을 권장합니다.

튜토리얼을 완료한 후, gym 환경의 기본 원리에 대한 이해를 보여주기 위해 다음 질문에 답하세요.

### 4.1 질문:

**1. `step()` 함수의 역할을 설명하세요. 어떤 종류의 정보를 반환하나요?**

**답변:** `step()` 함수는 한 타임스텝을 실행하고 다섯 개의 값을 반환합니다: observation (다음 상태), reward (이 전환에 대한 스칼라 값), terminated (로봇이 넘어지는 것처럼 에피소드가 자연스럽게 종료되면 true), truncated (최대 타임스텝 제한에 도달하면 true), 그리고 info (진단 정보가 담긴 딕셔너리)입니다.

**2. gym 환경에서 `reset()` 함수의 역할을 설명하세요. 이 함수의 반환값은 무엇인가요?**

**답변:** `reset()` 함수는 새로운 에피소드를 위해 환경을 시작 상태로 초기화합니다. 두 가지를 반환합니다: observation (에이전트가 시작하는 초기 상태)과 info (초기 상태에 대한 보조 정보가 담긴 딕셔너리—실제로 구현에서는 사용하지 않지만).

**3. 어떤 gym 환경이 이산 행동 공간(discrete action space)을 가지는지 연속 행동 공간(continuous action space)을 가지는지 어떻게 알 수 있나요?**

**답변:** `env.action_space`의 타입을 확인하면 됩니다. `gym.spaces.Discrete`라면 환경은 이산 행동 공간을 가지며 행동의 개수는 `env.action_space.n`입니다. `gym.spaces.Box`라면 환경은 연속 행동 공간을 가지며 행동 차원은 `env.action_space.shape[0]`이고, 행동은 NumPy float 배열입니다.

---

## 5. 행동 복제 (Behavioral Cloning)

### 5.1 코드 구조 개요

#### 5.1.1 메인 스크립트: `run_hw1.py`

- 행동 복제 실험을 실행하기 위한 진입점입니다.
- 명령줄 인자를 파싱하고 실험 파라미터를 설정합니다.
- 지정된 파일에서 전문가 정책을 로드합니다.
- 학습을 위해 BCTrainer를 초기화하고 실행합니다.
- 로깅 및 실험 출력 저장을 처리합니다.

#### 5.1.2 트레이너: `bc_trainer.py`

- 학습 루프, 데이터 수집 및 로깅을 관리하는 BCTrainer를 정의합니다.
- 주요 메서드:
  - `run_training_loop()`: 여러 학습 반복을 실행합니다.
  - `collect_training_trajectories()`: 현재 정책을 사용하여 데이터를 수집합니다.
  - `train_agent()`: 샘플링된 궤적을 사용하여 에이전트를 업데이트합니다.
  - `do_relabel_with_expert()`: 전문가 정책을 사용하여 행동을 수정합니다 (DAgger용).
  - `perform_logging()`: 학습 및 평가 통계를 로깅합니다.

#### 5.1.3 행동 복제 에이전트: `bc_agent.py`

- 행동 복제를 통해 학습된 에이전트를 나타내는 BCAgent를 구현합니다.
- 주요 속성:
  - `actor`: 관찰을 행동으로 매핑하기 위해 MLPPolicySL을 사용합니다.
  - `replay_buffer`: 과거 궤적을 저장합니다.
- 주요 메서드:
  - `train()`: 지도 학습을 사용하여 정책을 업데이트합니다.
  - `add_to_replay_buffer()`: 새로운 롤아웃을 저장합니다.
  - `sample()`: 리플레이 버퍼에서 학습 데이터를 검색합니다.

#### 5.1.4 정책 네트워크: `MLP_policy.py`

- 행동 복제를 위한 피드포워드 신경망인 MLPPolicySL을 구현합니다.
- 주요 메서드:
  - `get_action()`: 관찰이 주어지면 행동을 계산합니다.
  - `forward()`: 네트워크의 순전파를 정의합니다.
  - `update()`: 지도 손실 함수를 사용하여 정책을 학습합니다.

#### 5.1.5 리플레이 버퍼: `replay_buffer.py`

- 전환을 저장하고 샘플링하기 위한 간단한 경험 리플레이 버퍼를 구현합니다.
- 주요 메서드:
  - `add_rollouts()`: 버퍼에 새로운 궤적을 추가합니다.
  - `sample_random_data()`: 무작위 배치의 경험을 반환합니다.

#### 5.1.6 유틸리티 함수: `utils.py`

- 환경 상호작용을 위한 헬퍼 함수를 포함합니다.
- 주요 메서드:
  - `sample_trajectory()`: 단일 에피소드의 경험을 수집합니다.
  - `sample_trajectories()`: 여러 롤아웃을 수집합니다.
  - `sample_n_trajectories()`: 고정된 수의 궤적을 수집합니다.
  - `get_trajlength()`: 궤적의 길이를 계산합니다.

#### 5.1.7 PyTorch 헬퍼 함수: `pytorch_util.py`

- 신경망 구성 및 텐서 처리를 위한 유틸리티 함수를 제공합니다.
- 주요 메서드:
  - `build_mlp()`: 다층 퍼셉트론을 구성합니다.
  - `from_numpy()`, `to_numpy()`: NumPy 배열과 PyTorch 텐서 간 변환합니다.

---

### 5.2 행동 복제 구현

코드에서 TODO로 표시된 섹션을 작성하는 것이 여러분의 과제입니다. 특히 다음 파일들을 살펴보세요:

- `gcb6206/infrastructure/bc_trainer.py` (단, 다음 섹션인 DAgger를 위한 `do_relabel_with_expert` 함수는 제외)
- `gcb6206/policies/MLP_policy.py`
- `gcb6206/infrastructure/replay_buffer.py`
- `gcb6206/infrastructure/utils.py`
- `gcb6206/infrastructure/pytorch_util.py`

다음은 권장 구현 순서입니다:

1. 정책 네트워크에서 사용되므로 `pytorch_util.py`의 `build_mlp()`를 구현합니다.
2. 에이전트가 사용하므로 `MLP_policy.py`의 정책 네트워크를 구현합니다.
3. 기본적인 궤적 샘플링 함수를 제공하는 `utils.py`의 유틸리티 함수를 구현합니다.
4. 에이전트 학습에 필요하므로 `replay_buffer.py`의 리플레이 버퍼를 구현합니다.
5. `bc_agent.py`의 행동 복제 에이전트를 구현합니다.
6. `bc_trainer.py`의 트레이너를 구현합니다.
7. 위의 구현을 완료하면, 모든 것을 연결하는 `run_hw1.py`로 학습을 시작할 수 있습니다.

**두 가지 작업**에 대해 행동 복제(BC)를 실행하고 결과를 보고하세요: (1) Ant 환경(Ant-v4), 여기서 행동 복제 에이전트는 전문가 성능의 최소 30%를 달성해야 하며, (2) Walker2d-v4, HalfCheetah-v4, Hopper-v4 중 하나 (전문가 데이터도 제공됨).

전문가 정책의 성능은 로그 출력의 `Initial_DataCollection_AverageReturn`에서 찾을 수 있습니다.

위의 TODO를 구현하면 다음과 같이 Ant 작업에 대한 BC 정책을 학습할 수 있습니다:

```bash
python gcb6206/scripts/run_hw1.py \
    --expert_policy_file gcb6206/policies/experts/Ant.pkl \
    --env_name Ant-v4 --exp_name bc_ant --n_iter 1 \
    --expert_data gcb6206/expert_data/expert_data_Ant-v4.pkl \
    --video_log_freq -1
```

실행이 성공하면 다음 경로에서 텐서보드 로그 데이터를 찾을 수 있습니다:
`hw1_starter_code/data/q1_[--exp_name]_[--env_name]_[current_time]/`

결과를 제공할 때, 표에 여러 롤아웃에 대한 정책의 평균 및 표준편차를 보고하고 어떤 작업이 사용되었는지 명시하세요. 작동하는 것과 작동하지 않는 것을 비교할 때, 네트워크 크기, 데이터 양, 학습 반복 횟수 측면에서 공정한 비교를 설정해야 합니다. 표 캡션에 이러한 세부사항(그리고 적절하다고 생각하는 다른 모든 것)을 제공하세요.

**참고:** "평균 및 표준편차를 보고"한다는 것은 `eval_batch_size`가 `ep_len`보다 커야 한다는 것을 의미하며, 이렇게 하면 학습된 정책의 성능을 평가할 때 여러 롤아웃을 수집하게 됩니다. 예를 들어, `ep_len`이 1000이고 `eval_batch_size`가 5000이면 약 5개의 에피소드를 수집하게 되며(일부가 조기에 종료되면 더 많을 수 있음), 로깅된 `Eval_AverageReturn`과 `Eval_StdReturn`은 이 5개의 롤아웃에 대한 정책의 평균/표준편차를 나타냅니다. 표 캡션에 이러한 파라미터도 포함해야 합니다.

**참고:** 정책 롤아웃의 비디오를 생성하려면 `--video_log_freq -1` 플래그를 제거하세요. 그러나 이것은 느리므로 디버깅하는 동안에는 이 플래그를 켜 두는 것이 좋습니다.

#### 5.2.1 BC 결과

**표 1:** 두 개의 연속 제어 작업에 대한 행동 복제 성능. 네트워크: tanh 활성화 함수를 사용한 각 64 유닛의 은닉층 2개. 학습: 1000 그래디언트 스텝, 배치 크기 100, 학습률 5e-3. 평가: 5000 타임스텝 (길이 1000의 ~5 에피소드).

| 환경 | 성능 (평균 리턴 ± 표준편차) |
|------|---------------------------|
| Ant-v4 | 4004.17 ± 1297.84 |
| HalfCheetah-v4 | 3801.78 ± 116.14 |

---

### 5.3 롤아웃 렌더링 및 평가

환경 렌더링에 익숙해지기 위해, 학습된 정책의 평가 롤아웃 스크린샷을 포함해야 합니다. 텐서보드를 사용하는 경우 eval rollouts 섹션을 확인할 수 있습니다. 학습이 어떻게 진행되는지 분석하기 위해서는 숫자만 보는 것이 아니라 비디오를 렌더링하고 보는 것이 중요합니다!

#### 5.3.1 환경 렌더링

**그림 1:** Ant-v4에서 학습된 BC 정책 롤아웃. 네 다리 로봇이 1000 그래디언트 스텝의 학습 후 성공적으로 앞으로 걷습니다.

![Ant-v4 롤아웃 스크린샷]

---

### 5.4 행동 복제의 하이퍼파라미터 튜닝

행동 복제 에이전트의 성능에 영향을 미치는 **하나의 하이퍼파라미터 세트**를 실험하세요. 예를 들어 학습 스텝 수, 제공된 전문가 데이터 양, 또는 직접 생각해낸 것 등입니다. 이전 질문에서 사용한 작업 중 하나에 대해, BC 에이전트의 성능이 이 하이퍼파라미터 값에 따라 어떻게 변하는지 그래프로 보여주세요. 하이퍼파라미터와 선택한 이유에 대한 간단한 근거를 명시하세요.

이전 부분에서 사용한 기본 설정을 포함하여 **최소 4개의 다른 설정**을 포함해야 합니다.

**참고:** 명령줄 인자를 사용하여 지정할 수 있는 몇 가지 기본 하이퍼파라미터가 있습니다. 아래 나열된 하이퍼파라미터 중 하나를 선택할 수 있습니다:

- 정책 학습을 위한 그래디언트 스텝 수 (`--num_agent_train_steps_per_iter`, 기본값: 1000)
- 학습 데이터 양 (`--batch_size`, 기본값: 1000)
- 학습 배치 크기 (`--train_batch_size`, 기본값: 100)
- 정책 신경망의 깊이 (`--n_layers`, 기본값: 2)
- 정책 신경망의 너비 (`--size`, 기본값: 64)
- 지도 학습을 위한 학습률 (`--learning_rate`, 기본값: 5e-3)

스크립트를 실행할 때 명령줄에서 하이퍼파라미터를 지정할 수 있습니다. 예를 들어, 다음과 같이 명령을 실행하면 500 그래디언트 스텝 동안 정책을 학습할 수 있습니다:

```bash
python gcb6206/scripts/run_hw1.py \
    --num_agent_train_steps_per_iter 500 \
    --some other arguments...,
```

**참고:** 플롯을 그리기 위해 matplotlib을 사용하세요. matplotlib에 익숙하지 않다면 공식 튜토리얼을 참조할 수 있습니다.

#### 5.4.1 하이퍼파라미터 튜닝 결과

**하이퍼파라미터:** 그래디언트 스텝 수 (`num_agent_train_steps_per_iter`)

**테스트한 값:** [500, 1000, 2000, 3000]

**플롯:**

**그림 2:** Ant-v4에서 다양한 학습 스텝에 따른 BC 성능. 정책은 2000 그래디언트 스텝에서 최고점에 도달하며 (4744.97 ± 142.09), 실제로 전문가를 약간 능가합니다 (4713.65). 학습 설정: 각 64 유닛의 은닉층 2개, 배치 크기 100, 학습률 5e-3. 오차 막대는 5개의 평가 에피소드에 대한 표준편차를 나타냅니다.

![하이퍼파라미터 튜닝 그래프]

**근거:** 지도 학습의 핵심 하이퍼파라미터 중 하나이기 때문에 그래디언트 스텝 수를 변화시켰습니다. 이를 통해 초기 단계의 언더피팅과 과도한 학습으로 인한 오버피팅 사이의 트레이드오프를 볼 수 있습니다.

성능은 2000 스텝에서 최고점에 도달하며, 정책이 전문가 수준을 초과합니다 (100.7%). 하지만 3000 스텝까지 밀어붙이면 실제로 성능이 하락하여 전문가의 86.4%로 떨어집니다. 이는 제한된 전문가 데이터셋에 대한 오버피팅일 수 있지만, 일부는 단순히 평가 노이즈일 수도 있습니다.

또한 주목할 만한 것은 2000 스텝에서 표준편차가 훨씬 낮다는 것입니다 (142.09), 1000 스텝과 3000 스텝에 비해 (둘 다 1200 이상), 이는 정책이 거기서 더 안정적임을 시사합니다. 이를 바탕으로 2000 그래디언트 스텝이 이 작업의 최적점인 것 같습니다.

---

## 6. DAgger

### 6.1 DAgger 구현

이제 DAgger 알고리즘을 구현하는 것이 여러분의 과제입니다. BC 부분을 올바르게 구현했다면, 다음 파일의 TODO만 구현하면 됩니다.

- `gcb6206/infrastructure/bc_trainer.py`의 `do_relabel_with_expert` 함수

코드에서 TODO 주석으로 지정된 모든 지시사항을 작성하면 다음 명령으로 DAgger를 학습할 수 있습니다:

```bash
python gcb6206/scripts/run_hw1.py \
    --expert_policy_file gcb6206/policies/experts/Ant.pkl \
    --env_name Ant-v4 --exp_name dagger_ant --n_iter 10 \
    --do_dagger \
    --expert_data gcb6206/expert_data/expert_data_Ant-v4.pkl \
    --video_log_freq -1
```

---

### 6.2 BC와 DAgger 비교

이전에 BC로 테스트한 두 작업(즉, Ant + 다른 환경)에 대해 DAgger를 실행하고 결과를 보고하세요. 결과를 학습 곡선 형태로 보고하고, DAgger 반복 횟수 대 정책의 평균 리턴을 플로팅하세요. 캡션에 사용한 작업과 네트워크 아키텍처, 데이터 양 등에 관한 세부사항을 명시하세요(이전 섹션과 같이).

**참고:** 텐서보드 로그에서 데이터를 파싱하고 그림을 그리기 위해 예제 헬퍼 스크립트(`gcb6206/scripts/parse_tensorboard.py`)를 사용할 수 있습니다. 다음은 그림을 `output_plot.png`로 저장하는 예제 사용법입니다:

```bash
python gcb6206/scripts/parse_tensorboard.py \
    --input_log_files data/[replace_here_with_the_name_of_log_folder] \
    --data_key "Eval_AverageReturn" \
    --title "DAgger: Ant-v4" \
    --x_label_name "DAgger iterations" \
    --y_label_name "Mean Return" \
    --output_file "output_plot.png"
```

BC와 전문가 정책의 성능을 수평선으로 표시하고 표준편차를 오차 막대로 표시할 수도 있습니다. 예제 파싱 스크립트를 원하는 대로 자유롭게 수정하세요.

#### 6.2.1 DAgger 결과

**그림 3:** Ant-v4 (왼쪽)와 HalfCheetah-v4 (오른쪽)의 10번의 반복에 걸친 DAgger (주황색)와 BC (파란색)의 학습 곡선. 둘 다 동일한 네트워크 아키텍처를 사용합니다—tanh 활성화를 가진 각 64 유닛의 은닉층 2개. BC는 5000 전문가 타임스텝에 대해 한 번 학습하는 반면, DAgger는 전문가 재라벨링과 함께 반복당 1000개의 새로운 타임스텝을 추가합니다. 녹색 점선은 전문가 성능을 나타냅니다. DAgger가 BC에 비해 우위를 보이는 것을 명확히 볼 수 있습니다: Ant에서 전문가 수준의 102.2% (4816.39 대 전문가 4713.65), HalfCheetah에서 97.8% (4112.30 대 4205.78)에 도달하는 반면, BC는 각각 84.9%와 90.4%에만 도달합니다.

![DAgger vs BC 학습 곡선]

#### 6.2.2 결과 정당화

DAgger는 분포 이동(distributional shift) 문제를 해결함으로써 두 작업 모두에서 BC를 능가합니다. 표준 BC는 학습된 정책이 학습 데이터에서 본 것과 벗어날 때 실패합니다—작은 실수들이 눈덩이처럼 불어나는데, 정책이 이러한 상태에서 회복하는 방법을 배우지 못했기 때문입니다.

| 환경 | BC | DAgger | 전문가 |
|------|-----|--------|--------|
| Ant-v4 | 4004.17 (84.9%) | 4816.39 (102.2%) | 4713.65 |
| HalfCheetah-v4 | 3801.78 (90.4%) | 4112.30 (97.8%) | 4205.78 |

핵심 차이점은 학습 데이터를 수집하는 방법입니다. BC는 전문가 시연에 대해 한 번 학습하고 끝입니다. DAgger는 반복적 집계(iterative aggregation)를 사용합니다—반복 0에서는 원래 전문가 데이터에 대해 학습했습니다 (BC와 동일). 그런 다음 반복 1-9에서는 현재 정책을 롤아웃하여 실제로 어디로 가는지 확인하고 (실수 포함), 그 상태에서 올바른 행동에 대해 전문가에게 질의합니다. 각 반복은 정책이 실제로 만나는 상태에서 더 많은 학습 예제를 추가하며, 회복 행동도 포함됩니다.

이렇게 하면 정책은 단순히 전문가의 이상적인 궤적을 복사하는 대신 오류를 수정하는 방법을 배웁니다.

숫자를 보면, DAgger는 Ant-v4에서 4816.39에 도달했습니다—이는 전문가의 102.2%이며 BC의 4004.17보다 20.3% 더 좋습니다. 반복적 수집이 에이전트가 원래 전문가 데모에 없던 시나리오를 찾고 처리하게 해주기 때문에 이런 일이 일어난 것 같습니다. 신경망은 이러한 패턴을 다양한 상태에 걸쳐 꽤 잘 일반화하는 것 같습니다.

HalfCheetah-v4는 달랐습니다. DAgger는 전문가의 97.8%에 도달했습니다 (BC의 90.4%보다 8.2% 개선). 여기서는 전문가를 능가하지 못했지만 훨씬 더 가까워졌습니다. 정책이 분포 밖 상태에 대한 회복 전략을 습득했습니다.

학습 곡선은 꽤 시끄럽습니다—DAgger는 롤아웃이 매번 다른 상태를 샘플링하기 때문에 반복 사이에 튕깁니다. 하지만 반복 10까지 두 환경 모두 전문가 수준 근처 또는 그 이상으로 안정화되었으므로, 이 접근법은 이 작업에 효과가 있습니다.

---

## 7. 제출

"보고서" `hw1_[YourStudentID].pdf`를 제출하세요.

---

**문서 끝**
