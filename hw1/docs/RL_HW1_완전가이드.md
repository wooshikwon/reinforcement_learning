# ê°•í™”í•™ìŠµ HW1 ì™„ì „ ê°€ì´ë“œ: Behavioral Cloningê³¼ DAgger

ì´ ë¬¸ì„œëŠ” `run_hw1.py`ì˜ ì „ì²´ íŒŒì´í”„ë¼ì¸ì„ ì²˜ìŒë¶€í„° ëê¹Œì§€ ìƒì„¸í•˜ê²Œ ì„¤ëª…í•˜ëŠ” ê°•í™”í•™ìŠµ í•™ìŠµìë¥¼ ìœ„í•œ ì™„ì „í•œ ê°€ì´ë“œì…ë‹ˆë‹¤. ì½”ë“œì˜ ê° ì¤„, ê° ë©”ì„œë“œì˜ ë™ì‘ ì›ë¦¬ì™€ ì˜ë„, ê·¸ë¦¬ê³  ê°•í™”í•™ìŠµì˜ í•µì‹¬ ê°œë…ë“¤ì„ í•¨ê»˜ ë‹¤ë£¹ë‹ˆë‹¤.

> **ğŸ“Œ ì¤‘ìš” ì•ˆë‚´:**
> ì´ ê°€ì´ë“œëŠ” TODOë¡œ í‘œì‹œëœ ë¯¸êµ¬í˜„ ë¶€ë¶„ë“¤ì— ëŒ€í•´ **êµ¬í˜„ ì˜ˆì‹œ**ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
> **[êµ¬í˜„ ì˜ˆì‹œ - í•™ìŠµ ì°¸ê³ ìš©]**ìœ¼ë¡œ í‘œì‹œëœ ì½”ë“œëŠ” ì‹¤ì œ ì½”ë“œë² ì´ìŠ¤ì—ëŠ” êµ¬í˜„ë˜ì§€ ì•Šì€ ë¶€ë¶„ì´ë©°, í•™ìŠµê³¼ ì´í•´ë¥¼ ë•ê¸° ìœ„í•œ ì°¸ê³  ìë£Œì…ë‹ˆë‹¤.

## ëª©ì°¨
1. [ì „ì²´ êµ¬ì¡° ê°œìš”](#1-ì „ì²´-êµ¬ì¡°-ê°œìš”)
2. [íŒŒì´í”„ë¼ì¸ ì‹œì‘: main() í•¨ìˆ˜](#2-íŒŒì´í”„ë¼ì¸-ì‹œì‘-main-í•¨ìˆ˜)
3. [ì‹¤í—˜ ì„¤ì •: run_bc() í•¨ìˆ˜](#3-ì‹¤í—˜-ì„¤ì •-run_bc-í•¨ìˆ˜)
4. [í•™ìŠµ ê´€ë¦¬ì: BCTrainer ì´ˆê¸°í™”](#4-í•™ìŠµ-ê´€ë¦¬ì-bctrainer-ì´ˆê¸°í™”)
5. [ì—ì´ì „íŠ¸: BCAgent ì´ˆê¸°í™”](#5-ì—ì´ì „íŠ¸-bcagent-ì´ˆê¸°í™”)
6. [ì •ì±… ë„¤íŠ¸ì›Œí¬: MLPPolicySL ì´ˆê¸°í™”](#6-ì •ì±…-ë„¤íŠ¸ì›Œí¬-mlppolicysl-ì´ˆê¸°í™”)
7. [ì‹ ê²½ë§ êµ¬ì¶•: build_mlp() í•¨ìˆ˜](#7-ì‹ ê²½ë§-êµ¬ì¶•-build_mlp-í•¨ìˆ˜)
8. [ë©”ì¸ í•™ìŠµ ë£¨í”„: run_training_loop()](#8-ë©”ì¸-í•™ìŠµ-ë£¨í”„-run_training_loop)
9. [ë°ì´í„° ìˆ˜ì§‘: collect_training_trajectories()](#9-ë°ì´í„°-ìˆ˜ì§‘-collect_training_trajectories)
10. [í™˜ê²½ ìƒí˜¸ì‘ìš©: rollout_trajectory()](#10-í™˜ê²½-ìƒí˜¸ì‘ìš©-rollout_trajectory)
11. [ì „ë¬¸ê°€ ë¼ë²¨ë§: do_relabel_with_expert()](#11-ì „ë¬¸ê°€-ë¼ë²¨ë§-do_relabel_with_expert)
12. [ê²½í—˜ ì €ì¥: ReplayBuffer.add_rollouts()](#12-ê²½í—˜-ì €ì¥-replaybufferadd_rollouts)
13. [ì—ì´ì „íŠ¸ í•™ìŠµ: train_agent()](#13-ì—ì´ì „íŠ¸-í•™ìŠµ-train_agent)
14. [ë°ì´í„° ìƒ˜í”Œë§: ReplayBuffer.sample_random_data()](#14-ë°ì´í„°-ìƒ˜í”Œë§-replaybuffersample_random_data)
15. [ì •ì±… ì—…ë°ì´íŠ¸: MLPPolicySL í•µì‹¬ ë©”ì„œë“œë“¤](#15-ì •ì±…-ì—…ë°ì´íŠ¸-mlppolicysl-í•µì‹¬-ë©”ì„œë“œë“¤)

---

## 1. ì „ì²´ êµ¬ì¡° ê°œìš”

### 1.1 ì´ ì½”ë“œê°€ í•˜ëŠ” ì¼

ì´ ì½”ë“œë² ì´ìŠ¤ëŠ” **Behavioral Cloning (BC)**ê³¼ **DAgger (Dataset Aggregation)** ë‘ ê°€ì§€ ëª¨ë°© í•™ìŠµ(Imitation Learning) ì•Œê³ ë¦¬ì¦˜ì„ êµ¬í˜„í•©ë‹ˆë‹¤.

**Behavioral Cloningì´ë€?**
- ì „ë¬¸ê°€(expert)ê°€ í”Œë ˆì´í•œ ë°ì´í„°ë¥¼ ë³´ê³ , ê·¸ëŒ€ë¡œ ë”°ë¼í•˜ë„ë¡ í•™ìŠµí•˜ëŠ” ë°©ë²•ì´ì•¼
- ë§ˆì¹˜ ìš´ì „ì„ ë°°ìš¸ ë•Œ ì˜†ì—ì„œ ìˆ™ë ¨ëœ ìš´ì „ìê°€ ìš´ì „í•˜ëŠ” ê±¸ ë³´ê³  ë”°ë¼í•˜ëŠ” ê²ƒê³¼ ê°™ì•„
- ìˆ˜í•™ì ìœ¼ë¡œëŠ”: ì „ë¬¸ê°€ì˜ (ìƒíƒœ, í–‰ë™) ìŒë“¤ì„ í•™ìŠµ ë°ì´í„°ë¡œ ì‚¬ìš©í•´ì„œ supervised learningì„ í•˜ëŠ” ê±°ì§€

**DAggerë€?**
- BCì˜ ë¬¸ì œì ì„ í•´ê²°í•˜ê¸° ìœ„í•œ ë°©ë²•ì´ì•¼
- BCëŠ” ì „ë¬¸ê°€ê°€ ë°©ë¬¸í•œ ìƒíƒœì—ì„œë§Œ í•™ìŠµí•˜ëŠ”ë°, í•™ìŠµ ì¤‘ì¸ ì—ì´ì „íŠ¸ê°€ ì‹¤ìˆ˜í•˜ë©´ ì „ë¬¸ê°€ê°€ ê°€ë³´ì§€ ì•Šì€ ìƒíƒœì— ë„ë‹¬í•˜ê²Œ ë¼
- ê·¸ëŸ¼ ë­˜ í•´ì•¼ í• ì§€ ëª¨ë¥´ê²Œ ë˜ëŠ” ê±°ì§€ (ì´ê±¸ **distributional shift** ë¬¸ì œë¼ê³  í•´)
- DAggerëŠ” ì´ë ‡ê²Œ í•´ê²°í•´: "ë‚´ê°€ ì‹¤ìˆ˜í•´ì„œ ë„ë‹¬í•œ ì´ ìƒíƒœì—ì„œ, ì „ë¬¸ê°€ë¼ë©´ ë­˜ í–ˆì„ê¹Œ?" í•˜ê³  ì „ë¬¸ê°€ì—ê²Œ ë¬¼ì–´ë´ì„œ ê·¸ê²ƒë„ í•™ìŠµí•´

### 1.2 ì½”ë“œ êµ¬ì¡°

```
run_hw1.py (ë©”ì¸ ìŠ¤í¬ë¦½íŠ¸)
    â†“
BCTrainer (í•™ìŠµ ë£¨í”„ ê´€ë¦¬)
    â†“
BCAgent (ì—ì´ì „íŠ¸)
    â”œâ”€â”€ MLPPolicySL (ì •ì±… ë„¤íŠ¸ì›Œí¬)
    â””â”€â”€ ReplayBuffer (ê²½í—˜ ì €ì¥ì†Œ)
```

**í•µì‹¬ í´ë˜ìŠ¤ë“¤:**
- `BCTrainer`: ì „ì²´ í•™ìŠµ ê³¼ì •ì„ orchestrateí•˜ëŠ” ì§€íœ˜ì ì—­í• 
- `BCAgent`: í–‰ë™ì„ ê²°ì •í•˜ê³  í•™ìŠµí•˜ëŠ” ì—ì´ì „íŠ¸
- `MLPPolicySL`: ê´€ì°°ì„ ë°›ì•„ì„œ í–‰ë™ì„ ì¶œë ¥í•˜ëŠ” ì‹ ê²½ë§ (ì •ì±…)
- `ReplayBuffer`: ê³¼ê±° ê²½í—˜ë“¤ì„ ì €ì¥í•˜ëŠ” ë©”ëª¨ë¦¬

---

## 2. íŒŒì´í”„ë¼ì¸ ì‹œì‘: main() í•¨ìˆ˜

`run_hw1.py`ì˜ 66-159ë²ˆ ì¤„

### 2.1 ì»¤ë§¨ë“œë¼ì¸ ì¸ì íŒŒì‹±

```python
parser = argparse.ArgumentParser()
parser.add_argument("--expert_policy_file", "-epf", type=str, required=True)
parser.add_argument("--expert_data", "-ed", type=str, required=True)
parser.add_argument("--env_name", "-env", type=str, required=True)
```

**argparseë€?**
- Pythonì˜ í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ, ì»¤ë§¨ë“œë¼ì¸ì—ì„œ ì‹¤í–‰í•  ë•Œ ì¸ìë“¤ì„ ë°›ëŠ” ë„êµ¬ì•¼
- ì˜ˆë¥¼ ë“¤ì–´: `python run_hw1.py --expert_data data.pkl --env_name Ant-v4`
- ì´ë ‡ê²Œ ì‹¤í–‰í•˜ë©´ `args.expert_data`ëŠ” "data.pkl"ì´ ë˜ëŠ” ê±°ì§€

**ì£¼ìš” ì¸ìë“¤:**
- `expert_policy_file`: ì´ë¯¸ í•™ìŠµëœ ì „ë¬¸ê°€ ì •ì±…ì´ ì €ì¥ëœ íŒŒì¼ ê²½ë¡œ
- `expert_data`: ì „ë¬¸ê°€ê°€ í”Œë ˆì´í•œ ë°ì´í„° (pickle íŒŒì¼)
- `env_name`: ì–´ë–¤ í™˜ê²½ì—ì„œ í•™ìŠµí• ì§€ (ì˜ˆ: "Ant-v4", "Walker2d-v4")
- `do_dagger`: DAggerë¥¼ ì‚¬ìš©í• ì§€ ì—¬ë¶€ (ì´ê±´ flagë¼ì„œ ìˆìœ¼ë©´ True)
- `n_iter`: ëª‡ ë²ˆ ë°˜ë³µí• ì§€

### 2.2 BC vs DAgger êµ¬ë¶„

```python
if args.do_dagger:
    logdir_prefix = "q2_"
    assert args.n_iter > 1, "DAgger needs more than 1 iteration..."
else:
    logdir_prefix = "q1_"
    assert args.n_iter == 1, "Vanilla behavioral cloning collects expert data just once"
```

**ì™œ ì´ë ‡ê²Œ êµ¬ë¶„í• ê¹Œ?**

**BC (Behavioral Cloning):**
- `n_iter == 1`: ë”± í•œ ë²ˆë§Œ ëŒì•„
- ì „ë¬¸ê°€ ë°ì´í„° ë¡œë“œ â†’ ê·¸ê±¸ë¡œ í•™ìŠµ â†’ ë
- ìƒˆë¡œìš´ ë°ì´í„° ìˆ˜ì§‘ ì•ˆ í•¨

**DAgger:**
- `n_iter > 1`: ì—¬ëŸ¬ ë²ˆ ë°˜ë³µí•´ì•¼ í•´
- Iteration 0: ì „ë¬¸ê°€ ë°ì´í„°ë¡œ ì‹œì‘
- Iteration 1~N:
  1. í˜„ì¬ ì •ì±…ìœ¼ë¡œ ë°ì´í„° ìˆ˜ì§‘
  2. ì „ë¬¸ê°€ì—ê²Œ "ì´ ìƒíƒœì—ì„œ ë­˜ í•´ì•¼ ë¼?" ë¬¼ì–´ë´„
  3. ê·¸ ë‹µë³€ìœ¼ë¡œ ë‹¤ì‹œ í•™ìŠµ
  4. ë°˜ë³µ

**assertë¬¸ì´ë€?**
- ì¡°ê±´ì´ Falseë©´ ì—ëŸ¬ë¥¼ ë°œìƒì‹œí‚¤ëŠ” Python ë¬¸ë²•
- ì—¬ê¸°ì„œëŠ” ì˜ëª»ëœ ì„¤ì •ì„ ë¯¸ë¦¬ ì¡ê¸° ìœ„í•œ ì•ˆì „ì¥ì¹˜ì•¼
- DAggerì¸ë° n_iter=1ì´ë©´? "ì–´? í•œ ë²ˆë§Œ ëŒë©´ DAggerê°€ ì•„ë‹ˆì–ì•„!" í•˜ê³  ì—ëŸ¬ ë°œìƒ

### 2.3 ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±

```python
data_path = os.path.join(os.path.dirname(os.path.realpath(__file__)), "../../data")
if not os.path.exists(data_path):
    os.makedirs(data_path)

logdir = logdir_prefix + args.exp_name + "_" + args.env_name + "_" + time.strftime("%d-%m-%Y_%H-%M-%S")
logdir = os.path.join(data_path, logdir)
```

**os.path í•¨ìˆ˜ë“¤ ì„¤ëª…:**
- `os.path.realpath(__file__)`: í˜„ì¬ ì‹¤í–‰ ì¤‘ì¸ íŒŒì¼ì˜ ì ˆëŒ€ ê²½ë¡œë¥¼ ê°€ì ¸ì™€
- `os.path.dirname()`: ê²½ë¡œì—ì„œ ë””ë ‰í† ë¦¬ ë¶€ë¶„ë§Œ ì¶”ì¶œ
- `os.path.join()`: ê²½ë¡œë“¤ì„ OSì— ë§ê²Œ í•©ì³ì¤˜ (WindowsëŠ” `\`, Linux/Macì€ `/`)

**time.strftime() ì„¤ëª…:**
- í˜„ì¬ ì‹œê°„ì„ ë¬¸ìì—´ë¡œ í¬ë§·íŒ…í•´
- `"%d-%m-%Y_%H-%M-%S"`: "ì¼-ì›”-ë…„_ì‹œ-ë¶„-ì´ˆ" í˜•ì‹
- ì˜ˆ: "04-10-2025_14-30-45"
- ì™œ? ê°™ì€ ì‹¤í—˜ì„ ì—¬ëŸ¬ ë²ˆ ëŒë ¤ë„ ë¡œê·¸ê°€ ì„ì´ì§€ ì•Šê²Œ!

**ê²°ê³¼ ì˜ˆì‹œ:**
```
logdir = "data/q2_my_experiment_Ant-v4_04-10-2025_14-30-45"
```

---

## 3. ì‹¤í—˜ ì„¤ì •: run_bc() í•¨ìˆ˜

`run_hw1.py`ì˜ 16-63ë²ˆ ì¤„

### 3.1 Agent íŒŒë¼ë¯¸í„° ì„¤ì •

```python
agent_params = {
    "n_layers": params["n_layers"],
    "size": params["size"],
    "learning_rate": params["learning_rate"],
    "max_replay_buffer_size": params["max_replay_buffer_size"],
}
params["agent_class"] = BCAgent
params["agent_params"] = agent_params
```

**ì—¬ê¸°ì„œ ë­˜ í•˜ëŠ” ê±°ì•¼?**
- ì‹ ê²½ë§ êµ¬ì¡°ì™€ í•™ìŠµ ì„¤ì •ì„ ë”•ì…”ë„ˆë¦¬ë¡œ ì •ë¦¬í•´
- ë‚˜ì¤‘ì— BCAgentë¥¼ ë§Œë“¤ ë•Œ ì´ íŒŒë¼ë¯¸í„°ë“¤ì„ ì „ë‹¬í•  ê±°ì•¼

**ê° íŒŒë¼ë¯¸í„° ì˜ë¯¸:**
- `n_layers`: ì‹ ê²½ë§ì˜ ì€ë‹‰ì¸µ(hidden layer) ê°œìˆ˜
  - ì˜ˆ: n_layers=2ë©´ input â†’ hidden1 â†’ hidden2 â†’ output
- `size`: ê° ì€ë‹‰ì¸µì˜ ë‰´ëŸ°(neuron) ê°œìˆ˜
  - ì˜ˆ: size=64ë©´ ê° ì€ë‹‰ì¸µì— 64ê°œ ë‰´ëŸ°
- `learning_rate`: í•™ìŠµë¥  (ì–¼ë§ˆë‚˜ í¬ê²Œ íŒŒë¼ë¯¸í„°ë¥¼ ì—…ë°ì´íŠ¸í• ì§€)
  - ì˜ˆ: 0.001ì´ë©´ ì²œì²œíˆ, 0.1ì´ë©´ ë¹ ë¥´ê²Œ í•™ìŠµ
- `max_replay_buffer_size`: ë²„í¼ì— ìµœëŒ€ ëª‡ ê°œì˜ transitionì„ ì €ì¥í• ì§€
  - ì˜ˆ: 1000000ì´ë©´ ë°±ë§Œ ê°œê¹Œì§€ ì €ì¥

### 3.2 í™˜ê²½ íŒŒë¼ë¯¸í„° ì„¤ì •

```python
params["env_kwargs"] = MJ_ENV_KWARGS[params["env_name"]]
```

**MJ_ENV_KWARGSëŠ” ë­ì•¼?**
```python
# utils.pyì— ì •ì˜ë¨
MJ_ENV_NAMES = ["Ant-v4", "Walker2d-v4", "HalfCheetah-v4", "Hopper-v4"]
MJ_ENV_KWARGS = {name: {"render_mode": "rgb_array"} for name in MJ_ENV_NAMES}
MJ_ENV_KWARGS["Ant-v4"]["use_contact_forces"] = True
```

- **ë”•ì…”ë„ˆë¦¬ ì»´í”„ë¦¬í—¨ì…˜**: `{key: value for item in list}` ë¬¸ë²•
- ê° í™˜ê²½ ì´ë¦„ì„ keyë¡œ, í™˜ê²½ ì„¤ì •ì„ valueë¡œ í•˜ëŠ” ë”•ì…”ë„ˆë¦¬ ìƒì„±
- `render_mode="rgb_array"`: í™˜ê²½ì„ ì´ë¯¸ì§€(RGB ë°°ì—´)ë¡œ ë Œë”ë§
- Ant-v4ëŠ” ì¶”ê°€ë¡œ `use_contact_forces=True`: ì ‘ì´‰ë ¥ ì •ë³´ ì‚¬ìš©

### 3.3 Expert Policy ë¡œë“œ

```python
print("Loading expert policy from...", params["expert_policy_file"])
loaded_expert_policy = LoadedGaussianPolicy(params["expert_policy_file"])
print("Done restoring expert policy...")
```

**LoadedGaussianPolicyë€?**
- ì´ë¯¸ í•™ìŠµëœ ì „ë¬¸ê°€ ì •ì±…ì„ íŒŒì¼ì—ì„œ ë¶ˆëŸ¬ì˜¤ëŠ” í´ë˜ìŠ¤ì•¼
- pickle íŒŒì¼ì—ì„œ ì‹ ê²½ë§ ê°€ì¤‘ì¹˜ë¥¼ ì½ì–´ì„œ ë³µì›í•´
- **Gaussian Policy**: ì •ê·œë¶„í¬ë¥¼ ì‚¬ìš©í•˜ëŠ” í™•ë¥ ì  ì •ì±…
  - ê´€ì°° së¥¼ ë°›ì•„ì„œ â†’ í–‰ë™ ë¶„í¬ N(Î¼, ÏƒÂ²)ë¥¼ ì¶œë ¥
  - ì—¬ê¸°ì„œ í–‰ë™ì„ ìƒ˜í”Œë§í•´

**ì™œ ì „ë¬¸ê°€ ì •ì±…ì´ í•„ìš”í•´?**
- BC: ì „ë¬¸ê°€ ë°ì´í„°ë¥¼ ì œê³µí•˜ê¸° ìœ„í•´
- DAgger: ìƒˆë¡œ ìˆ˜ì§‘í•œ ìƒíƒœì—ì„œ ì •ë‹µ í–‰ë™ì„ ë¼ë²¨ë§í•˜ê¸° ìœ„í•´

### 3.4 í•™ìŠµ ì‹œì‘

```python
trainer = BCTrainer(params)
trainer.run_training_loop(
    n_iter=params["n_iter"],
    initial_expertdata=params["expert_data"],
    collect_policy=trainer.agent.actor,
    eval_policy=trainer.agent.actor,
    relabel_with_expert=params["do_dagger"],
    expert_policy=loaded_expert_policy,
)
```

**ì¸ì ì„¤ëª…:**
- `n_iter`: ì´ ëª‡ iteration ëŒë¦´ì§€
- `initial_expertdata`: ì²« iterationì— ì‚¬ìš©í•  ì „ë¬¸ê°€ ë°ì´í„° íŒŒì¼ ê²½ë¡œ
- `collect_policy`: ë°ì´í„° ìˆ˜ì§‘ì— ì‚¬ìš©í•  ì •ì±… (ìš°ë¦¬ê°€ í•™ìŠµ ì¤‘ì¸ ì •ì±…)
- `eval_policy`: í‰ê°€ì— ì‚¬ìš©í•  ì •ì±… (ë˜‘ê°™ì´ ìš°ë¦¬ ì •ì±…)
- `relabel_with_expert`: DAgger ëª¨ë“œì¸ì§€ (Trueë©´ ì „ë¬¸ê°€ê°€ ë‹¤ì‹œ ë¼ë²¨ë§)
- `expert_policy`: ë¼ë²¨ë§ì— ì‚¬ìš©í•  ì „ë¬¸ê°€ ì •ì±…

**trainer.agent.actorëŠ” ë­ì•¼?**
- `trainer`: BCTrainer ì¸ìŠ¤í„´ìŠ¤
- `trainer.agent`: BCAgent ì¸ìŠ¤í„´ìŠ¤ (ë‚˜ì¤‘ì— ë§Œë“¤ì–´ì§)
- `trainer.agent.actor`: MLPPolicySL ì¸ìŠ¤í„´ìŠ¤ (ì‹¤ì œ ì •ì±… ì‹ ê²½ë§)

---

## 4. í•™ìŠµ ê´€ë¦¬ì: BCTrainer ì´ˆê¸°í™”

`bc_trainer.py`ì˜ 52-106ë²ˆ ì¤„

### 4.1 ê¸°ë³¸ ì„¤ì •

```python
def __init__(self, params):
    self.params = params
    self.logger = Logger(self.params["logdir"])
```

**__init__ì€ ë­ì•¼?**
- Pythonì˜ íŠ¹ë³„ ë©”ì„œë“œ(magic method)ì•¼
- í´ë˜ìŠ¤ì˜ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë§Œë“¤ ë•Œ ìë™ìœ¼ë¡œ í˜¸ì¶œë¼
- `trainer = BCTrainer(params)` í•˜ë©´ ì´ í•¨ìˆ˜ê°€ ì‹¤í–‰ë˜ëŠ” ê±°ì§€

**LoggerëŠ” ë­í•˜ëŠ” ê±°ì•¼?**
- TensorBoardì— í•™ìŠµ ê³¼ì •ì„ ê¸°ë¡í•˜ëŠ” í´ë˜ìŠ¤
- ì†ì‹¤(loss), ë³´ìƒ(reward), ë¹„ë””ì˜¤ ë“±ì„ ì €ì¥
- ë‚˜ì¤‘ì— TensorBoardë¡œ ì‹œê°í™”í•´ì„œ ë³¼ ìˆ˜ ìˆì–´

### 4.2 Random Seed ì„¤ì •

```python
seed = self.params["seed"]
np.random.seed(seed)
torch.manual_seed(seed)
ptu.init_gpu(use_gpu=not self.params["no_gpu"], gpu_id=self.params["which_gpu"])
```

**ì™œ seedë¥¼ ì„¤ì •í•´?**
- **ì¬í˜„ì„±(Reproducibility)**: ê°™ì€ seedë©´ ê°™ì€ ëœë¤ ê²°ê³¼ê°€ ë‚˜ì™€
- ë”¥ëŸ¬ë‹ì€ ëœë¤ ìš”ì†Œê°€ ë§ì•„:
  - ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”
  - ë°ì´í„° ì…”í”Œë§
  - dropout ë“±
- Seedë¥¼ ê³ ì •í•˜ë©´ ì‹¤í—˜ì„ ì •í™•íˆ ì¬í˜„í•  ìˆ˜ ìˆì–´

**ê° seed í•¨ìˆ˜:**
- `np.random.seed(seed)`: NumPyì˜ ëœë¤ ìƒì„±ê¸° ì´ˆê¸°í™”
- `torch.manual_seed(seed)`: PyTorchì˜ ëœë¤ ìƒì„±ê¸° ì´ˆê¸°í™”
- `ptu.init_gpu()`: GPU ì„¤ì • ë° CUDA seed ì´ˆê¸°í™”

**ptu.init_gpu() ìì„¸íˆ:**
```python
def init_gpu(use_gpu=True, gpu_id=0):
    global device
    if torch.cuda.is_available() and use_gpu:
        device = torch.device("cuda:" + str(gpu_id))
        print("Using GPU id {}".format(gpu_id))
    else:
        device = torch.device("cpu")
        print("GPU not detected. Defaulting to CPU.")
```

- `torch.cuda.is_available()`: CUDA(NVIDIA GPU) ì‚¬ìš© ê°€ëŠ¥í•œì§€ ì²´í¬
- `torch.device()`: ì—°ì‚°ì„ ì–´ë””ì„œ í• ì§€ (GPU or CPU)
- global device: ì „ì—­ ë³€ìˆ˜ë¡œ ì„¤ì •í•´ì„œ ì–´ë””ì„œë“  ì‚¬ìš© ê°€ëŠ¥

### 4.3 í™˜ê²½(Environment) ì„¤ì •

```python
self.env = gym.make(self.params["env_name"], **self.params["env_kwargs"])
self.env.reset(seed=seed)
```

**gym.make()ë€?**
- OpenAI Gym/Gymnasiumì˜ í•¨ìˆ˜
- í™˜ê²½ ì´ë¦„ì„ ì£¼ë©´ ê·¸ í™˜ê²½ì„ ë§Œë“¤ì–´ì¤˜
- `**env_kwargs`: ë”•ì…”ë„ˆë¦¬ë¥¼ keyword argumentsë¡œ ì–¸íŒ©
  - `gym.make("Ant-v4", render_mode="rgb_array", use_contact_forces=True)` ì™€ ë™ì¼

**env.reset()ì´ë€?**
- í™˜ê²½ì„ ì´ˆê¸° ìƒíƒœë¡œ ë¦¬ì…‹
- ì²« observationê³¼ infoë¥¼ ë°˜í™˜
- seedë¥¼ ì£¼ë©´ í™˜ê²½ì˜ ëœë¤ì„±ë„ ê³ ì •ë¼

### 4.4 í™˜ê²½ ì •ë³´ ì¶”ì¶œ

```python
self.params["ep_len"] = self.params["ep_len"] or self.env.spec.max_episode_steps
```

**`or` ì—°ì‚°ì íŠ¸ë¦­:**
- `A or B`: Aê°€ False(ë˜ëŠ” None, 0)ë©´ Bë¥¼ ë°˜í™˜
- `params["ep_len"]`ì´ Noneì´ë©´ â†’ `env.spec.max_episode_steps` ì‚¬ìš©
- ì‚¬ìš©ìê°€ ì§€ì • ì•ˆ í–ˆìœ¼ë©´ í™˜ê²½ ê¸°ë³¸ê°’ ì‚¬ìš©

```python
discrete = isinstance(self.env.action_space, gym.spaces.Discrete)
```

**isinstance()ë€?**
- ê°ì²´ê°€ íŠ¹ì • í´ë˜ìŠ¤ì˜ ì¸ìŠ¤í„´ìŠ¤ì¸ì§€ í™•ì¸
- `gym.spaces.Discrete`: ì´ì‚° í–‰ë™ ê³µê°„ (0, 1, 2, ... ê°™ì€ ì •ìˆ˜)
- `gym.spaces.Box`: ì—°ì† í–‰ë™ ê³µê°„ (ì‹¤ìˆ˜ ë²¡í„°)

**ì´ì‚° vs ì—°ì†:**
- ì´ì‚°: ì²´ìŠ¤ (64ì¹¸ ì¤‘ í•˜ë‚˜ ì„ íƒ)
- ì—°ì†: ë¡œë´‡ ì œì–´ (ê´€ì ˆ ê°ë„ë¥¼ ì—°ì†ì ìœ¼ë¡œ ì¡°ì ˆ)

```python
ob_dim = self.env.observation_space.shape[0]
ac_dim = self.env.action_space.n if discrete else self.env.action_space.shape[0]
```

**shape[0]ì€ ë­ì•¼?**
- NumPy ë°°ì—´ì˜ ì²« ë²ˆì§¸ ì°¨ì› í¬ê¸°
- ì˜ˆ: observationì´ 17ì°¨ì› ë²¡í„°ë©´ `shape = (17,)` â†’ `shape[0] = 17`

**ac_dim ê³„ì‚°:**
- ì´ì‚°: `action_space.n` (í–‰ë™ ê°œìˆ˜, ì˜ˆ: 4ê°œ ë°©í–¥)
- ì—°ì†: `action_space.shape[0]` (í–‰ë™ ë²¡í„° ì°¨ì›, ì˜ˆ: 8ê°œ ê´€ì ˆ)

### 4.5 FPS ì„¤ì •

```python
if "model" in dir(self.env):
    self.fps = 1 / self.env.model.opt.timestep
else:
    self.fps = self.env.env.metadata["render_fps"]
```

**dir() í•¨ìˆ˜:**
- ê°ì²´ê°€ ê°€ì§„ ì†ì„±(attribute)ê³¼ ë©”ì„œë“œ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
- `"model" in dir(self.env)`: envì— model ì†ì„±ì´ ìˆëŠ”ì§€ í™•ì¸

**ì™œ ì´ë ‡ê²Œ í•´?**
- MuJoCo í™˜ê²½: `env.model.opt.timestep`ì—ì„œ ì‹œë®¬ë ˆì´ì…˜ timestep ê°€ì ¸ì™€ â†’ fps ê³„ì‚°
- ë‹¤ë¥¸ í™˜ê²½: metadataì—ì„œ ì§ì ‘ fps ê°€ì ¸ì™€
- ë¹„ë””ì˜¤ ì €ì¥í•  ë•Œ ì˜¬ë°”ë¥¸ fps í•„ìš”!

### 4.6 Agent ìƒì„±

```python
agent_class = self.params["agent_class"]  # BCAgent
self.agent = agent_class(self.env, self.params["agent_params"])
```

**ë™ì  í´ë˜ìŠ¤ í˜¸ì¶œ:**
- `agent_class`ëŠ” ë³€ìˆ˜ì— ì €ì¥ëœ í´ë˜ìŠ¤ (BCAgent)
- `agent_class(...)`: ê·¸ í´ë˜ìŠ¤ì˜ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
- ì™œ ì´ë ‡ê²Œ? ë‚˜ì¤‘ì— ë‹¤ë¥¸ agentë„ ì‰½ê²Œ ì‚¬ìš©í•˜ë ¤ê³  (í™•ì¥ì„±)

---

## 5. ì—ì´ì „íŠ¸: BCAgent ì´ˆê¸°í™”

`bc_agent.py`ì˜ 25-43ë²ˆ ì¤„

### 5.1 í´ë˜ìŠ¤ êµ¬ì¡°

```python
class BCAgent(BaseAgent):
    def __init__(self, env, agent_params):
        super(BCAgent, self).__init__()
```

**ìƒì†(Inheritance):**
- `class BCAgent(BaseAgent)`: BCAgentëŠ” BaseAgentë¥¼ ìƒì†
- BaseAgentëŠ” ì¶”ìƒ í´ë˜ìŠ¤(abstract class)ë¡œ ì¸í„°í˜ì´ìŠ¤ ì •ì˜
- `super().__init__()`: ë¶€ëª¨ í´ë˜ìŠ¤ì˜ ì´ˆê¸°í™” í˜¸ì¶œ

**BaseAgent ì‚´í´ë³´ê¸°:**
```python
class BaseAgent(object):
    def train(self) -> dict:
        raise NotImplementedError
    def add_to_replay_buffer(self, trajs):
        raise NotImplementedError
    def sample(self, batch_size):
        raise NotImplementedError
```

- `NotImplementedError`: ìì‹ í´ë˜ìŠ¤ì—ì„œ êµ¬í˜„í•˜ë¼ëŠ” ëœ»
- ì´ê²Œ **ì¶”ìƒ ë©”ì„œë“œ íŒ¨í„´**ì´ì•¼

### 5.2 ì •ì±…(Actor) ìƒì„±

```python
self.actor = MLPPolicySL(
    self.agent_params["ac_dim"],
    self.agent_params["ob_dim"],
    self.agent_params["n_layers"],
    self.agent_params["size"],
    discrete=self.agent_params["discrete"],
    learning_rate=self.agent_params["learning_rate"],
)
```

**MLPPolicySL ì¸ì:**
- `ac_dim`: í–‰ë™ ì°¨ì› (ì˜ˆ: 8ê°œ ê´€ì ˆ)
- `ob_dim`: ê´€ì°° ì°¨ì› (ì˜ˆ: 17ì°¨ì› ë²¡í„°)
- `n_layers`: ì€ë‹‰ì¸µ ê°œìˆ˜ (ì˜ˆ: 2)
- `size`: ê° ì¸µ í¬ê¸° (ì˜ˆ: 64)
- `discrete`: ì´ì‚°/ì—°ì† (ì—¬ê¸°ì„  False, ì—°ì†)
- `learning_rate`: í•™ìŠµë¥  (ì˜ˆ: 0.001)

**ì™œ actorë¼ê³  ë¶ˆëŸ¬?**
- Actor-Critic êµ¬ì¡°ì—ì„œ ìœ ë˜
- Actor: í–‰ë™ì„ ê²°ì •í•˜ëŠ” ì •ì±…
- Critic: ê°€ì¹˜ë¥¼ í‰ê°€í•˜ëŠ” í•¨ìˆ˜
- BCëŠ” actorë§Œ ìˆê³  criticì€ ì—†ì–´

### 5.3 Replay Buffer ìƒì„±

```python
self.replay_buffer = ReplayBuffer(self.agent_params["max_replay_buffer_size"])
```

**Replay Bufferë€?**
- ê³¼ê±° ê²½í—˜ë“¤ì„ ì €ì¥í•˜ëŠ” ë©”ëª¨ë¦¬
- (observation, action, reward, next_observation, done) íŠœí”Œë“¤ì„ ì €ì¥
- ë‚˜ì¤‘ì— ë¬´ì‘ìœ„ë¡œ ìƒ˜í”Œë§í•´ì„œ í•™ìŠµì— ì‚¬ìš©

**ì™œ í•„ìš”í•´?**
- **Off-policy í•™ìŠµ**: ê³¼ê±° ë°ì´í„° ì¬ì‚¬ìš©
- **Sample efficiency**: ë°ì´í„°ë¥¼ ì—¬ëŸ¬ ë²ˆ ì‚¬ìš©í•´ í•™ìŠµ íš¨ìœ¨â†‘
- **Breaking correlation**: ì‹œê°„ì  ìƒê´€ê´€ê³„ ì œê±°

### 5.4 BCAgent ë©”ì„œë“œë“¤

```python
def train(self, ob_no, ac_na):
    log = self.actor.update(ob_no, ac_na)
    return log
```

**ë³€ìˆ˜ëª… ê·œì¹™:**
- `ob_no`: observation, batch_size Ã— ob_dim
- `ac_na`: action, batch_size Ã— ac_dim
- `_no`, `_na`ëŠ” ì°¨ì› í‘œì‹œ ê´€ë¡€ (n=batch, o=obs, a=action)

```python
def add_to_replay_buffer(self, trajs):
    self.replay_buffer.add_rollouts(trajs)
```

**ë‹¨ìˆœ ìœ„ì„(delegation):**
- BCAgentê°€ ì§ì ‘ í•˜ì§€ ì•Šê³  replay_bufferì—ê²Œ ë„˜ê¹€
- **ê´€ì‹¬ì‚¬ ë¶„ë¦¬(Separation of Concerns)** ì›ì¹™

```python
def sample(self, batch_size):
    return self.replay_buffer.sample_random_data(batch_size)
```

**ìƒ˜í”Œë§ ë©”ì„œë“œ:**
- ë²„í¼ì—ì„œ batch_sizeë§Œí¼ ë¬´ì‘ìœ„ ìƒ˜í”Œ ì¶”ì¶œ
- 5ê°œ ë°°ì—´ ë°˜í™˜: obs, actions, rewards, next_obs, terminals

---

## 6. ì •ì±… ë„¤íŠ¸ì›Œí¬: MLPPolicySL ì´ˆê¸°í™”

`MLP_policy.py`ì˜ 48-99ë²ˆ ì¤„

### 6.1 í´ë˜ìŠ¤ ì •ì˜

```python
class MLPPolicySL(BasePolicy, nn.Module, metaclass=abc.ABCMeta):
```

**ë‹¤ì¤‘ ìƒì†(Multiple Inheritance):**
- `BasePolicy`: ì •ì±… ì¸í„°í˜ì´ìŠ¤
- `nn.Module`: PyTorch ì‹ ê²½ë§ ê¸°ë³¸ í´ë˜ìŠ¤
- `metaclass=abc.ABCMeta`: ì¶”ìƒ í´ë˜ìŠ¤ ë©”íƒ€í´ë˜ìŠ¤

**nn.Moduleì´ë€?**
- PyTorchì˜ ëª¨ë“  ì‹ ê²½ë§ì€ nn.Module ìƒì†
- ì œê³µ ê¸°ëŠ¥:
  - `.parameters()`: í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„° ê´€ë¦¬
  - `.to(device)`: GPU/CPU ì´ë™
  - `.train()`, `.eval()`: í•™ìŠµ/í‰ê°€ ëª¨ë“œ ì „í™˜
  - forward() ìë™ í˜¸ì¶œ

### 6.2 ì´ˆê¸°í™” íŒŒë¼ë¯¸í„°

```python
def __init__(self, ac_dim, ob_dim, n_layers, size, discrete=False,
             learning_rate=1e-4, training=True, nn_baseline=False, **kwargs):
    super().__init__(**kwargs)

    self.ac_dim = ac_dim
    self.ob_dim = ob_dim
    self.n_layers = n_layers
    self.discrete = discrete
    self.size = size
    self.learning_rate = learning_rate
```

**super().__init__(**kwargs):**
- ë¶€ëª¨ í´ë˜ìŠ¤ë“¤ì˜ ì´ˆê¸°í™” í˜¸ì¶œ
- `**kwargs`: ì¶”ê°€ keyword argumentsë¥¼ ë°›ì•„ì„œ ì „ë‹¬

### 6.3 ì—°ì† í–‰ë™ ê³µê°„ ì„¤ì •

```python
if self.discrete:
    # ... ì´ì‚° í–‰ë™ ê³µê°„ (ìš°ë¦¬ëŠ” ì•ˆ ì”€)
else:
    self.logits_na = None
    self.mean_net = ptu.build_mlp(
        input_size=self.ob_dim,
        output_size=self.ac_dim,
        n_layers=self.n_layers,
        size=self.size,
    )
    self.mean_net.to(ptu.device)
```

**mean_netì´ë€?**
- ê´€ì°° â†’ í–‰ë™ í‰ê· ì„ ì¶œë ¥í•˜ëŠ” ì‹ ê²½ë§
- ì…ë ¥: ob_dim ì°¨ì› ë²¡í„° (ì˜ˆ: 17ì°¨ì› ìƒíƒœ)
- ì¶œë ¥: ac_dim ì°¨ì› ë²¡í„° (ì˜ˆ: 8ì°¨ì› í–‰ë™ í‰ê· )

**ptu.build_mlp():**
- MLP(Multi-Layer Perceptron) ìƒì„± í•¨ìˆ˜
- ë‚˜ì¤‘ì— ìì„¸íˆ ë³¼ê²Œ

**.to(ptu.device):**
- ì‹ ê²½ë§ì„ GPU ë˜ëŠ” CPUë¡œ ì´ë™
- ptu.deviceëŠ” ì´ˆê¸°í™” ë•Œ ì„¤ì •í•œ ì „ì—­ ë³€ìˆ˜

### 6.4 Log Standard Deviation

```python
self.logstd = nn.Parameter(
    torch.zeros(self.ac_dim, dtype=torch.float32, device=ptu.device)
)
self.logstd.to(ptu.device)
```

**nn.Parameterë€?**
- PyTorchì—ì„œ í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°ë¥¼ ë‚˜íƒ€ë‚´ëŠ” íŠ¹ë³„í•œ Tensor
- ì‹ ê²½ë§ì— ë“±ë¡ë˜ì–´ `.parameters()`ì— í¬í•¨ë¨
- optimizerê°€ ìë™ìœ¼ë¡œ ì—…ë°ì´íŠ¸í•¨

**ì™œ log stdë¥¼ ì‚¬ìš©í•´?**
- stdëŠ” í•­ìƒ ì–‘ìˆ˜ì—¬ì•¼ í•´ (í‘œì¤€í¸ì°¨ë‹ˆê¹Œ)
- logstdëŠ” unbounded: -âˆ ~ +âˆ ë²”ìœ„
- `std = exp(logstd)` í•˜ë©´:
  - logstdê°€ ìŒìˆ˜ì—¬ë„ stdëŠ” ì–‘ìˆ˜
  - logstd = 0 â†’ std = 1
  - logstd = -2 â†’ std â‰ˆ 0.135
  - logstd = 2 â†’ std â‰ˆ 7.39
- **ìˆ˜ì¹˜ì  ì•ˆì •ì„±**: log spaceì—ì„œ í•™ìŠµí•˜ë©´ gradient flowê°€ ì¢‹ì•„

**ì´ˆê¸°ê°’ì´ 0ì¸ ì´ìœ :**
- logstd = 0 â†’ std = exp(0) = 1
- ì´ˆê¸°ì— ì ë‹¹í•œ exploration ì œê³µ

### 6.5 Optimizer ì„¤ì •

```python
self.optimizer = optim.Adam(
    itertools.chain([self.logstd], self.mean_net.parameters()),
    self.learning_rate,
)
```

**itertools.chain():**
- ì—¬ëŸ¬ iterableì„ í•˜ë‚˜ë¡œ ì—°ê²°
- `[self.logstd]`: ë¦¬ìŠ¤íŠ¸ì— logstd íŒŒë¼ë¯¸í„°
- `self.mean_net.parameters()`: mean_netì˜ ëª¨ë“  íŒŒë¼ë¯¸í„°
- ê²°ê³¼: ëª¨ë“  í•™ìŠµ íŒŒë¼ë¯¸í„°ë¥¼ í•˜ë‚˜ì˜ iteratorë¡œ

**Adam optimizer:**
- Adaptive Moment Estimation
- ê° íŒŒë¼ë¯¸í„°ë§ˆë‹¤ ì ì‘ì  í•™ìŠµë¥  ì‚¬ìš©
- ì™œ Adam?
  - SGDë³´ë‹¤ ë¹ ë¥¸ ìˆ˜ë ´
  - Learning rateì— ëœ ë¯¼ê°
  - ëª¨ë©˜í…€ê³¼ RMSPropì˜ ì¥ì  ê²°í•©

**ì‘ë™ ì›ë¦¬ (ê°„ë‹¨íˆ):**
```
m = Î²â‚ Â· m + (1-Î²â‚) Â· gradient     # 1ì°¨ ëª¨ë©˜íŠ¸ (í‰ê· )
v = Î²â‚‚ Â· v + (1-Î²â‚‚) Â· gradientÂ²    # 2ì°¨ ëª¨ë©˜íŠ¸ (ë¶„ì‚°)
Î¸ = Î¸ - lr Â· m / (âˆšv + Îµ)          # íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸
```

---

## 7. ì‹ ê²½ë§ êµ¬ì¶•: build_mlp() í•¨ìˆ˜

`pytorch_util.py`ì˜ 25-59ë²ˆ ì¤„

### 7.1 í•¨ìˆ˜ ì‹œê·¸ë‹ˆì²˜

```python
def build_mlp(
    input_size: int,
    output_size: int,
    n_layers: int,
    size: int,
    activation: Activation = "tanh",
    output_activation: Activation = "identity",
) -> nn.Module:
```

**Type hints:**
- `input_size: int`: input_sizeëŠ” int íƒ€ì…ì´ì–´ì•¼ í•¨
- `-> nn.Module`: ë°˜í™˜ê°’ì€ nn.Module íƒ€ì…
- Python 3.5+ì˜ ê¸°ëŠ¥, ì‹¤í–‰ì—ëŠ” ì˜í–¥ ì—†ì§€ë§Œ ì½”ë“œ ê°€ë…ì„±â†‘

### 7.2 í™œì„±í™” í•¨ìˆ˜ ì²˜ë¦¬

```python
if isinstance(activation, str):
    activation = _str_to_activation[activation]
if isinstance(output_activation, str):
    output_activation = _str_to_activation[output_activation]
```

**_str_to_activation ë”•ì…”ë„ˆë¦¬:**
```python
_str_to_activation = {
    "relu": nn.ReLU(),
    "tanh": nn.Tanh(),
    "leaky_relu": nn.LeakyReLU(),
    "sigmoid": nn.Sigmoid(),
    "selu": nn.SELU(),
    "softplus": nn.Softplus(),
    "identity": nn.Identity(),
}
```

**í™œì„±í™” í•¨ìˆ˜ë“¤:**
- **ReLU**: max(0, x) - ê°€ì¥ ë§ì´ ì“°ì„
- **Tanh**: ìŒê³¡íƒ„ì  íŠ¸, ë²”ìœ„ [-1, 1]
- **LeakyReLU**: max(0.01x, x) - ReLUì˜ dying ë¬¸ì œ í•´ê²°
- **Sigmoid**: 1/(1+e^-x), ë²”ìœ„ [0, 1]
- **SELU**: Self-normalizing íš¨ê³¼
- **Softplus**: log(1+e^x) - ReLUì˜ ë¶€ë“œëŸ¬ìš´ ë²„ì „
- **Identity**: f(x) = x - ê·¸ëŒ€ë¡œ í†µê³¼

### 7.3 MLP êµ¬ì¡° ìƒì„±

**[êµ¬í˜„ ì˜ˆì‹œ - í•™ìŠµ ì°¸ê³ ìš©]**

```python
layers = []

# ì…ë ¥ì¸µ â†’ ì²« ì€ë‹‰ì¸µ
layers.append(nn.Linear(input_size, size))
layers.append(activation)

# ì€ë‹‰ì¸µë“¤
for _ in range(n_layers - 1):
    layers.append(nn.Linear(size, size))
    layers.append(activation)

# ì¶œë ¥ì¸µ
layers.append(nn.Linear(size, output_size))
layers.append(output_activation)

return nn.Sequential(*layers)
```

**nn.Linearë€?**
```python
class Linear(nn.Module):
    def forward(self, x):
        return x @ self.weight.T + self.bias
```
- ì„ í˜• ë³€í™˜: y = Wx + b
- W: (output_size, input_size) í–‰ë ¬
- b: (output_size,) ë²¡í„°

**êµ¬ì¡° ì˜ˆì‹œ:**
n_layers=2, input_size=17, size=64, output_size=8, activation=tanh

```
Layer 1: Linear(17, 64)  â†’ [batch, 17] â†’ [batch, 64]
         Tanh()          â†’ [batch, 64] â†’ [batch, 64]
Layer 2: Linear(64, 64)  â†’ [batch, 64] â†’ [batch, 64]
         Tanh()          â†’ [batch, 64] â†’ [batch, 64]
Output:  Linear(64, 8)   â†’ [batch, 64] â†’ [batch, 8]
         Identity()      â†’ [batch, 8]  â†’ [batch, 8]
```

**nn.Sequential():**
- ë ˆì´ì–´ë“¤ì„ ìˆœì„œëŒ€ë¡œ ì‹¤í–‰
- `*layers`: ë¦¬ìŠ¤íŠ¸ë¥¼ positional argumentsë¡œ ì–¸íŒ©
- ì‚¬ìš© ì˜ˆ: `output = model(input)` â†’ ìˆœì„œëŒ€ë¡œ í†µê³¼

**ì™œ Tanhë¥¼ ê¸°ë³¸ê°’ìœ¼ë¡œ?**
- ë²”ìœ„: [-1, 1] â†’ í–‰ë™ì´ ë³´í†µ normalize ë¼ìˆì–´
- Zero-centered: í‰ê· ì´ 0 ê·¼ì²˜ â†’ gradient flow ì¢‹ìŒ
- ReLUë³´ë‹¤ ë¶€ë“œëŸ¬ì›€

**ì™œ ì¶œë ¥ì¸µì€ Identity?**
- ì—°ì† í–‰ë™: unbounded ì¶œë ¥ í•„ìš”
- ë‚˜ì¤‘ì— í‰ê· (mean)ìœ¼ë¡œ ì‚¬ìš©í•  ê±°ë¼ ì œí•œ ë¶ˆí•„ìš”

---

## 8. ë©”ì¸ í•™ìŠµ ë£¨í”„: run_training_loop()

`bc_trainer.py`ì˜ 108-181ë²ˆ ì¤„

### 8.1 ì´ˆê¸°í™”

```python
def run_training_loop(
    self,
    n_iter,
    collect_policy,
    eval_policy,
    initial_expertdata=None,
    relabel_with_expert=False,
    start_relabel_with_expert=1,
    expert_policy=None,
):
    self.total_envsteps = 0
    self.start_time = time.time()
```

**time.time():**
- í˜„ì¬ ì‹œê°ì„ ì´ˆ ë‹¨ìœ„ë¡œ ë°˜í™˜ (Unix timestamp)
- ë‚˜ì¤‘ì— `time.time() - self.start_time`ìœ¼ë¡œ ê²½ê³¼ ì‹œê°„ ê³„ì‚°

### 8.2 Iteration ë£¨í”„

```python
for itr in range(n_iter):
    print("\n\n********** Iteration %i ************" % itr)
```

**string formatting:**
- `%i`: ì •ìˆ˜ placeholder
- ì˜ˆ: itr=0ì´ë©´ "Iteration 0" ì¶œë ¥
- ë‹¤ë¥¸ ë°©ë²•: f"Iteration {itr}" (Python 3.6+)

### 8.3 ë¡œê¹… ë¹ˆë„ ì œì–´

```python
if (itr % self.params["video_log_freq"] == 0 and
    self.params["video_log_freq"] != -1):
    self.log_video = True
else:
    self.log_video = False
```

**% ì—°ì‚°ì (modulo):**
- `itr % video_log_freq`: itrì„ video_log_freqë¡œ ë‚˜ëˆˆ ë‚˜ë¨¸ì§€
- ì˜ˆ: video_log_freq=5ì¼ ë•Œ
  - itr=0: 0%5=0 â†’ True (ë¹„ë””ì˜¤ ì €ì¥)
  - itr=1: 1%5=1 â†’ False
  - itr=5: 5%5=0 â†’ True (ë¹„ë””ì˜¤ ì €ì¥)
  - itr=10: 10%5=0 â†’ True

**ì™œ ì´ë ‡ê²Œ í•´?**
- ë§¤ iterationë§ˆë‹¤ ë¹„ë””ì˜¤ ì €ì¥í•˜ë©´ ìš©ëŸ‰â†‘
- ì£¼ê¸°ì ìœ¼ë¡œë§Œ ì €ì¥í•´ì„œ íš¨ìœ¨â†‘
- `-1`: ë¹„ë””ì˜¤ ì•ˆ ì €ì¥

```python
if itr % self.params["scalar_log_freq"] == 0:
    self.log_metrics = True
```

**scalar vs video:**
- scalar: ìˆ«ì ë©”íŠ¸ë¦­ (loss, reward ë“±)
- video: ì‹¤ì œ í”Œë ˆì´ ì˜ìƒ
- ë³´í†µ scalarëŠ” ë§¤ë²ˆ, videoëŠ” ê°€ë”

### 8.4 í•™ìŠµ ë°ì´í„° ìˆ˜ì§‘

```python
training_returns = self.collect_training_trajectories(
    itr, collect_policy, initial_expertdata
)
trajs, envsteps_this_batch, train_video_trajs = training_returns
self.total_envsteps += envsteps_this_batch
```

**ë°˜í™˜ê°’ ì–¸íŒ¨í‚¹:**
- `training_returns`ëŠ” íŠœí”Œ: (trajs, envsteps, videos)
- í•œ ì¤„ì— 3ê°œ ë³€ìˆ˜ì— í• ë‹¹

**self.total_envsteps:**
- ëˆ„ì  í™˜ê²½ step ìˆ˜
- í•™ìŠµ progress ì¶”ì ìš©

### 8.5 ì „ë¬¸ê°€ ë¼ë²¨ë§ (DAgger)

```python
if relabel_with_expert and itr >= start_relabel_with_expert:
    trajs = self.do_relabel_with_expert(expert_policy, trajs)
```

**ì¡°ê±´:**
- `relabel_with_expert=True`: DAgger ëª¨ë“œ
- `itr >= start_relabel_with_expert`: íŠ¹ì • iterationë¶€í„° ì‹œì‘
  - ê¸°ë³¸ê°’ 1: iteration 0ì€ ì „ë¬¸ê°€ ë°ì´í„° ê·¸ëŒ€ë¡œ, 1ë¶€í„° relabel

**ì™œ ë‚˜ì¤‘ë¶€í„° relabel?**
- Iteration 0: ì „ë¬¸ê°€ ë°ì´í„°ë¡œ warm-start
- Iteration 1+: í•™ìŠµí•œ ì •ì±…ìœ¼ë¡œ ë°ì´í„° ìˆ˜ì§‘ â†’ relabel

### 8.6 ë²„í¼ì— ì¶”ê°€ ë° í•™ìŠµ

```python
self.agent.add_to_replay_buffer(trajs)
training_logs = self.train_agent()
```

**ìˆœì„œê°€ ì¤‘ìš”í•´:**
1. ë²„í¼ì— ìƒˆ ë°ì´í„° ì¶”ê°€
2. ë²„í¼ì—ì„œ ìƒ˜í”Œë§í•´ì„œ í•™ìŠµ
3. ì´ì „ ë°ì´í„°ë„ í•¨ê»˜ í•™ìŠµ (replay)

### 8.7 ë¡œê¹… ë° ì €ì¥

```python
if self.log_video or self.log_metrics:
    print("\nBeginning logging procedure...")
    self.perform_logging(itr, trajs, eval_policy, train_video_trajs, training_logs)

    if self.params["save_params"]:
        print("\nSaving agent params")
        self.agent.save("{}/policy_itr_{}.pt".format(self.params["logdir"], itr))
```

**íŒŒì¼ëª… í¬ë§·:**
- `policy_itr_0.pt`: iteration 0ì˜ ì •ì±…
- `.pt`: PyTorch ëª¨ë¸ íŒŒì¼ í™•ì¥ì

**ì™œ ë§¤ iterationë§ˆë‹¤ ì €ì¥?**
- ì¤‘ê°„ ì²´í¬í¬ì¸íŠ¸: ë‚˜ì¤‘ì— íŠ¹ì • iteration ëª¨ë¸ ë¡œë“œ ê°€ëŠ¥
- í•™ìŠµ ê³¼ì • ì¶”ì 
- ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ë³´ì¡´

---

## 9. ë°ì´í„° ìˆ˜ì§‘: collect_training_trajectories()

`bc_trainer.py`ì˜ 186-222ë²ˆ ì¤„

### 9.1 í•¨ìˆ˜ ë¡œì§

```python
def collect_training_trajectories(
    self, itr, collect_policy, load_initial_expertdata=None
):
```

**ë‘ ê°€ì§€ ê²½ë¡œ:**

**ê²½ë¡œ 1: ì „ë¬¸ê°€ ë°ì´í„° ë¡œë“œ (itr == 0)**
```python
if itr == 0 and load_initial_expertdata is not None:
    with open(load_initial_expertdata, 'rb') as f:
        loaded_trajs = pickle.load(f)
    return loaded_trajs, 0, None
```

**pickleì´ë€?**
- Python ê°ì²´ë¥¼ íŒŒì¼ë¡œ ì§ë ¬í™”(serialize)
- `pickle.dump(obj, file)`: ì €ì¥
- `pickle.load(file)`: ë¡œë“œ
- ì£¼ì˜: ë³´ì•ˆ ìœ„í—˜ (ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” íŒŒì¼ë§Œ!)

**with ë¬¸:**
```python
with open(file, mode) as f:
    # f ì‚¬ìš©
# ìë™ìœ¼ë¡œ f.close() í˜¸ì¶œ
```
- **Context manager**: ìì› ê´€ë¦¬ ìë™í™”
- íŒŒì¼ ë‹«ê¸° ë³´ì¥ (ì—ëŸ¬ë‚˜ë„)

**ê²½ë¡œ 2: í™˜ê²½ì—ì„œ ë°ì´í„° ìˆ˜ì§‘**

**[êµ¬í˜„ ì˜ˆì‹œ - í•™ìŠµ ì°¸ê³ ìš©]**

```python
if itr == 0:
    batch_size = self.params['batch_size_initial']
else:
    batch_size = self.params['batch_size']

trajs, envsteps = utils.rollout_trajectories(
    self.env,
    collect_policy,
    batch_size,
    self.params['ep_len']
)
```

**batch_size ì°¨ì´:**
- `batch_size_initial`: ì²« ìˆ˜ì§‘ (ë³´í†µ ë” ë§ì´)
- `batch_size`: ì´í›„ ìˆ˜ì§‘ (ì ë‹¹íˆ)
- ì™œ? ì´ˆê¸°ì—” ë°ì´í„° ë§ì´ í•„ìš”, ì´í›„ì—” ì ì§„ì 

### 9.2 ë¹„ë””ì˜¤ ìˆ˜ì§‘

```python
train_video_trajs = None
if self.log_video:
    print("\nCollecting train rollouts to be used for saving videos...")
    train_video_trajs = utils.rollout_n_trajectories(
        self.env,
        collect_policy,
        MAX_NVIDEO,
        MAX_VIDEO_LEN,
        render=True
    )
```

**MAX_NVIDEO, MAX_VIDEO_LEN:**
```python
MAX_NVIDEO = 2
MAX_VIDEO_LEN = 40  # ë‚˜ì¤‘ì— ep_lenìœ¼ë¡œ ë®ì–´ì”€
```

- 2ê°œì˜ trajectoryë§Œ ë¹„ë””ì˜¤ë¡œ ì €ì¥
- ê° ìµœëŒ€ 40 steps
- `render=True`: ì´ë¯¸ì§€ ë Œë”ë§ í™œì„±í™”

**rollout_n_trajectories vs rollout_trajectories:**
- `rollout_n_trajectories`: ì •í™•íˆ Nê°œ trajectory
- `rollout_trajectories`: ìµœì†Œ Nê°œ timesteps

---

## 10. í™˜ê²½ ìƒí˜¸ì‘ìš©: rollout_trajectory()

`utils.py`ì˜ 21-68ë²ˆ ì¤„

### 10.1 í™˜ê²½ ì´ˆê¸°í™”

**[êµ¬í˜„ ì˜ˆì‹œ - í•™ìŠµ ì°¸ê³ ìš©]**

```python
def rollout_trajectory(env, policy, max_traj_length, render=False):
    ob, _ = env.reset()
```

**env.reset() ë°˜í™˜ê°’:**
- Gymnasium (ìƒˆ ë²„ì „): `(observation, info)` íŠœí”Œ
- `_`: infoëŠ” ì•ˆ ì“°ë‹ˆê¹Œ ì–¸ë”ìŠ¤ì½”ì–´ë¡œ ë¬´ì‹œ

**observationì´ë€?**
- ì—ì´ì „íŠ¸ê°€ ê´€ì°°í•˜ëŠ” í™˜ê²½ ìƒíƒœ
- ì˜ˆì‹œ (Ant-v4):
  - ê´€ì ˆ ê°ë„: 8ì°¨ì›
  - ê´€ì ˆ ì†ë„: 8ì°¨ì›
  - ê¸°íƒ€: 1ì°¨ì›
  - ì´ 17ì°¨ì› ë²¡í„°

### 10.2 ë°ì´í„° ì €ì¥ ë¦¬ìŠ¤íŠ¸

```python
obs, acs, rewards, next_obs, terminals, image_obs = [], [], [], [], [], []
steps = 0
```

**ê° ë¦¬ìŠ¤íŠ¸ ì—­í• :**
- `obs`: ê° stepì˜ observation
- `acs`: ê° stepì˜ action
- `rewards`: ê° stepì˜ reward
- `next_obs`: ê° stepì˜ ë‹¤ìŒ observation
- `terminals`: ê° stepì˜ ì¢…ë£Œ ì—¬ë¶€
- `image_obs`: ê° stepì˜ ë Œë”ë§ ì´ë¯¸ì§€ (ì„ íƒ)

### 10.3 ë©”ì¸ ë£¨í”„

```python
while True:
    if render:
        if hasattr(env, "sim"):
            image_obs.append(
                env.sim.render(camera_name="track", height=500, width=500)[::-1]
            )
        else:
            image_obs.append(env.render())
```

**hasattr() í•¨ìˆ˜:**
- ê°ì²´ì— íŠ¹ì • ì†ì„±ì´ ìˆëŠ”ì§€ í™•ì¸
- `hasattr(env, "sim")`: envì— sim ì†ì„± ìˆë‚˜?

**MuJoCo ë Œë”ë§:**
- `env.sim.render()`: MuJoCo ì‹œë®¬ë ˆì´í„° ì§ì ‘ ë Œë”ë§
- `camera_name="track"`: ì¶”ì  ì¹´ë©”ë¼ ì‚¬ìš©
- `[::-1]`: ì´ë¯¸ì§€ ìƒí•˜ ë°˜ì „ (OpenGL ì¢Œí‘œê³„ ë•Œë¬¸)

**ë‹¤ë¥¸ í™˜ê²½:**
- `env.render()`: í‘œì¤€ ë Œë”ë§ ë©”ì„œë“œ

### 10.4 í–‰ë™ ì„ íƒ

```python
obs.append(ob)
ac = policy.get_action(ob)
acs.append(ac)
```

**policy.get_action(ob):**
1. obë¥¼ torch tensorë¡œ ë³€í™˜
2. ì‹ ê²½ë§ forward pass
3. í–‰ë™ ë¶„í¬ì—ì„œ ìƒ˜í”Œë§
4. numpy arrayë¡œ ë°˜í™˜

**ì˜ˆì‹œ ê°’:**
- ob: `[0.1, -0.3, 0.5, ..., 0.2]` (17ì°¨ì›)
- ac: `[0.05, -0.15, 0.02, ..., -0.1]` (8ì°¨ì›)

### 10.5 í™˜ê²½ step

```python
ob, rew, terminated, truncated, _ = env.step(ac)
```

**env.step() ë°˜í™˜ê°’:**
- `ob`: ë‹¤ìŒ observation (s')
- `rew`: ì¦‰ì‹œ ë³´ìƒ (r)
- `terminated`: í™˜ê²½ì´ ì¢…ë£Œ ìƒíƒœ ë„ë‹¬ (ì˜ˆ: ë¡œë´‡ ë„˜ì–´ì§)
- `truncated`: ì‹œê°„ ì œí•œ ë„ë‹¬
- `_`: info (ì•ˆ ì”€)

**terminated vs truncated (Gymnasium):**
- terminated: ì§„ì§œ ë (ì„±ê³µ or ì‹¤íŒ¨)
- truncated: ì‹œê°„ ë‹¤ ë¨
- ì™œ êµ¬ë¶„? ê°€ì¹˜ í•¨ìˆ˜ ê³„ì‚° ë•Œ ë‹¤ë¥´ê²Œ ì²˜ë¦¬

### 10.6 ê²°ê³¼ ê¸°ë¡

```python
steps += 1
next_obs.append(ob)
rewards.append(rew)

rollout_done = (terminated or truncated) or (steps >= max_traj_length)
terminals.append(rollout_done)

if rollout_done:
    break
```

**ì¢…ë£Œ ì¡°ê±´ 3ê°€ì§€:**
1. `terminated`: í™˜ê²½ì´ ëë‚¨
2. `truncated`: ì‹œê°„ ì œí•œ
3. `steps >= max_traj_length`: ê°•ì œ ì¢…ë£Œ

**terminals ê°’:**
- ì¤‘ê°„: 0 (ë˜ëŠ” False)
- ë§ˆì§€ë§‰: 1 (ë˜ëŠ” True)

### 10.7 Trajectory ë°˜í™˜

```python
return Traj(obs, image_obs, acs, rewards, next_obs, terminals)
```

**Traj í•¨ìˆ˜:**
```python
def Traj(obs, image_obs, acs, rewards, next_obs, terminals):
    if image_obs != []:
        image_obs = np.stack(image_obs, axis=0)
    return {
        "observation": np.array(obs, dtype=np.float32),
        "image_obs": np.array(image_obs, dtype=np.uint8),
        "reward": np.array(rewards, dtype=np.float32),
        "action": np.array(acs, dtype=np.float32),
        "next_observation": np.array(next_obs, dtype=np.float32),
        "terminal": np.array(terminals, dtype=np.float32),
    }
```

**np.stack():**
- ë¦¬ìŠ¤íŠ¸ì˜ ë°°ì—´ë“¤ì„ ìƒˆ ì°¨ì›ìœ¼ë¡œ ìŒ“ìŒ
- `axis=0`: ì²« ë²ˆì§¸ ì°¨ì›ì— ìŒ“ê¸°
- ì˜ˆ: [(H,W,3), (H,W,3), ...] â†’ (T,H,W,3)

**dtype ì„¤ëª…:**
- `float32`: ë©”ëª¨ë¦¬ ì ˆì•½ (float64ë³´ë‹¤ ì ˆë°˜)
- `uint8`: ì´ë¯¸ì§€ (0-255 ë²”ìœ„)

**ë°˜í™˜ ë”•ì…”ë„ˆë¦¬ shape:**
```python
{
    "observation": (T, ob_dim),      # ì˜ˆ: (100, 17)
    "action": (T, ac_dim),           # ì˜ˆ: (100, 8)
    "reward": (T,),                  # ì˜ˆ: (100,)
    "next_observation": (T, ob_dim),
    "terminal": (T,),
    "image_obs": (T, H, W, 3) or []
}
```

### 10.8 rollout_trajectories êµ¬í˜„

**[êµ¬í˜„ ì˜ˆì‹œ - í•™ìŠµ ì°¸ê³ ìš©]**

```python
def rollout_trajectories(env, policy, min_timesteps_per_batch, max_traj_length, render=False):
    timesteps_this_batch = 0
    trajs = []

    while timesteps_this_batch < min_timesteps_per_batch:
        traj = rollout_trajectory(env, policy, max_traj_length, render)
        trajs.append(traj)
        timesteps_this_batch += len(traj["reward"])

    return trajs, timesteps_this_batch
```

**ë¡œì§:**
1. timesteps ì¹´ìš´í„° ì´ˆê¸°í™”
2. ëª©í‘œ timesteps ë„ë‹¬ê¹Œì§€ ë°˜ë³µ
3. ê° trajectory ìˆ˜ì§‘
4. timesteps ëˆ„ì 
5. trajs ë¦¬ìŠ¤íŠ¸ì™€ ì´ timesteps ë°˜í™˜

**ì™œ trajectory ê°œìˆ˜ê°€ ì•„ë‹ˆë¼ timestepsë¡œ?**
- Trajectoryë§ˆë‹¤ ê¸¸ì´ê°€ ë‹¤ë¦„
- í•™ìŠµ ë°ì´í„° ì–‘ì„ ì¼ì •í•˜ê²Œ ìœ ì§€
- ì˜ˆ: batch_size=1000
  - ì§§ì€ trajs: 20ê°œ í•„ìš” (ê° 50 steps)
  - ê¸´ trajs: 10ê°œ í•„ìš” (ê° 100 steps)

### 10.9 rollout_n_trajectories êµ¬í˜„

**[êµ¬í˜„ ì˜ˆì‹œ - í•™ìŠµ ì°¸ê³ ìš©]**

```python
def rollout_n_trajectories(env, policy, ntraj, max_traj_length, render=False):
    trajs = []
    for _ in range(ntraj):
        traj = rollout_trajectory(env, policy, max_traj_length, render)
        trajs.append(traj)
    return trajs
```

**ê°„ë‹¨í•œ ë°˜ë³µ:**
- ì •í™•íˆ ntrajê°œ ìˆ˜ì§‘
- ê¸¸ì´ ìƒê´€ì—†ì´ ê°œìˆ˜ë§Œ ì¤‘ìš”
- ë¹„ë””ì˜¤/í‰ê°€ìš©

---

## 11. ì „ë¬¸ê°€ ë¼ë²¨ë§: do_relabel_with_expert()

`bc_trainer.py`ì˜ 249-264ë²ˆ ì¤„

### 11.1 DAggerì˜ í•µì‹¬ ì•„ì´ë””ì–´

**ë¬¸ì œ: Distributional Shift**
- í•™ìŠµ ì¤‘ì¸ ì •ì±…ì´ ì „ë¬¸ê°€ì™€ ë‹¤ë¥¸ í–‰ë™ â†’ ì „ë¬¸ê°€ê°€ ì•ˆ ê°€ë³¸ ìƒíƒœ ë„ë‹¬
- ê·¸ ìƒíƒœì—ì„œ ë­˜ í•´ì•¼ í• ì§€ ëª¨ë¦„
- ì ì  ë” ì´ìƒí•œ ìƒíƒœë¡œ...

**í•´ê²°: DAgger**
- í•™ìŠµ ì •ì±…ì´ ë°©ë¬¸í•œ ìƒíƒœì—ì„œ ì „ë¬¸ê°€ì—ê²Œ ë¬¼ì–´ë´„
- "ë‚´ê°€ ì—¬ê¸° ì™”ëŠ”ë°, ë‹¹ì‹ ì´ë¼ë©´ ë­˜ í•  ê±´ê°€ìš”?"
- ê·¸ ë‹µë³€ìœ¼ë¡œ í•™ìŠµ ë°ì´í„° ë³´ê°•

### 11.2 êµ¬í˜„

**[êµ¬í˜„ ì˜ˆì‹œ - í•™ìŠµ ì°¸ê³ ìš©]**

```python
def do_relabel_with_expert(self, expert_policy, trajs):
    print("\nRelabelling collected observations with labels from an expert policy...")

    for i in range(len(trajs)):
        observations = trajs[i]["observation"]
        expert_actions = expert_policy.get_action(observations)
        trajs[i]["action"] = expert_actions

    return trajs
```

**ê³¼ì •:**
1. ê° trajectory ìˆœíšŒ
2. observation ì¶”ì¶œ (í•™ìŠµ ì •ì±…ì´ ë°©ë¬¸í•œ ìƒíƒœ)
3. expert_policy.get_action() í˜¸ì¶œ
4. ê¸°ì¡´ actionì„ expert actionìœ¼ë¡œ êµì²´

**ì˜ˆì‹œ:**
```python
# ì›ë³¸ trajectory (í•™ìŠµ ì •ì±…ì´ ìˆ˜ì§‘)
traj = {
    "observation": [[0.1, 0.2, ...], [0.3, 0.1, ...], ...],
    "action": [[0.5, -0.2, ...], [0.1, 0.3, ...], ...],  # í•™ìŠµ ì •ì±…ì˜ í–‰ë™
}

# ì „ë¬¸ê°€ ë¼ë²¨ë§ í›„
expert_actions = expert_policy.get_action(traj["observation"])
# expert_actions = [[0.3, -0.1, ...], [0.05, 0.2, ...], ...]

traj["action"] = expert_actions  # ì „ë¬¸ê°€ í–‰ë™ìœ¼ë¡œ êµì²´
```

### 11.3 ì™œ ì´ê²Œ íš¨ê³¼ì ì¸ê°€?

**ìˆ˜í•™ì  ì„¤ëª…:**
- BC: D_expert = {(s, a) | s ~ Ï€_expert}ì—ì„œ í•™ìŠµ
  - ì „ë¬¸ê°€ê°€ ë°©ë¬¸í•œ ìƒíƒœë§Œ
- DAgger: D_agg = {(s, a) | s ~ Ï€_learner, a = Ï€_expert(s)}ë¡œ í™•ì¥
  - í•™ìŠµ ì •ì±…ì´ ë°©ë¬¸í•œ ìƒíƒœ + ì „ë¬¸ê°€ì˜ ì •ë‹µ

**ë°˜ë³µ í•™ìŠµ:**
```
Iteration 0: ì „ë¬¸ê°€ ë°ì´í„°ë¡œ í•™ìŠµ â†’ Ï€â‚€
Iteration 1: Ï€â‚€ë¡œ ë°ì´í„° ìˆ˜ì§‘ â†’ ì „ë¬¸ê°€ê°€ ë¼ë²¨ë§ â†’ í•™ìŠµ â†’ Ï€â‚
Iteration 2: Ï€â‚ë¡œ ë°ì´í„° ìˆ˜ì§‘ â†’ ì „ë¬¸ê°€ê°€ ë¼ë²¨ë§ â†’ í•™ìŠµ â†’ Ï€â‚‚
...
```

**ìˆ˜ë ´:**
- Ï€ê°€ ì „ë¬¸ê°€ì— ê°€ê¹Œì›Œì§ˆìˆ˜ë¡ ë°©ë¬¸ ìƒíƒœ ë¶„í¬ë„ ë¹„ìŠ·í•´ì§
- ê²°êµ­ distributional shift ê°ì†Œ

---

## 12. ê²½í—˜ ì €ì¥: ReplayBuffer.add_rollouts()

`replay_buffer.py`ì˜ 59-98ë²ˆ ì¤„

### 12.1 Trajectory ì¶”ê°€

```python
for traj in trajs:
    self.trajs.append(traj)
```

**self.trajs:**
- ì „ì²´ trajectoryë¥¼ ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ì €ì¥
- ë‚˜ì¤‘ì— ë¶„ì„ì´ë‚˜ ì¬ì‚¬ìš© ê°€ëŠ¥

### 12.2 ì„±ë¶„ë³„ ë³€í™˜

```python
observations, actions, rewards, next_observations, terminals = (
    convert_listofrollouts(trajs, concat_rew)
)
```

**convert_listofrollouts() í•¨ìˆ˜:**
```python
def convert_listofrollouts(trajs, concat_rew=True):
    observations = np.concatenate([traj["observation"] for traj in trajs])
    actions = np.concatenate([traj["action"] for traj in trajs])
    if concat_rew:
        rewards = np.concatenate([traj["reward"] for traj in trajs])
    else:
        rewards = [traj["reward"] for traj in trajs]
    next_observations = np.concatenate([traj["next_observation"] for traj in trajs])
    terminals = np.concatenate([traj["terminal"] for traj in trajs])
    return observations, actions, rewards, next_observations, terminals
```

**np.concatenate() ìì„¸íˆ:**
- ì—¬ëŸ¬ ë°°ì—´ì„ ì²« ë²ˆì§¸ ì°¨ì›(axis=0)ì„ ë”°ë¼ ì—°ê²°
- ì˜ˆì‹œ:
```python
traj1["observation"]: (50, 17)
traj2["observation"]: (30, 17)
traj3["observation"]: (40, 17)

concatenated: (120, 17)  # 50+30+40 = 120
```

**List comprehension:**
- `[traj["observation"] for traj in trajs]`
- trajsì˜ ê° trajì—ì„œ "observation" ì¶”ì¶œí•˜ì—¬ ë¦¬ìŠ¤íŠ¸ ìƒì„±

**concat_rew íŒŒë¼ë¯¸í„°:**
- True: rewardë¥¼ í•˜ë‚˜ì˜ ë°°ì—´ë¡œ í•©ì¹¨
- False: rewardë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ìœ ì§€ (trajectoryë³„ë¡œ)
- ì—¬ê¸°ì„  True (BCëŠ” reward ì•ˆ ì”€)

### 12.3 ë²„í¼ ì´ˆê¸°í™” ë˜ëŠ” ì¶”ê°€

**ì²« ì¶”ê°€ (ë²„í¼ ë¹„ì–´ìˆìŒ):**
```python
if self.obs is None:
    self.obs = observations[-self.max_size :]
    self.acs = actions[-self.max_size :]
    self.rews = rewards[-self.max_size :]
    self.next_obs = next_observations[-self.max_size :]
    self.terminals = terminals[-self.max_size :]
```

**[-self.max_size:]ë€?**
- ë°°ì—´ì˜ ë§ˆì§€ë§‰ max_sizeê°œ ìš”ì†Œë§Œ ê°€ì ¸ì˜¤ê¸°
- ì˜ˆ: max_size=1000000
  - observationsê°€ 500000ê°œë©´? â†’ 500000ê°œ ì „ë¶€
  - observationsê°€ 1500000ê°œë©´? â†’ ë§ˆì§€ë§‰ 1000000ê°œë§Œ
- ì™œ? ë²„í¼ í¬ê¸° ì œí•œ ë¯¸ë¦¬ ì ìš©

**ê¸°ì¡´ ë°ì´í„° ìˆìŒ:**
```python
else:
    self.obs = np.concatenate([self.obs, observations])[-self.max_size :]
    self.acs = np.concatenate([self.acs, actions])[-self.max_size :]
    # ... ë‚˜ë¨¸ì§€ë„ ë™ì¼
```

**FIFO (First In First Out):**
1. ê¸°ì¡´ ë°ì´í„° + ìƒˆ ë°ì´í„° í•©ì¹¨
2. ë§ˆì§€ë§‰ max_sizeê°œë§Œ ìœ ì§€
3. ì˜¤ë˜ëœ ë°ì´í„° ìë™ ì œê±°

**ì˜ˆì‹œ:**
```python
# ê¸°ì¡´ ë²„í¼
self.obs: (800000, 17)

# ìƒˆ ë°ì´í„°
observations: (300000, 17)

# í•©ì¹˜ê¸°
concatenated: (1100000, 17)

# max_size = 1000000
self.obs = concatenated[-1000000:]  # (1000000, 17)
# ì•ì˜ 100000ê°œ ì œê±°ë¨
```

### 12.4 __len__ ë©”ì„œë“œ

```python
def __len__(self):
    if self.obs is not None:
        return self.obs.shape[0]
    else:
        return 0
```

**__len__ì€ íŠ¹ë³„ ë©”ì„œë“œ:**
- `len(buffer)` í˜¸ì¶œ ì‹œ ì‹¤í–‰
- ë²„í¼ì˜ transition ê°œìˆ˜ ë°˜í™˜
- `.shape[0]`: ì²« ë²ˆì§¸ ì°¨ì› í¬ê¸°

---

## 13. ì—ì´ì „íŠ¸ í•™ìŠµ: train_agent()

`bc_trainer.py`ì˜ 224-247ë²ˆ ì¤„

### 13.1 í•™ìŠµ ë£¨í”„

**[êµ¬í˜„ ì˜ˆì‹œ - í•™ìŠµ ì°¸ê³ ìš©]**

```python
def train_agent(self):
    print("\nTraining agent using sampled data from replay buffer...")
    all_logs = []

    for train_step in range(self.params["num_agent_train_steps_per_iter"]):
        ob_batch, ac_batch, re_batch, next_ob_batch, terminal_batch = (
            self.agent.sample(self.params["train_batch_size"])
        )

        train_log = self.agent.train(ob_batch, ac_batch)
        all_logs.append(train_log)

    return all_logs
```

**num_agent_train_steps_per_iter:**
- í•œ iterationì— ëª‡ ë²ˆ gradient step í• ì§€
- ì˜ˆ: 1000ì´ë©´ 1000ë²ˆ íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸
- ë§ì„ìˆ˜ë¡: í•™ìŠµ ì¶©ë¶„, ì‹œê°„â†‘
- ì ì„ìˆ˜ë¡: ë¹ ë¦„, í•™ìŠµ ë¶€ì¡±

**train_batch_size:**
- ê° gradient stepì—ì„œ ì‚¬ìš©í•  ìƒ˜í”Œ ê°œìˆ˜
- ì˜ˆ: 100ì´ë©´ ë²„í¼ì—ì„œ 100ê°œ transition ìƒ˜í”Œë§
- í¬ë©´: ì•ˆì •ì , ë©”ëª¨ë¦¬â†‘
- ì‘ìœ¼ë©´: ë¹ ë¦„, ë…¸ì´ì¦ˆâ†‘

### 13.2 BCAgent.sample()

```python
def sample(self, batch_size):
    return self.replay_buffer.sample_random_data(batch_size)
```

**ë°˜í™˜ê°’:**
- ob_batch: (batch_size, ob_dim)
- ac_batch: (batch_size, ac_dim)
- re_batch: (batch_size,) - BCëŠ” ì•ˆ ì”€
- next_ob_batch: (batch_size, ob_dim) - BCëŠ” ì•ˆ ì”€
- terminal_batch: (batch_size,) - BCëŠ” ì•ˆ ì”€

### 13.3 BCAgent.train()

```python
def train(self, ob_no, ac_na):
    log = self.actor.update(ob_no, ac_na)
    return log
```

**ë³€ìˆ˜ëª… ê·œì¹™ ë³µìŠµ:**
- ob_no: observation, (n)umber Ã— (o)bservation dimension
- ac_na: action, (n)umber Ã— (a)ction dimension

**ë‹¨ìˆœ ìœ„ì„:**
- BCAgentëŠ” actor(ì •ì±…)ì—ê²Œ ì—…ë°ì´íŠ¸ ë§¡ê¹€
- ì‹¤ì œ í•™ìŠµì€ MLPPolicySL.update()ì—ì„œ

---

## 14. ë°ì´í„° ìƒ˜í”Œë§: ReplayBuffer.sample_random_data()

`replay_buffer.py`ì˜ 103-132ë²ˆ ì¤„

### 14.1 êµ¬í˜„

**[êµ¬í˜„ ì˜ˆì‹œ - í•™ìŠµ ì°¸ê³ ìš©]**

```python
def sample_random_data(self, batch_size):
    assert (
        self.obs.shape[0]
        == self.acs.shape[0]
        == self.rews.shape[0]
        == self.next_obs.shape[0]
        == self.terminals.shape[0]
    )

    indices = np.random.choice(
        len(self.obs),
        size=batch_size,
        replace=False
    )

    return (
        self.obs[indices],
        self.acs[indices],
        self.rews[indices],
        self.next_obs[indices],
        self.terminals[indices]
    )
```

### 14.2 Assert ê²€ì¦

```python
assert (
    self.obs.shape[0] == self.acs.shape[0] == ...
)
```

**ì—°ì‡„ ë¹„êµ:**
- Pythonì˜ í¸ë¦¬í•œ ë¬¸ë²•
- ëª¨ë“  ë°°ì—´ì˜ ì²« ì°¨ì› í¬ê¸°ê°€ ê°™ì€ì§€ í™•ì¸
- ë°ì´í„° ì •í•©ì„± ê²€ì¦

**ì™œ í•„ìš”?**
- ë²„ê·¸ ì¡°ê¸° ë°œê²¬
- ì˜ˆ: obs 1000ê°œ, acs 999ê°œ â†’ ì—ëŸ¬ ë°œìƒ
- ë””ë²„ê¹… ì‹œê°„ ì ˆì•½

### 14.3 ë¬´ì‘ìœ„ ì¸ë±ìŠ¤ ìƒ˜í”Œë§

```python
indices = np.random.choice(
    len(self.obs),
    size=batch_size,
    replace=False
)
```

**np.random.choice() ìƒì„¸:**
- ì²« ì¸ì: 0ë¶€í„° len(self.obs)-1ê¹Œì§€ ë²”ìœ„
- size: ëª‡ ê°œ ìƒ˜í”Œë§í• ì§€
- replace: ë³µì› ì¶”ì¶œ ì—¬ë¶€
  - False: ì¤‘ë³µ ì—†ì´ (without replacement)
  - True: ì¤‘ë³µ í—ˆìš© (with replacement)

**ì˜ˆì‹œ:**
```python
len(self.obs) = 10000
batch_size = 100

# indices: [3472, 189, 7834, 215, ..., 9001]  (100ê°œ)
# 0~9999 ë²”ìœ„ì—ì„œ ë¬´ì‘ìœ„ë¡œ 100ê°œ, ì¤‘ë³µ ì—†ìŒ
```

### 14.4 ì™œ replace=Falseì¸ê°€?

**Without replacement (replace=False):**
- ê°™ì€ ìƒ˜í”Œ ì—¬ëŸ¬ ë²ˆ ì•ˆ ë½‘í˜
- í•œ ë°°ì¹˜ ë‚´ ë‹¤ì–‘ì„± ë³´ì¥
- ì¼ë°˜ì ìœ¼ë¡œ ê¶Œì¥

**With replacement (replace=True):**
- ê°™ì€ ìƒ˜í”Œ ì—¬ëŸ¬ ë²ˆ ë½‘í ìˆ˜ ìˆìŒ
- ë²„í¼ í¬ê¸° < batch_sizeì¼ ë•Œ í•„ìš”
- ë³´í†µ ì•ˆ ì”€

### 14.5 ì¸ë±ì‹±

```python
return (
    self.obs[indices],
    ...
)
```

**NumPy ê³ ê¸‰ ì¸ë±ì‹±:**
- ë°°ì—´ë¡œ ì¸ë±ì‹±í•˜ë©´ í•´ë‹¹ ì¸ë±ìŠ¤ë“¤ì˜ ìš”ì†Œ ì¶”ì¶œ
- ì˜ˆ:
```python
arr = np.array([10, 20, 30, 40, 50])
indices = np.array([0, 2, 4])
arr[indices]  # array([10, 30, 50])
```

**2D ë°°ì—´:**
```python
self.obs: (10000, 17)
indices: (100,)

self.obs[indices]: (100, 17)
# indicesì— í•´ë‹¹í•˜ëŠ” 100ê°œ í–‰ ì¶”ì¶œ
```

### 14.6 ì™œ ë¬´ì‘ìœ„ ìƒ˜í”Œë§ì¸ê°€?

**i.i.d. (Independent and Identically Distributed) ê°€ì •:**
- ë¨¸ì‹ ëŸ¬ë‹ì€ ë°ì´í„°ê°€ ë…ë¦½ì ì´ê³  ë™ì¼ ë¶„í¬ë¼ê³  ê°€ì •
- TrajectoryëŠ” ì‹œê°„ì  ìƒê´€ê´€ê³„ ìˆìŒ:
  - s_tì™€ s_{t+1}ì€ ì—°ê´€ë¨
  - ìˆœì°¨ì ìœ¼ë¡œ ìƒ˜í”Œë§í•˜ë©´ ìƒê´€ê´€ê³„ ìœ ì§€
- ë¬´ì‘ìœ„ ìƒ˜í”Œë§ìœ¼ë¡œ ìƒê´€ê´€ê³„ ì œê±°

**í•™ìŠµ ì•ˆì •ì„±:**
- ì—°ì†ëœ ìƒ˜í”Œ: gradient í¸í–¥
- ë¬´ì‘ìœ„ ìƒ˜í”Œ: ë‹¤ì–‘í•œ ìƒí™© í•™ìŠµ
- ì¼ë°˜í™” ì„±ëŠ¥â†‘

---

## 15. ì •ì±… ì—…ë°ì´íŠ¸: MLPPolicySL í•µì‹¬ ë©”ì„œë“œë“¤

### 15.1 forward() - í–‰ë™ ë¶„í¬ ìƒì„±

`MLP_policy.py`ì˜ 130-147ë²ˆ ì¤„

**[êµ¬í˜„ ì˜ˆì‹œ - í•™ìŠµ ì°¸ê³ ìš©]**

```python
def forward(self, observation: torch.FloatTensor) -> Any:
    mean = self.mean_net(observation)
    std = torch.exp(self.logstd)
    return torch.distributions.Normal(mean, std)
```

**ì…ë ¥:**
- observation: (batch_size, ob_dim) ë˜ëŠ” (ob_dim,) í…ì„œ
- ì˜ˆ: (100, 17) - 100ê°œ ìƒ˜í”Œ, 17ì°¨ì› ê´€ì°°

**mean_net(observation):**
- MLPë¥¼ í†µê³¼
- ì…ë ¥: (100, 17)
- ì¶œë ¥: (100, ac_dim) ì˜ˆ: (100, 8)
- ê° ìƒ˜í”Œë§ˆë‹¤ ac_dimì°¨ì› í‰ê·  ë²¡í„°

**torch.exp(self.logstd):**
- logstd: (ac_dim,) ì˜ˆ: (8,)
- exp() ì ìš©: í•­ìƒ ì–‘ìˆ˜
- std: (ac_dim,) ì˜ˆ: (8,)

**ì™œ expë¥¼ ì‚¬ìš©?**
```python
logstd = -2.0 â†’ std = exp(-2.0) â‰ˆ 0.135  (ì‘ì€ íƒìƒ‰)
logstd = 0.0  â†’ std = exp(0.0) = 1.0     (ë³´í†µ íƒìƒ‰)
logstd = 2.0  â†’ std = exp(2.0) â‰ˆ 7.39    (í° íƒìƒ‰)
```
- logstdëŠ” unbounded (-âˆ ~ +âˆ)
- stdëŠ” í•­ìƒ ì–‘ìˆ˜
- ìˆ˜ì¹˜ì  ì•ˆì •ì„±

**torch.distributions.Normal:**
```python
Normal(loc=mean, scale=std)
```
- loc: í‰ê·  (mean), shape (100, 8)
- scale: í‘œì¤€í¸ì°¨ (std), shape (8,)
- **Broadcasting**: stdê°€ ëª¨ë“  ìƒ˜í”Œì— ì ìš©ë¨

**Broadcasting ì˜ˆì‹œ:**
```python
mean: (100, 8)
std:  (8,)      # ìë™ìœ¼ë¡œ (100, 8)ë¡œ í™•ì¥

# ê° ìƒ˜í”Œë§ˆë‹¤ ë…ë¦½ì ì¸ ì •ê·œë¶„í¬
distribution[0]: N(mean[0], std)  # 8ì°¨ì› ë¶„í¬
distribution[1]: N(mean[1], std)  # 8ì°¨ì› ë¶„í¬
...
```

**ë°˜í™˜ Distribution ê°ì²´:**
```python
distribution = Normal(mean, std)

# ì‚¬ìš© ê°€ëŠ¥ ë©”ì„œë“œ:
distribution.sample()       # ìƒ˜í”Œë§
distribution.log_prob(x)    # ë¡œê·¸ í™•ë¥ 
distribution.entropy()      # ì—”íŠ¸ë¡œí”¼
distribution.mean          # í‰ê· 
distribution.stddev        # í‘œì¤€í¸ì°¨
```

### 15.2 get_action() - í–‰ë™ ìƒ˜í”Œë§

`MLP_policy.py`ì˜ 111-128ë²ˆ ì¤„

**[êµ¬í˜„ ì˜ˆì‹œ - í•™ìŠµ ì°¸ê³ ìš©]**

```python
def get_action(self, obs: np.ndarray) -> np.ndarray:
    if len(obs.shape) > 1:
        observation = obs
    else:
        observation = obs[None]

    observation = ptu.from_numpy(observation)
    action_distribution = self.forward(observation)
    action = action_distribution.sample()
    return ptu.to_numpy(action)
```

**ë°°ì¹˜ ì°¨ì› ì²˜ë¦¬:**
```python
# ë‹¨ì¼ ìƒ˜í”Œ
obs.shape = (17,)
obs[None] = (1, 17)  # ë°°ì¹˜ ì°¨ì› ì¶”ê°€

# ì—¬ëŸ¬ ìƒ˜í”Œ
obs.shape = (100, 17)
# ê·¸ëŒ€ë¡œ ì‚¬ìš©
```

**obs[None]ì´ë€?**
- Noneì€ np.newaxisì˜ ë³„ì¹­
- ìƒˆ ì°¨ì› ì¶”ê°€
- ì˜ˆ:
```python
arr = np.array([1, 2, 3])     # shape: (3,)
arr[None]                      # shape: (1, 3)
arr[:, None]                   # shape: (3, 1)
```

**ptu.from_numpy():**
```python
def from_numpy(*args, **kwargs):
    return torch.from_numpy(*args, **kwargs).float().to(device)
```
1. numpy â†’ torch tensor
2. float64 â†’ float32 (`.float()`)
3. CPU â†’ GPU (`.to(device)`)

**action_distribution.sample():**
```python
# distribution: Normal(mean, std)
# mean: (1, 8), std: (8,)

action = distribution.sample()  # (1, 8)

# ê° ì°¨ì› ë…ë¦½ì ìœ¼ë¡œ ìƒ˜í”Œë§
# action[0][i] ~ N(mean[0][i], std[i])
```

**ptu.to_numpy():**
```python
def to_numpy(tensor):
    return tensor.to("cpu").detach().numpy()
```
1. GPU â†’ CPU (`.to("cpu")`)
2. gradient ëŠê¸° (`.detach()`)
3. torch â†’ numpy (`.numpy()`)

**ì™œ detach()?**
- gradient ê³„ì‚° ë¶ˆí•„ìš” (inferenceë§Œ)
- ë©”ëª¨ë¦¬ ì ˆì•½
- computational graphì—ì„œ ë¶„ë¦¬

**ì „ì²´ íë¦„ ì˜ˆì‹œ:**
```python
# ì…ë ¥
obs: (17,) numpy array, CPU

# 1. ë°°ì¹˜ ì°¨ì› ì¶”ê°€
observation: (1, 17)

# 2. torch ë³€í™˜
observation: (1, 17) tensor, GPU

# 3. forward
mean: (1, 8) tensor, GPU
std: (8,) tensor, GPU
distribution: Normal(mean, std)

# 4. ìƒ˜í”Œë§
action: (1, 8) tensor, GPU

# 5. numpy ë³€í™˜
action: (1, 8) numpy array, CPU

# í™˜ê²½ì€ (8,)ì„ ê¸°ëŒ€í•˜ë¯€ë¡œ squeeze í•„ìš”
# (ì‹¤ì œë¡  [0]ìœ¼ë¡œ ì²« ìƒ˜í”Œë§Œ ì‚¬ìš©)
```

### 15.3 update() - ì •ì±… í•™ìŠµ

`MLP_policy.py`ì˜ 149-166ë²ˆ ì¤„

**[êµ¬í˜„ ì˜ˆì‹œ - í•™ìŠµ ì°¸ê³ ìš©]**

```python
def update(self, observations, actions):
    self.optimizer.zero_grad()

    observations = ptu.from_numpy(observations)
    actions = ptu.from_numpy(actions)

    action_distribution = self.forward(observations)
    log_prob = action_distribution.log_prob(actions).sum(dim=-1)
    loss = -log_prob.mean()

    loss.backward()
    self.optimizer.step()

    return {
        "Training Loss": ptu.to_numpy(loss),
    }
```

**optimizer.zero_grad():**
- ì´ì „ gradient ì´ˆê¸°í™”
- PyTorchëŠ” gradientë¥¼ ëˆ„ì í•˜ê¸° ë•Œë¬¸
- ë§¤ stepë§ˆë‹¤ í•„ìš”

**ì™œ ëˆ„ì ë˜ëŠ”ê°€?**
```python
# step 1
loss1.backward()  # grad += âˆ‚loss1/âˆ‚Î¸

# step 2 (zero_grad ì•ˆ í•˜ë©´)
loss2.backward()  # grad += âˆ‚loss2/âˆ‚Î¸
# grad = âˆ‚loss1/âˆ‚Î¸ + âˆ‚loss2/âˆ‚Î¸ (ì˜ëª»ë¨!)

# ì˜¬ë°”ë¥¸ ë°©ë²•
optimizer.zero_grad()  # grad = 0
loss2.backward()       # grad = âˆ‚loss2/âˆ‚Î¸
```

**Numpy â†’ Torch ë³€í™˜:**
```python
observations: (100, 17) numpy â†’ (100, 17) tensor, GPU
actions: (100, 8) numpy â†’ (100, 8) tensor, GPU
```

**Forward pass:**
```python
action_distribution = self.forward(observations)
# Normal(mean=(100,8), std=(8,))
```

**Log probability ê³„ì‚°:**
```python
log_prob = action_distribution.log_prob(actions)
# shape: (100, 8)
```

**log_prob(actions)ë€?**
- ì£¼ì–´ì§„ actionsì˜ ë¡œê·¸ í™•ë¥  ê³„ì‚°
- ê° ì°¨ì› ë…ë¦½ì ìœ¼ë¡œ:
```python
log_prob[i][j] = log P(actions[i][j] | mean[i][j], std[j])
                = log (1/âˆš(2Ï€ÏƒÂ²)) - (x-Î¼)Â²/(2ÏƒÂ²)
```

**sum(dim=-1):**
```python
log_prob: (100, 8)
log_prob.sum(dim=-1): (100,)
```
- ë§ˆì§€ë§‰ ì°¨ì›(í–‰ë™ ì°¨ì›)ì„ ë”°ë¼ í•©ì‚°
- ì™œ? ê° í–‰ë™ ì°¨ì›ì´ ë…ë¦½ì´ë©´:
```
P(aâ‚, aâ‚‚, ..., aâ‚ˆ) = P(aâ‚) Ã— P(aâ‚‚) Ã— ... Ã— P(aâ‚ˆ)
log P(aâ‚, ..., aâ‚ˆ) = log P(aâ‚) + ... + log P(aâ‚ˆ)
```

**dim ì¸ì ì„¤ëª…:**
```python
tensor: (100, 8)

sum(dim=0): (8,)     # ìƒ˜í”Œ ì¶•ìœ¼ë¡œ í•©
sum(dim=1): (100,)   # í–‰ë™ ì¶•ìœ¼ë¡œ í•©
sum(dim=-1): (100,)  # ë§ˆì§€ë§‰ ì¶• (dim=1ê³¼ ë™ì¼)
```

**Loss ê³„ì‚°:**
```python
loss = -log_prob.mean()
```

**ì™œ negative?**
- ëª©í‘œ: log P(a|s) ìµœëŒ€í™”
- optimizerëŠ” ìµœì†Œí™”í•˜ë¯€ë¡œ
- -log P(a|s) ìµœì†Œí™” = log P(a|s) ìµœëŒ€í™”

**Maximum Likelihood Estimation (MLE):**
```
Î¸* = argmax Î£ log P(aáµ¢|sáµ¢; Î¸)
   = argmin Î£ -log P(aáµ¢|sáµ¢; Î¸)
   = argmin E[-log P(a|s; Î¸)]
```

**mean():**
- ë°°ì¹˜ í‰ê· 
- log_prob: (100,) â†’ scalar
- ì „ì²´ ì†ì‹¤ì˜ ê¸°ëŒ“ê°’ ì¶”ì •

**Backward pass:**
```python
loss.backward()
```

**ë¬´ìŠ¨ ì¼ì´?**
1. Computational graph ì—­ë°©í–¥ íƒìƒ‰
2. Chain ruleë¡œ gradient ê³„ì‚°:
```
âˆ‚loss/âˆ‚mean_net.weight
âˆ‚loss/âˆ‚logstd
```
3. ê° íŒŒë¼ë¯¸í„°ì˜ .gradì— ì €ì¥

**Computational graph ì˜ˆì‹œ:**
```
observations â†’ mean_net â†’ mean â”
logstd â†’ exp â†’ std             â”œâ†’ Normal â†’ log_prob â†’ sum â†’ mean â†’ neg â†’ loss
actions â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Optimizer step:**
```python
self.optimizer.step()
```

**Adam update (ê°„ëµ):**
```python
for param in parameters:
    m = Î²â‚*m + (1-Î²â‚)*param.grad
    v = Î²â‚‚*v + (1-Î²â‚‚)*param.gradÂ²
    param -= lr * m / (âˆšv + Îµ)
```

**ë°˜í™˜ ë¡œê·¸:**
```python
return {
    "Training Loss": ptu.to_numpy(loss),
}
```
- ì†ì‹¤ ê°’ì„ numpyë¡œ ë³€í™˜í•´ì„œ ë°˜í™˜
- ë¡œê¹…/ëª¨ë‹ˆí„°ë§ìš©

### 15.4 í•™ìŠµ ê³¼ì • ì „ì²´ íë¦„

**1íšŒ Update ì „ì²´:**
```python
# 1. ë²„í¼ì—ì„œ ìƒ˜í”Œë§
obs, actions = replay_buffer.sample(100)
# obs: (100, 17), actions: (100, 8)

# 2. Gradient ì´ˆê¸°í™”
optimizer.zero_grad()

# 3. Forward
obs_tensor = from_numpy(obs)              # (100, 17) GPU
actions_tensor = from_numpy(actions)      # (100, 8) GPU

mean = mean_net(obs_tensor)               # (100, 8)
std = exp(logstd)                         # (8,)
distribution = Normal(mean, std)

# 4. Loss
log_prob = distribution.log_prob(actions) # (100, 8)
log_prob_sum = log_prob.sum(-1)          # (100,)
loss = -log_prob_sum.mean()              # scalar

# 5. Backward
loss.backward()
# mean_net.weight.grad, logstd.grad ê³„ì‚°ë¨

# 6. Update
optimizer.step()
# mean_net.weight -= lr * ...
# logstd -= lr * ...
```

**1000 steps í›„:**
- ì •ì±…ì´ ì „ë¬¸ê°€ í–‰ë™ì„ ì˜ ëª¨ë°©
- mean_netì´ ì „ë¬¸ê°€ì²˜ëŸ¼ í–‰ë™ ì¶œë ¥
- stdëŠ” ì ì ˆí•œ exploration ìˆ˜ì¤€ í•™ìŠµ

---

## 16. ì „ì²´ íŒŒì´í”„ë¼ì¸ ìš”ì•½

### 16.1 ì´ˆê¸°í™” ë‹¨ê³„

```
1. main() - ì¸ì íŒŒì‹±, ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±
   â†“
2. run_bc() - íŒŒë¼ë¯¸í„° ì„¤ì •, expert policy ë¡œë“œ
   â†“
3. BCTrainer.__init__() - í™˜ê²½ ìƒì„±, seed ì„¤ì •
   â†“
4. BCAgent.__init__() - actor, replay buffer ìƒì„±
   â†“
5. MLPPolicySL.__init__() - ì‹ ê²½ë§, optimizer ìƒì„±
   â†“
6. build_mlp() - MLP êµ¬ì¡° êµ¬ì¶•
```

### 16.2 í•™ìŠµ ë£¨í”„ (n_iter ë°˜ë³µ)

```
for itr in range(n_iter):

    1. collect_training_trajectories()
       - itr==0: ì „ë¬¸ê°€ ë°ì´í„° ë¡œë“œ
       - itr>0: í˜„ì¬ ì •ì±…ìœ¼ë¡œ ë°ì´í„° ìˆ˜ì§‘
         â†“
       rollout_trajectories()
         â†“
       rollout_trajectory() (ì—¬ëŸ¬ ë²ˆ)
         - env.reset()
         - while not done:
             policy.get_action() â†’ env.step()

    2. do_relabel_with_expert() [DAggerë§Œ]
       - ìˆ˜ì§‘í•œ obsì— ëŒ€í•´ ì „ë¬¸ê°€ í–‰ë™ ì¿¼ë¦¬
       - actionì„ ì „ë¬¸ê°€ actionìœ¼ë¡œ êµì²´

    3. agent.add_to_replay_buffer()
       - trajectory â†’ ì„±ë¶„ë³„ ë°°ì—´ ë³€í™˜
       - ë²„í¼ì— ì¶”ê°€ (FIFO)

    4. train_agent()
       - for _ in range(num_train_steps):
           - replay_buffer.sample_random_data()
             â†“
           - agent.train()
             â†“
           - actor.update()
             - forward() â†’ distribution
             - log_prob â†’ loss
             - backward() â†’ optimizer.step()

    5. perform_logging()
       - í‰ê°€ ë°ì´í„° ìˆ˜ì§‘
       - ë©”íŠ¸ë¦­ ê³„ì‚° ë° ë¡œê¹…
       - ëª¨ë¸ ì €ì¥
```

### 16.3 í•µì‹¬ ë°ì´í„° íë¦„

**Environment â†’ Trajectories:**
```
env.reset() â†’ observation (17,)
  â†“
policy.get_action() â†’ action (8,)
  â†“
env.step() â†’ next_obs, reward, done
  â†“
ë°˜ë³µ â†’ trajectory
{
    "observation": (T, 17),
    "action": (T, 8),
    "reward": (T,),
    "next_observation": (T, 17),
    "terminal": (T,)
}
```

**Trajectories â†’ Replay Buffer:**
```
List[trajectory] â†’ convert_listofrollouts()
  â†“
obs: (N, 17)
acs: (N, 8)
...
  â†“
replay_buffer.add_rollouts()
```

**Replay Buffer â†’ Training:**
```
sample_random_data(100)
  â†“
obs_batch: (100, 17)
ac_batch: (100, 8)
  â†“
actor.update()
  â†“
loss.backward()
optimizer.step()
```

### 16.4 ì£¼ìš” ê°ì²´ Shape ì •ë¦¬

**í™˜ê²½ ê´€ë ¨:**
- observation: (ob_dim,) = (17,)
- action: (ac_dim,) = (8,)
- reward: scalar
- done: boolean

**Trajectory:**
- observations: (T, ob_dim)
- actions: (T, ac_dim)
- rewards: (T,)
- next_observations: (T, ob_dim)
- terminals: (T,)

**Replay Buffer:**
- obs: (N, ob_dim) - Nì€ ì´ transition ìˆ˜
- acs: (N, ac_dim)
- ...

**Training Batch:**
- ob_batch: (batch_size, ob_dim) = (100, 17)
- ac_batch: (batch_size, ac_dim) = (100, 8)

**ì‹ ê²½ë§:**
- mean_net ì…ë ¥: (batch, ob_dim)
- mean_net ì¶œë ¥: (batch, ac_dim)
- logstd: (ac_dim,)
- std: (ac_dim,)
- distribution: Normal(mean=(batch, ac_dim), std=(ac_dim,))

---

## 17. ê°•í™”í•™ìŠµ í•µì‹¬ ê°œë… ì •ë¦¬

### 17.1 Imitation Learning

**ì •ì˜:**
- ì „ë¬¸ê°€ì˜ í–‰ë™ì„ ê´€ì°°í•˜ê³  ëª¨ë°©í•˜ëŠ” í•™ìŠµ

**ì¢…ë¥˜:**
1. **Behavioral Cloning (BC)**
   - Supervised learningìœ¼ë¡œ ì§ì ‘ ëª¨ë°©
   - ì¥ì : ê°„ë‹¨, ë¹ ë¦„
   - ë‹¨ì : Distributional shift

2. **DAgger**
   - ë°˜ë³µì ìœ¼ë¡œ ì „ë¬¸ê°€ì—ê²Œ ë¼ë²¨ ìš”ì²­
   - ì¥ì : Distributional shift í•´ê²°
   - ë‹¨ì : ì „ë¬¸ê°€ í•„ìš”

### 17.2 Distributional Shift

**ë¬¸ì œ:**
```
í•™ìŠµ: s ~ Ï€_expert â†’ ì „ë¬¸ê°€ê°€ ë°©ë¬¸í•œ ìƒíƒœ
ì‹¤í–‰: s ~ Ï€_í•™ìŠµ â†’ ë‹¤ë¥¸ ìƒíƒœ ë°©ë¬¸ ê°€ëŠ¥
```

**ì˜ˆì‹œ:**
- ììœ¨ì£¼í–‰ í•™ìŠµ: ë„ë¡œ ì¤‘ì•™ ë°ì´í„°ë§Œ
- ì‹¤í–‰: ì•½ê°„ ë²—ì–´ë‚¨ â†’ ë³µêµ¬ ë°©ë²• ëª¨ë¦„ â†’ ë” ë²—ì–´ë‚¨ â†’ ì¶©ëŒ

**í•´ê²°: DAgger**
- í•™ìŠµ ì •ì±…ì´ ë°©ë¬¸í•œ ìƒíƒœì—ì„œë„ ì „ë¬¸ê°€ í–‰ë™ í•™ìŠµ
- ì ì§„ì ìœ¼ë¡œ ë¶„í¬ ì¼ì¹˜

### 17.3 Policy Representation

**Stochastic Policy:**
```
Ï€(a|s) = P(action=a | state=s)
```

**Gaussian Policy:**
```
Ï€(a|s) = N(Î¼(s), Î£)
Î¼(s) = mean_net(s)
Î£ = diag(ÏƒÂ²) where Ïƒ = exp(logstd)
```

**ì™œ í™•ë¥ ì ?**
- Exploration: ë‹¤ì–‘í•œ í–‰ë™ ì‹œë„
- Robustness: ë¹„ìŠ·í•œ ìƒíƒœì—ì„œ ì•½ê°„ ë‹¤ë¥¸ í–‰ë™
- Expressiveness: ë³µì¡í•œ ì •ì±… í‘œí˜„

### 17.4 Supervised Learning for BC

**ëª©í‘œ:**
```
Î¸* = argmin E_{(s,a)~D} [loss(Ï€_Î¸(s), a)]
```

**Negative Log Likelihood Loss:**
```
loss = -log Ï€_Î¸(a|s)
     = -log P(a | s; Î¸)
```

**Gradient:**
```
âˆ‡_Î¸ loss = -âˆ‡_Î¸ log P(a|s; Î¸)
```

### 17.5 Replay Buffer

**ëª©ì :**
1. **Experience reuse**: ë°ì´í„° ì¬ì‚¬ìš©
2. **Breaking correlation**: ì‹œê°„ ìƒê´€ê´€ê³„ ì œê±°
3. **Sample efficiency**: íš¨ìœ¨ì  í•™ìŠµ

**ë™ì‘:**
```
1. Collect: í™˜ê²½ì—ì„œ ë°ì´í„° ìˆ˜ì§‘
2. Store: ë²„í¼ì— ì €ì¥
3. Sample: ë¬´ì‘ìœ„ ì¶”ì¶œ
4. Train: ìƒ˜í”Œë¡œ í•™ìŠµ
```

**FIFO:**
```
[ì˜¤ë˜ëœ ë°ì´í„°] ... [ìƒˆ ë°ì´í„°]
         â†“ ë²„í¼ ê°€ë“ ì°¨ë©´
[        ìƒˆ ë°ì´í„°ë§Œ        ]
```

### 17.6 Neural Network êµ¬ì¡°

**MLP (Multi-Layer Perceptron):**
```
x â†’ Linear â†’ Activation â†’ ... â†’ Linear â†’ y
```

**ì—­í• :**
- Linear: ì„ í˜• ë³€í™˜
- Activation: ë¹„ì„ í˜•ì„± ì¶”ê°€

**ì™œ ë¹„ì„ í˜•?**
- Linearë§Œ: ì „ì²´ê°€ Linear
- Activation ì¶”ê°€: ë³µì¡í•œ í•¨ìˆ˜ ê·¼ì‚¬ ê°€ëŠ¥

### 17.7 Optimization

**Gradient Descent:**
```
Î¸_{t+1} = Î¸_t - Î± âˆ‡_Î¸ loss
```

**Adam:**
- Adaptive learning rate
- Momentum ì‚¬ìš©
- ë¹ ë¥¸ ìˆ˜ë ´

**Backpropagation:**
- Chain ruleë¡œ gradient ê³„ì‚°
- Computational graph ì‚¬ìš©
- ìë™ ë¯¸ë¶„

---

## 18. ì‹¤ì „ íŒ ë° ë””ë²„ê¹…

### 18.1 ìì£¼ í•˜ëŠ” ì‹¤ìˆ˜

1. **Gradient ì´ˆê¸°í™” ì•ˆ í•¨**
```python
# ì˜ëª»
loss.backward()
optimizer.step()

# ì˜¬ë°”ë¦„
optimizer.zero_grad()
loss.backward()
optimizer.step()
```

2. **Shape ì•ˆ ë§ìŒ**
```python
# ë‹¨ì¼ ìƒ˜í”Œ ì²˜ë¦¬
obs.shape = (17,)
obs[None]  # (1, 17)ë¡œ ë§Œë“¤ê¸°
```

3. **Device ë¶ˆì¼ì¹˜**
```python
# ì˜ëª»
model.to('cuda')
input.to('cpu')  # ì—ëŸ¬!

# ì˜¬ë°”ë¦„
model.to('cuda')
input.to('cuda')
```

### 18.2 ë””ë²„ê¹… ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] ëª¨ë“  ë°ì´í„° shape í™•ì¸
- [ ] Gradient ì´ˆê¸°í™” í™•ì¸
- [ ] Device ì¼ì¹˜ í™•ì¸
- [ ] Lossê°€ ê°ì†Œí•˜ëŠ”ì§€
- [ ] Replay bufferì— ë°ì´í„° ìˆëŠ”ì§€
- [ ] ìƒ˜í”Œë§ì´ ì œëŒ€ë¡œ ë˜ëŠ”ì§€

### 18.3 ì„±ëŠ¥ í–¥ìƒ

**í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹:**
- learning_rate: 0.001, 0.0001, ...
- batch_size: 32, 64, 128, ...
- n_layers: 2, 3, 4
- size: 64, 128, 256

**ë°ì´í„°:**
- ë” ë§ì€ expert ë°ì´í„°
- DAgger iterations ì¦ê°€
- ë” ê¸´ training steps

---

## ë§ˆë¬´ë¦¬

ì´ ê°€ì´ë“œëŠ” run_hw1.pyì˜ ì „ì²´ íŒŒì´í”„ë¼ì¸ì„ ì²˜ìŒë¶€í„° ëê¹Œì§€ ìƒì„¸íˆ ë‹¤ë¤˜ìŠµë‹ˆë‹¤.

**í•µì‹¬ ìš”ì•½:**
1. BCëŠ” ì „ë¬¸ê°€ ëª¨ë°©, DAggerëŠ” ë°˜ë³µì  ê°œì„ 
2. ì‹ ê²½ë§ìœ¼ë¡œ ì •ì±… í‘œí˜„ (Gaussian policy)
3. Replay bufferë¡œ íš¨ìœ¨ì  í•™ìŠµ
4. MLEë¡œ supervised learning
5. PyTorchë¡œ êµ¬í˜„

**ë‹¤ìŒ ë‹¨ê³„:**
- ì½”ë“œ ì§ì ‘ êµ¬í˜„í•´ë³´ê¸°
- ë‹¤ë¥¸ í™˜ê²½ì—ì„œ ì‹¤í—˜
- ì„±ëŠ¥ ë¹„êµ ë° ë¶„ì„
- ë…¼ë¬¸ ì½ê¸° (DAgger, GAIL ë“±)

ì´ì œ ì—¬ëŸ¬ë¶„ì€ Imitation Learningì˜ í•µì‹¬ì„ ì™„ì „íˆ ì´í•´í–ˆìŠµë‹ˆë‹¤! ğŸ‰
