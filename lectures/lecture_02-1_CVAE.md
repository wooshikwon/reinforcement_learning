# Lecture 02-1: ACT 알고리즘 심화(CVAE & Transformer)

> 이 문서는 Lecture 02: Imitation Learning의 보충 자료로, ACT (Action Chunking with Transformers) 알고리즘의 CVAE (Conditional Variational Auto-Encoder) 아키텍처를 상세히 설명합니다.

## 목차

- [개요](#개요)
- [ACT의 핵심 설계 철학](#act의-핵심-설계-철학)
- [해결책 1: 액션 청킹](#해결책-1-액션-청킹-action-chunking)
- [해결책 2: 절대 좌표 제어](#해결책-2-절대-좌표-제어)
- [해결책 3: CVAE를 통한 확률적 정책 모델링](#해결책-3-cvae를-통한-확률적-정책-모델링)
- [ACT 전체 데이터 흐름](#act-전체-데이터-흐름-구체적-예시)
- [훈련 vs 추론 비교](#훈련-vs-추론-비교)
- [자주 묻는 질문](#자주-묻는-질문-faq)

---

## 개요

ACT (Action Chunking with Transformers)는 ALOHA 로봇 시스템에서 제안된 모방 학습 알고리즘으로, 기존 행동 복제(Behavioral Cloning)의 두 가지 주요 문제를 해결합니다:

1. **누적 오차 (Compounding Errors)**: 특히 50Hz라는 빠른 제어 주기에서 작은 오차가 순식간에 누적
2. **다중 모드 데이터 (Multimodal Data)**: 사람이 시연할 때마다 미세하게 다른 방식으로 작업 수행

이 문제들 때문에 기존 모방 학습 방식은 **성공률 0%**를 기록했지만, ACT는 **96% 성공률**을 달성했습니다. 이 극적인 개선의 비결은 무엇일까요?

---

## ACT의 핵심 설계 철학

ACT는 세 가지 핵심 아이디어를 제안합니다:

1. **액션 청킹 (Action Chunking)**: 의사결정 빈도를 줄여 누적 오차를 완화
2. **절대 좌표 제어 (Absolute Positioning)**: 자기 교정 효과로 안정성 향상
3. **CVAE 기반 확률적 모델링**: 다중 모드 분포를 표현하여 평균내기 문제 해결

이 세 가지가 **트랜스포머 (Transformer)** 신경망 아키텍처 안에서 통합되어 작동합니다. 특히 세 번째 아이디어인 CVAE는 ACT의 가장 독창적인 부분으로, 이를 제대로 이해하면 ACT가 왜 그렇게 효과적인지 명확하게 알 수 있습니다.

---

## 해결책 1: 액션 청킹 (Action Chunking)

'누적 오차'는 매 순간 의사결정을 할 때마다 오차가 쌓이는 문제입니다. 그렇다면, 의사결정 빈도를 줄이면 어떨까요? 이것이 액션 청킹의 핵심입니다.

### 개념

기존 방식처럼 1/50초마다 한번씩 행동을 결정하는 대신, ACT 정책은 한번에 **미래의 행동 묶음 (chunk), 예를 들어 약 60개의 행동 시퀀스 전체를 예측**합니다. 그리고 로봇은 이 60개의 행동이 끝날 때까지 중간에 새로운 관찰 (카메라 영상)을 보지 않고 예측된 행동을 그대로 수행합니다. 이를 **개루프 (open-loop)** 제어라고 합니다. 그리고 60 스텝이 지난 후에야 다시 주변을 둘러보고 다음 행동 묶음을 예측합니다.

### 수학적 표현

**전통적인 방식:**
- 매 시점 $t$마다 정책 호출: $a_t = \pi(o_t)$
- 의사결정 빈도: 50Hz (1/50초마다)
- 누적 오차 발생 기회: 초당 50회

**ACT의 액션 청킹:**
- 한 번의 정책 호출로 미래 $k$개 행동 예측: $[a_t, a_{t+1}, ..., a_{t+k-1}] = \pi(o_t)$
- 청크 길이 $k = 60$
- 의사결정 빈도: $\frac{50}{60} \approx 0.8$Hz
- 누적 오차 발생 기회: 초당 0.8회

### 효과

이렇게 함으로써, 50Hz로 매우 자주 하던 의사결정을 약 0.8Hz의 느린 주기로 바꾸는 효과를 얻습니다. 이는 반응성은 약간 희생하지만, 오차가 빠르게 누적되는 것을 막아 훨씬 안정적인 제어를 가능하게 하는 똑똑한 트레이드오프 전략입니다.

또한 **시간적 일관성 (temporal consistency)**도 향상됩니다. 60개의 행동을 한꺼번에 예측하기 때문에, 정책은 단기 미래 전체를 고려한 일관된 계획을 세울 수 있습니다. 이는 "지금 당장 좋아 보이는 행동"만 선택하다가 막다른 골목에 빠지는 문제를 방지합니다.

### Temporal Ensembling (선택적 기법)

실제 구현에서는 더욱 안정성을 높이기 위해 **Temporal Ensembling**이라는 기법을 사용하기도 합니다:

1. 매 시점마다 정책을 호출하여 60개 청크를 예측
2. 현재 시점 $t$에 대해 여러 청크에서 겹치는 예측들을 평균
3. 예: $t$에서의 행동은 "$t$부터 시작한 청크의 첫 액션", "$t-1$부터 시작한 청크의 두 번째 액션", ... 등을 평균

이를 통해 예측의 스무딩 효과와 함께, open-loop의 단점(환경 변화 대응 느림)을 일부 완화할 수 있습니다.

---

## 해결책 2: 절대 좌표 제어

행동을 '현재 위치에서 얼마만큼 움직여라' 같은 **상대적인 값 (delta/incremental control)**이 아니라, '목표 지점인 [x, y, z] 좌표로 가라'와 같은 **절대적인 관절 위치 (absolute joint positions)**로 예측합니다.

### 왜 절대 좌표인가?

**상대적 움직임:**
- 오차가 계속 누적됨: $\text{실제위치}_t = \text{실제위치}_{t-1} + (\text{명령}_t + \text{오차}_t)$
- 시간이 지날수록 목표 위치에서 점점 더 멀어질 수 있음

**절대적 목표 위치:**
- 매번 목표 지점을 새로 명시: $\text{목표위치}_t = \text{명령}_t$
- 이전 오차와 무관하게 항상 올바른 목표를 향함
- 자기 교정 효과 (self-correcting): 제어기가 매번 현재 위치와 목표 위치의 차이를 보정

이는 특히 60스텝 동안 open-loop로 실행되는 ACT에서 매우 중요합니다. 60개의 절대 위치 명령은 각각이 독립적인 목표 지점이므로, 중간에 외란이나 미세한 실행 오차가 있어도 다음 명령이 올바른 방향으로 되돌립니다.

---

## 해결책 3: CVAE를 통한 확률적 정책 모델링

이제 ACT의 가장 핵심적이고 독창적인 부분을 자세히 살펴보겠습니다. 바로 **CVAE (Conditional Variational Auto-Encoder)**를 이용한 확률적 정책 모델링입니다.

### CVAE란 무엇인가?

먼저 기본 개념부터 이해해 봅시다.

**VAE (Variational Auto-Encoder)**는 데이터의 복잡한 분포를 학습하는 생성 모델입니다:
- **인코더**: 입력 데이터 $x$를 압축된 **잠재 벡터 (latent vector)** $z$의 분포로 변환 ($q_\phi(z|x)$)
- **디코더**: 잠재 벡터 $z$로부터 원본 데이터를 재구성 ($p_\theta(x|z)$)
- **잠재 공간 (latent space)**: 고차원 데이터의 본질적인 특성을 저차원으로 압축한 공간

**CVAE (Conditional VAE)**는 VAE에 **조건 (condition)** $c$를 추가한 버전입니다:
- **조건부 인코더**: $q_\phi(z|x, c)$ - 데이터 $x$와 조건 $c$를 모두 보고 $z$ 추론
- **조건부 디코더**: $p_\theta(x|z, c)$ - 조건 $c$와 잠재 벡터 $z$를 받아 데이터 $x$ 생성

### 왜 ACT에 CVAE가 필요한가?

앞서 배운 **다중 모드 (multimodal) 데이터 문제**를 떠올려 봅시다. 같은 시작 상태에서도 여러 가지 올바른 행동 시퀀스가 존재할 수 있습니다 (예: 장애물을 왼쪽/오른쪽으로 피하기).

일반적인 회귀 모델 (regression)로 정책을 학습하면:
- 여러 정답 행동들의 **평균**을 예측하려 함
- 결과: 장애물을 향해 직진하는 최악의 행동

ACT는 이 문제를 CVAE로 해결합니다:
- **잠재 벡터 $z$**가 "어떤 스타일의 행동을 할지"를 나타냄
- 예: $z_1$은 "왼쪽으로 피하기", $z_2$는 "오른쪽으로 피하기"
- 정책은 $z$가 주어지면 **그에 해당하는 일관된 행동 시퀀스**를 생성
- 평균내기 없이, 여러 유효한 해결책 중 **하나를 일관되게** 선택

### ACT의 CVAE 아키텍처: 확률 모델 관점

ACT의 CVAE는 다음과 같이 정의됩니다:

**조건 (condition) $c$**: 현재 관찰 $o_t$ (이미지 + 조인트 센서)

**데이터 $x$**: 미래 행동 시퀀스 $a_{t:t+k-1}$ (청크)

**목표**: 조건 $o_t$가 주어졌을 때, 전문가의 행동 분포 $p(a_{t:t+k-1}|o_t)$를 학습

**CVAE의 두 가지 구성 요소:**

1. **CVAE 인코더 (Posterior Network) $q_\phi(z | a_{t:t+k-1}, j_{t:t+k-1})$**
   - 역할: **정답 행동 시퀀스**와 **조인트 관측 시퀀스**를 보고, 이 행동을 만들어낸 **잠재 의도 $z$**를 추정
   - 사용 시점: **훈련할 때만** 사용
   - 왜 미래를 봐도 되나?: 이것은 **사후 추정 (posterior estimation)**이기 때문. "이미 일어난 행동"을 설명하는 $z$를 찾는 것이므로, 훈련 데이터에서는 미래를 볼 수 있음

2. **CVAE 디코더 (Policy Network) $p_\theta(a_{t:t+k-1} | z, o_t)$**
   - 역할: **현재 관찰 $o_t$**와 **잠재 벡터 $z$**를 받아 **미래 행동 시퀀스**를 생성
   - 사용 시점: **훈련 및 추론 모두** 사용
   - 이것이 실제 정책: 추론 시에는 CVAE 인코더 없이 이것만 사용

**ELBO (Evidence Lower Bound) 손실:**

$$
\mathcal{L} = \mathbb{E}_{z \sim q_\phi(z|a,j)} [\log p_\theta(a|z,o)] - \beta \cdot \text{KL}(q_\phi(z|a,j) || p(z))
$$

- 첫 번째 항: **재구성 손실 (reconstruction loss)** - 예측한 행동이 정답과 얼마나 가까운지
- 두 번째 항: **KL 발산 (KL divergence)** - 사후 분포 $q_\phi$가 사전 분포 $p(z) = \mathcal{N}(0, I)$에서 너무 멀어지지 않도록 정규화
- $\beta$: KL 항의 가중치 (보통 작은 값, 예: 0.0001)

### ACT의 CVAE 아키텍처: 신경망 구현 관점

이제 이 확률 모델이 실제로 어떤 신경망으로 구현되는지 살펴봅시다. 여기서 헷갈리기 쉬운 점은 **"확률 모델의 역할 이름"**과 **"신경망 블록 이름"**이 다르다는 것입니다.

**핵심 구분:**
- **확률 모델 이름**: CVAE 인코더, CVAE 디코더 (역할)
- **신경망 구현 이름**: Transformer 인코더, Transformer 디코더, CNN (구체적인 레이어)

**신경망 구성 요소:**

#### 1. CNN 백본 (Image Encoder)

- **역할**: 멀티뷰 카메라 이미지를 특징 벡터로 변환
- **구조**: ResNet-18 등의 사전학습 모델
- **입출력**: RGB 이미지 → 특징 맵 (feature map)

#### 2. 관찰 인코더 (Observation Encoder) - Transformer 인코더

- **역할**: 현재 시점의 여러 관찰(이미지 특징 + 조인트)을 통합하여 **컨텍스트 메모리** 생성
- **입력**:
  - 이미지 특징들 (각 카메라 뷰마다)
  - 현재 조인트 상태 $j_t$
- **출력**: 컨텍스트 메모리 (형상: $N_{\text{ctx}} \times d$)
- 이 컨텍스트는 나중에 **정책 디코더의 Key/Value**로 사용됨

#### 3. CVAE 인코더 - Transformer 인코더 (훈련 전용)

- **역할**: **정답 행동 시퀀스**와 **조인트 시퀀스**를 요약하여 **잠재 벡터 $z$** 추정
- **입력**:
  - 정답 액션 시퀀스 $a_{t:t+k-1}$ (형상: $k \times D_a$)
  - 조인트 관측 시퀀스 $j_{t:t+k-1}$ (형상: $k \times D_j$)
- **처리 과정**:
  1. 각 시퀀스를 선형 임베딩 → 임베딩 벡터 (형상: $k \times d$)
  2. 위치 인코딩 (positional encoding) 추가
  3. 두 시퀀스를 결합 (concatenate 또는 add)
  4. **Transformer 인코더** 레이어들 통과 → 시퀀스 출력 (형상: $k \times d$)
  5. **풀링 (pooling)** 또는 **[CLS] 토큰**으로 요약 → 단일 벡터 (형상: $1 \times d$)
  6. 선형 레이어로 평균 $\mu$ 와 로그 분산 $\log\sigma^2$ 추정 (각각 형상: $1 \times d_z$)
  7. **Reparameterization trick**: $z = \mu + \sigma \odot \epsilon$, 여기서 $\epsilon \sim \mathcal{N}(0,I)$
- **출력**: 잠재 벡터 $z$ (형상: $1 \times d_z$)

> **60개 시점 → 1개 벡터로 압축되는 원리:**
>
> Transformer 인코더는 시퀀스의 각 위치마다 출력을 생성하지만 (60개), 우리는 **전체 시퀀스를 대표하는 하나의 요약 벡터**가 필요합니다. 이를 위해:
> - **[CLS] 토큰 방식**: 시퀀스 맨 앞에 특수 토큰을 추가하고, 그 위치의 출력을 요약으로 사용
> - **평균 풀링 (mean pooling)**: 60개 출력을 평균내어 1개로
> - **어텐션 풀링 (attention pooling)**: 학습 가능한 가중치로 60개를 가중평균
>
> 이렇게 얻은 1개 벡터가 "60개 행동 전체를 만들어낸 잠재 의도"를 나타냅니다.

#### 4. 정책 디코더 (Policy Decoder) - Transformer 디코더

- **역할**: **현재 관찰 컨텍스트**와 **잠재 벡터 $z$**를 조건으로 **미래 행동 시퀀스** 예측
- **입력 구성**:
  - **Query (쿼리)**: 미래 $k$개 시점의 위치 임베딩 (learnable position embeddings, 형상: $k \times d$)
    - 이것은 "미래 60칸을 가리키는 빈 슬롯"
  - **Key & Value (키/밸류)**: 관찰 인코더가 만든 컨텍스트 메모리 (형상: $N_{\text{ctx}} \times d$)
    - 현재 상황에 대한 정보
  - **Latent Injection (잠재 벡터 주입)**: $z$ (형상: $1 \times d_z$)를 선형 변환하여 $d$ 차원으로 맵핑 후, 컨텍스트에 추가하거나 각 레이어에 주입
- **처리 과정**:
  1. 쿼리 토큰들이 **Cross-Attention**을 통해 컨텍스트(K/V)를 참조
  2. 잠재 벡터 $z$의 정보가 디코더 내에서 행동 스타일을 결정
  3. 각 쿼리 위치마다 독립적으로 행동 예측 (병렬 예측)
- **출력**:
  - Transformer 디코더 출력 (형상: $k \times d$)
  - 각 위치에 **공유 Linear 헤드** 적용 → 행동 벡터 (형상: $k \times D_a$)
  - 최종: 예측 행동 시퀀스 $\hat{a}_{t:t+k-1}$

### 전체 구조 다이어그램

#### 훈련 시

```
입력: 이미지(현재) + 조인트(현재) + 액션(정답, 미래 60스텝) + 조인트(미래 60스텝)

┌─────────────────────────────────────────────────────────┐
│ CVAE 인코더 (훈련 전용, 사후 추정기)                        │
│  입력: 액션(60스텝) + 조인트(60스텝)                       │
│  ↓                                                        │
│  [Embedding + Positional Encoding]                       │
│  ↓                                                        │
│  [Transformer Encoder] → 시퀀스 출력 (60×d)               │
│  ↓                                                        │
│  [Pooling/CLS] → 요약 벡터 (1×d)                          │
│  ↓                                                        │
│  [Linear] → μ, log σ² (각 1×d_z)                          │
│  ↓                                                        │
│  [Reparameterization] → z (1×d_z)                         │
└─────────────────────────────────────────────────────────┘
                           ↓ z
                           ↓
┌─────────────────────────────────────────────────────────┐
│ 관찰 인코더 (현재 상황 이해)                               │
│  입력: 이미지(현재) + 조인트(현재)                         │
│  ↓                                                        │
│  [CNN 백본] → 이미지 특징                                  │
│  ↓                                                        │
│  [Transformer Encoder] → 컨텍스트 메모리 (N_ctx×d)         │
└─────────────────────────────────────────────────────────┘
                           ↓ 컨텍스트
                           ↓
┌─────────────────────────────────────────────────────────┐
│ 정책 디코더 (CVAE 디코더, 행동 생성기)                      │
│  입력:                                                    │
│   - Query: 미래 위치 임베딩 (60×d)                        │
│   - Key/Value: 컨텍스트 메모리                             │
│   - Condition: 잠재 벡터 z                                │
│  ↓                                                        │
│  [Transformer Decoder] → 디코더 출력 (60×d)                │
│  ↓                                                        │
│  [Linear Head] → 예측 액션 (60×D_a)                       │
└─────────────────────────────────────────────────────────┘
                           ↓
                     출력: â (60스텝 예측)

손실: Reconstruction Loss(â vs a) + KL Loss(q(z|a,j) || N(0,I))
```

#### 추론 시

```
입력: 이미지(현재) + 조인트(현재)

┌─────────────────────────────────────────────────────────┐
│ CVAE 인코더 - 사용하지 않음!                               │
│ 대신 z = 0 (또는 z ~ N(0,I) 샘플링)                        │
└─────────────────────────────────────────────────────────┘
                           ↓ z=0
                           ↓
┌─────────────────────────────────────────────────────────┐
│ 관찰 인코더                                               │
│  [동일]                                                   │
└─────────────────────────────────────────────────────────┘
                           ↓
┌─────────────────────────────────────────────────────────┐
│ 정책 디코더                                               │
│  [동일]                                                   │
└─────────────────────────────────────────────────────────┘
                           ↓
                     출력: â (60스텝 예측)
                           ↓
                    로봇 실행 (open-loop)
```

---

## ACT 전체 데이터 흐름: 구체적 예시

이제 실제 숫자 예시로 전체 흐름을 따라가 봅시다.

### 가정 (7-DoF 양팔 로봇 예시)

- **로봇 구성**: 7자유도 양팔 + 그리퍼 2개 → 행동 차원 $D_a = 16$
- **조인트 센서**: 관절 각도 + 각속도 + 그리퍼 상태 → $D_j = 16$ (예시)
- **청크 길이**: $k = 60$
- **임베딩 차원**: $d = 512$
- **잠재 차원**: $d_z = 64$
- **카메라**: 4대 (전방, 상단, 양쪽 손목)

### 훈련 시 단계별 차원 흐름

#### Step 0: 원시 데이터

- 현재 이미지: $4 \times (224 \times 224 \times 3)$ (4개 카메라, RGB)
- 현재 조인트: $D_j = 16$
- 정답 액션 시퀀스: $60 \times 16$ (미래 60스텝)
- 정답 조인트 시퀀스: $60 \times 16$ (미래 60스텝)

#### Step 1: CVAE 인코더 (사후 추정)

**입력:**
- 액션 시퀀스 $A \in \mathbb{R}^{60 \times 16}$
- 조인트 시퀀스 $J \in \mathbb{R}^{60 \times 16}$

**처리:**
1. 선형 임베딩:
   - $A_{\text{emb}} = \text{Linear}(A) \in \mathbb{R}^{60 \times 512}$
   - $J_{\text{emb}} = \text{Linear}(J) \in \mathbb{R}^{60 \times 512}$
2. 위치 인코딩 추가:
   - $X = A_{\text{emb}} + J_{\text{emb}} + \text{PosEnc} \in \mathbb{R}^{60 \times 512}$
3. Transformer 인코더 (예: 4 레이어):
   - $H = \text{TransformerEnc}(X) \in \mathbb{R}^{60 \times 512}$
4. 풀링 (평균):
   - $h = \text{mean}(H) \in \mathbb{R}^{1 \times 512}$
5. 분포 파라미터:
   - $\mu = \text{Linear}_\mu(h) \in \mathbb{R}^{1 \times 64}$
   - $\log\sigma^2 = \text{Linear}_\sigma(h) \in \mathbb{R}^{1 \times 64}$
6. 샘플링:
   - $z = \mu + \sigma \odot \epsilon \in \mathbb{R}^{1 \times 64}$, where $\epsilon \sim \mathcal{N}(0,I)$

**출력:** $z \in \mathbb{R}^{1 \times 64}$

#### Step 2: 관찰 인코더 (현재 상황 이해)

**입력:**
- 4개 이미지 (각 $224 \times 224 \times 3$)
- 현재 조인트 $j_t \in \mathbb{R}^{16}$

**처리:**
1. CNN 백본 (각 이미지):
   - $f_1, f_2, f_3, f_4 = \text{ResNet18}(\text{images}) \in \mathbb{R}^{4 \times 512}$
2. 조인트 임베딩:
   - $j_{\text{emb}} = \text{Linear}(j_t) \in \mathbb{R}^{1 \times 512}$
3. 결합:
   - $X_{\text{obs}} = [f_1, f_2, f_3, f_4, j_{\text{emb}}] \in \mathbb{R}^{5 \times 512}$
4. Transformer 인코더:
   - $\text{Context} = \text{TransformerEnc}(X_{\text{obs}}) \in \mathbb{R}^{5 \times 512}$

**출력:** 컨텍스트 메모리 $\in \mathbb{R}^{5 \times 512}$

#### Step 3: 정책 디코더 (행동 예측)

**입력:**
- 컨텍스트 $\in \mathbb{R}^{5 \times 512}$ (Key/Value)
- 잠재 벡터 $z \in \mathbb{R}^{1 \times 64}$
- 미래 위치 쿼리 $Q_{\text{pos}} \in \mathbb{R}^{60 \times 512}$ (학습 가능한 파라미터)

**처리:**
1. 잠재 벡터 투사:
   - $z' = \text{Linear}(z) \in \mathbb{R}^{1 \times 512}$
   - 컨텍스트에 추가: $\text{Context}' = [\text{Context}, z'] \in \mathbb{R}^{6 \times 512}$
2. Transformer 디코더 (예: 6 레이어):
   - Cross-Attention: Query는 $Q_{\text{pos}}$, Key/Value는 $\text{Context}'$
   - $H_{\text{dec}} = \text{TransformerDec}(Q_{\text{pos}}, \text{Context}') \in \mathbb{R}^{60 \times 512}$
3. 행동 헤드:
   - $\hat{A} = \text{Linear}_{\text{action}}(H_{\text{dec}}) \in \mathbb{R}^{60 \times 16}$

**출력:** 예측 액션 시퀀스 $\hat{A} \in \mathbb{R}^{60 \times 16}$

#### Step 4: 손실 계산

**재구성 손실:**
$$
\mathcal{L}_{\text{recon}} = \text{MSE}(\hat{A}, A) = \frac{1}{60 \times 16} \sum_{i,j} (\hat{A}_{ij} - A_{ij})^2
$$

**KL 발산:**
$$
\mathcal{L}_{\text{KL}} = \text{KL}(q_\phi(z|A,J) || \mathcal{N}(0,I)) = -\frac{1}{2}\sum_{i=1}^{64}(1 + \log\sigma_i^2 - \mu_i^2 - \sigma_i^2)
$$

**전체 손실:**
$$
\mathcal{L} = \mathcal{L}_{\text{recon}} + \beta \cdot \mathcal{L}_{\text{KL}}
$$
(여기서 $\beta = 0.0001$ 등 작은 값)

### 추론 시 단계별 차원 흐름

#### Step 0: 원시 데이터
- 현재 이미지: $4 \times (224 \times 224 \times 3)$
- 현재 조인트: $16$

#### Step 1: CVAE 인코더 - 건너뜀
- 대신 $z = \mathbf{0} \in \mathbb{R}^{1 \times 64}$ 사용 (또는 $z \sim \mathcal{N}(0,I)$ 샘플)

#### Step 2: 관찰 인코더 - 동일
- 출력: $\text{Context} \in \mathbb{R}^{5 \times 512}$

#### Step 3: 정책 디코더 - 동일
- 입력: Context + $z=0$ + $Q_{\text{pos}}$
- 출력: $\hat{A} \in \mathbb{R}^{60 \times 16}$

#### Step 4: 실행
- 60개 행동을 로봇에 순차 전송 (open-loop)
- 60 스텝 후, 새로운 관찰로 다시 Step 0부터 반복

---

## 훈련 vs 추론 비교

| 구성 요소 | 훈련 시 | 추론 시 |
|---------|--------|--------|
| **CVAE 인코더** | ✅ 사용 (정답 액션+조인트 → z 추정) | ❌ 사용 안 함 |
| **잠재 벡터 z** | 정답 데이터로부터 사후 추정 | $z=0$ 또는 prior 샘플 |
| **관찰 인코더** | ✅ 사용 (현재 이미지+조인트 → 컨텍스트) | ✅ 사용 (동일) |
| **정책 디코더** | ✅ 사용 (컨텍스트+z → 행동 예측) | ✅ 사용 (동일) |
| **손실 계산** | ✅ 재구성 + KL 손실 | ❌ 손실 없음 (추론만) |
| **정보 흐름** | 미래 정답을 볼 수 있음 (사후 추정) | 현재 관찰만 사용 |
| **목적** | 다양한 전문가 행동 스타일 학습 | 학습된 정책으로 일관된 행동 생성 |

### 왜 CVAE 인코더는 훈련 때만 쓰나?

CVAE 인코더는 "정답을 이미 알고 있는 상황"에서 "이 정답을 만들어낸 잠재 의도 z"를 역추론하는 것입니다. 이는 **사후 확률 (posterior)** $q_\phi(z|a,j)$를 추정하는 과정입니다.

추론 시에는 정답이 없으므로 사후 확률을 계산할 수 없습니다. 대신 **사전 확률 (prior)** $p(z) = \mathcal{N}(0,I)$에서 샘플링하거나, 실용적으로는 평균인 $z=0$을 사용합니다.

훈련 과정에서 KL 손실이 $q_\phi(z|a,j)$를 $\mathcal{N}(0,I)$에 가깝게 유지하도록 정규화하므로, 추론 시 $z=0$ (또는 작은 랜덤 샘플)을 써도 정책이 합리적인 행동을 생성할 수 있습니다.

---

## 자주 묻는 질문 (FAQ)

### Q1. CVAE 인코더가 Transformer 인코더인가요?

네, 맞습니다. 하지만 용어가 혼동될 수 있습니다:
- **"CVAE 인코더"**는 **확률 모델에서의 역할 이름**입니다 ($q_\phi(z|x,c)$를 구현하는 부분).
- 이 CVAE 인코더를 **구현할 때 Transformer 인코더 신경망**을 사용합니다.
- 마찬가지로 관찰을 처리하는 부분도 Transformer 인코더를 씁니다.

즉, ACT에는 **여러 개의 Transformer 인코더**가 있으며, 각각 다른 역할을 합니다:
1. CVAE 인코더 안의 Transformer 인코더 (정답 시퀀스 → z)
2. 관찰 인코더로서의 Transformer 인코더 (관찰 → 컨텍스트)

### Q2. "정책 디코더"와 "Transformer 디코더"는 같은 건가요?

네, 같습니다:
- **"정책 디코더" = "CVAE 디코더"**: 확률 모델에서의 역할 이름 ($p_\theta(a|z,o)$)
- **"Transformer 디코더"**: 이를 구현하는 신경망 아키텍처

### Q3. 왜 CVAE 인코더는 미래 데이터를 봐도 되나요? 정보 누수 아닌가요?

아닙니다. CVAE 인코더는 **훈련 전용 모듈**이며, **추론 경로에 포함되지 않습니다**.

- **훈련 시**: 정답 액션 시퀀스를 보고 z를 추정하는 것은 **사후 추정 (posterior inference)**입니다. 이미 일어난 일을 설명하는 변수를 찾는 것이므로 정당합니다.
- **추론 시**: CVAE 인코더를 사용하지 않으므로, 미래 정보를 전혀 볼 수 없습니다.

이는 VAE/CVAE의 표준적인 학습 방식입니다.

### Q4. 60개 시점 시퀀스가 어떻게 1개 잠재 벡터 z로 압축되나요?

Transformer 인코더는 시퀀스의 각 위치마다 출력을 만듭니다 (60개 입력 → 60개 출력). 하지만 우리는 **전체 시퀀스를 대표하는 단일 벡터**가 필요합니다.

이를 위해 **풀링 (pooling)** 메커니즘을 사용합니다:
- **[CLS] 토큰**: 시퀀스 앞에 특수 토큰을 추가하고, 그 위치의 최종 출력을 요약으로 사용 (BERT 스타일)
- **평균 풀링**: 60개 출력 벡터의 평균
- **어텐션 풀링**: 학습 가능한 가중치로 가중평균

이렇게 얻은 1개 벡터를 선형층에 통과시켜 μ, σ를 얻고, reparameterization으로 z를 샘플링합니다.

### Q5. 추론 시 z=0을 쓰면 항상 똑같은 행동만 나오지 않나요?

실제로는 그렇지 않습니다:
- **관찰이 다르면 행동도 달라집니다**: 정책 디코더는 z뿐 아니라 **현재 관찰 (컨텍스트)**에도 조건화됩니다.
- **z=0은 "평균적/일반적 스타일"을 의미**: 훈련 중 KL 손실이 z 분포를 N(0,I) 주변으로 정규화하므로, z=0 근처 값들이 가장 흔한 성공적인 행동 패턴을 나타냅니다.
- **선택적 다양성**: 필요하다면 z ~ N(0, I)에서 샘플링하여 약간씩 다른 행동 스타일을 얻을 수도 있습니다.

### Q6. 조인트 관측과 조인트 액션의 차이는 무엇인가요?

- **조인트 관측 (joint observation)** $j_t$: 현재 시점의 **센서 읽기** (관절 각도, 각속도, 그리퍼 위치 등) - **입력**
- **조인트 액션 (joint action)** $a_t$: 로봇에게 보내는 **제어 명령** (목표 관절 위치, 목표 속도, 그리퍼 개폐 명령 등) - **출력**

둘 다 **연속 벡터**이며, 물리적으로는 같은 공간(관절 공간)에 있지만 역할이 다릅니다.

### Q7. "관찰"에는 정확히 무엇이 포함되나요?

ACT의 관찰 $o_t$는:
- **이미지**: 멀티뷰 카메라 RGB 프레임 (시각 정보)
- **조인트 센서**: 현재 관절 각도, 각속도, 그리퍼 상태 등 (프로프리오셉션, proprioception)

**포함되지 않는 것**:
- 객체의 상대 위치 벡터 (이런 것은 비전으로부터 **유도**될 수 있지만 직접 관찰되지는 않음)
- 분류 라벨
- 미래 정보

### Q8. Temporal Ensembling은 언제 쓰나요?

선택적 기법입니다:
- **기본 실행**: 60스텝마다 정책 호출 (open-loop)
- **Temporal Ensembling**: 매 스텝마다 정책 호출하되, 겹치는 청크들의 예측을 평균

**장점:**
- 예측 스무딩 효과
- 환경 변화에 더 빠른 대응 (closed-loop처럼 작동)

**단점:**
- 계산 비용 증가 (60배)

실습에서는 작업의 동적 특성에 따라 선택합니다.

---

## 요약

ACT 알고리즘은 세 가지 핵심 아이디어의 조합으로 모방 학습의 고질적인 문제들을 해결했습니다:

1. **액션 청킹**: 의사결정 빈도 감소로 누적 오차 완화 (50Hz → 0.8Hz)
2. **절대 좌표 제어**: 자기 교정 효과로 안정성 향상
3. **CVAE 기반 모델링**: 다중 모드 분포 학습으로 평균내기 문제 해결

특히 CVAE 아키텍처는:
- **훈련 시**: CVAE 인코더로 정답 시퀀스로부터 잠재 의도 z 추정 (사후 추정)
- **추론 시**: z=0 또는 prior 샘플 사용, CVAE 인코더 불필요
- **확률 모델 역할**과 **신경망 구현**(Transformer)을 명확히 구분

이러한 설계를 통해 ACT는 단 50개의 시연 데이터로 96% 성공률을 달성하며, 모방 학습의 새로운 가능성을 제시했습니다.

---

## 참고 문헌

- Zhao et al. (2023). "Learning Fine-Grained Bimanual Manipulation with Low-Cost Hardware" (ACT/ALOHA paper)
- Kingma & Welling (2014). "Auto-Encoding Variational Bayes" (VAE)
- Sohn et al. (2015). "Learning Structured Output Representation using Deep Conditional Generative Models" (CVAE)
- Vaswani et al. (2017). "Attention Is All You Need" (Transformer)
