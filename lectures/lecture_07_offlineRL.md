# Lecture 7: 오프라인 강화학습 (Offline Reinforcement Learning)

## 목차
- [학습 목표](#학습-목표)
- [오프라인 RL의 정의](#오프라인-rl의-정의)
  - [오프라인 RL vs 온라인 RL](#오프라인-rl-vs-온라인-rl)
  - [수식적 정의](#수식적-정의)
  - [오프라인 RL vs 행동 복제](#오프라인-rl-vs-행동-복제)
- [오프라인 RL의 도전과제](#오프라인-rl의-도전과제)
  - [분포 불일치 문제](#분포-불일치-문제)
  - [적용 분야](#적용-분야)
  - [Q-value 과대평가 문제](#q-value-과대평가-문제)
- [해결책 1: 정책 제약 (TD3+BC)](#해결책-1-정책-제약-td3bc)
- [해결책 2: 보수적 Q 추정 (CQL)](#해결책-2-보수적-q-추정-cql)
  - [CQL의 핵심 아이디어](#cql의-핵심-아이디어)
  - [CQL 수식](#cql-수식)
  - [CQL 구현](#cql-구현)
- [해결책 3: 보상 기반 접근법](#해결책-3-보상-기반-접근법)
  - [Filtered Behavioral Cloning](#filtered-behavioral-cloning)
  - [Advantage-Weighted Regression (AWR)](#advantage-weighted-regression-awr)
- [해결책 4: Implicit Q-Learning (IQL)](#해결책-4-implicit-q-learning-iql)
  - [Expectile Regression](#expectile-regression)
  - [IQL 알고리즘](#iql-알고리즘)
  - [성능 비교](#성능-비교)
- [오프라인 RL 평가 방법](#오프라인-rl-평가-방법)
  - [D4RL 벤치마크](#d4rl-벤치마크)
  - [벤치마크 결과](#벤치마크-결과)
- [실제 응용 사례](#실제-응용-사례)
  - [로보틱스](#로보틱스)
  - [핵융합 제어](#핵융합-제어)
  - [빌딩 냉각 시스템](#빌딩-냉각-시스템)
  - [칩 디자인](#칩-디자인)
- [요약](#요약)

---

## 학습 목표

이번 강의에서는 현대 강화학습 연구에서 매우 중요하고 실용적인 분야로 각광받고 있는 **오프라인 강화학습(Offline Reinforcement Learning)**을 다룹니다.

다음 주제들을 순차적으로 학습합니다:

* **오프라인 RL(Offline RL)**의 정의와 기존의 온라인 RL과의 차이점, 그리고 왜 필요한지에 대한 근본적인 이해
* 오프라인 RL의 문제를 해결하기 위한 여러 대표적인 알고리즘들:
    * **TD3+BC**: 기존 Off-policy 알고리즘인 TD3에 Behavioral Cloning을 결합한 간단하면서도 효과적인 접근법
    * **Conservative Q-Learning (CQL)**: Q-value 자체를 보수적으로 추정하려는 중요한 알고리즘
    * **Filtered BC**: 좋은 데이터만 골라서 모방하려는 아이디어
    * **Advantage-Weighted Regression (AWR)**: 단순한 모방을 넘어, '얼마나 좋은지'를 가중치로 주어 학습하는 방식
    * **Implicit Q-Learning (IQL)**: 가장 정교한 접근법 중 하나

이번 강의에서는 특히 오프라인 RL의 정의와 핵심 난제, 그리고 이를 해결하기 위한 접근법인 TD3+BC와 CQL을 중심으로 자세히 다룹니다.

---

## 오프라인 RL의 정의

### 오프라인 RL vs 온라인 RL

**오프라인 강화학습**의 핵심 키워드는 **주어진 정적 데이터로부터의 학습(learns from given static data)**입니다.

기존의 온라인 RL과의 차이점을 비교해 보겠습니다:

* **온라인 RL (On-policy 또는 Off-policy)**:
    * 에이전트가 환경과 **실시간으로 상호작용**합니다.
    * 직접 데이터를 수집하고, 그 최신 데이터를 바탕으로 (혹은 리플레이 버퍼에 쌓인 데이터를 바탕으로) 정책을 업데이트합니다.
    * 핵심은 **탐험(Exploration)**, 즉 새로운 행동을 시도하고 그 결과를 피드백 받는 과정이 가능하다는 것입니다.

* **오프라인 RL**:
    * 에이전트와 환경 간의 **상호작용이 일절 허용되지 않습니다**.
    * 대신, 우리에게는 누군가가 이미 수집해 놓은 **정적인(static) 데이터셋**이 주어집니다.
    * 우리의 유일한 임무는 이 고정된 오프라인 데이터만을 가지고 정책을 학습시키는 것입니다. 더 이상의 데이터 수집은 없습니다.

---

### 수식적 정의

오프라인 RL을 수식적으로 정의해 보겠습니다.

우리에게 주어진 정적 데이터셋을 $D$라고 합니다. 이 $D$는 $(s, a, s', r)$ 튜플들의 집합입니다. ($s$: 상태, $a$: 행동, $s'$: 다음 상태, $r$: 보상)

이 데이터셋 $D$는 어떤 알 수 없는 정책(unknown policy) $\pi_{\beta}$에 의해 수집되었습니다. 우리는 이 $\pi_{\beta}$를 **행동 정책(behavior policy)**이라고 부릅니다.

* 데이터 $D$의 상태 $s$는 $\pi_{\beta}$가 만들어낸 궤적(trajectory) $\tau$에서 샘플링된 것입니다.
* 행동 $a$는 그 상태 $s$에서 $\pi_{\beta}$가 선택한 행동입니다. ($a \sim \pi_{\beta}(\cdot|s)$)

이 $\pi_{\beta}$는 단 하나의 정책일 수도 있지만, 실제로는 여러 다른 정책들(예: 전문가, 초보자, 랜덤 정책)이 섞여 있는 **정책의 혼합(mixture of policies)**일 수도 있습니다.

**오프라인 RL의 목표**는 놀랍게도 온라인 RL과 동일합니다:

$$\max_{\theta} \sum_{t} E_{\tau \sim \pi_{\theta}(\tau)}[r(s_{t}, a_{t})]$$

우리가 새로 학습할 정책 $\pi_{\theta}$를 따랐을 때의 누적 보상 기댓값을 최대화하는 것입니다.

여기서 바로 오프라인 RL의 딜레마가 발생합니다. **"데이터는 $\pi_{\beta}$에서 수집되었는데, 평가는 $\pi_{\theta}$로 해야 한다."** 이 **분포 불일치(Distribution Mismatch)**가 오프라인 RL의 모든 문제의 근원입니다.

---

### 오프라인 RL vs 행동 복제

오프라인 RL과 **행동 복제(Behavioral Cloning, BC)**는 둘 다 정적 데이터셋 $D$를 사용하지만, **목표가 근본적으로 다릅니다**.

* **행동 복제 (BC)**: $\pi_{\theta}$가 $\pi_{\beta}$를 **모방(imitate)**하도록, 즉 $D$에 있는 $(s, a)$ 쌍을 그대로 따라 하도록 학습시킵니다.
* **오프라인 RL**: $D$를 **활용(leverage)**하여, $\pi_{\beta}$보다 **더 나은(better)** 정책 $\pi_{\theta}$를 찾는 것이 목표입니다.

예를 들어, 학습 데이터에 'A에서 B로 가는 궤적'과 'B에서 C로 가는 궤적'만 있다고 가정합니다. BC를 사용한다면, A에서는 B로, B에서는 C로 가는 것만 배울 뿐, 데이터에 없는 'A에서 C로 가는 경로'는 절대 학습할 수 없습니다.

반면, 오프라인 RL의 목표는 데이터에 직접적으로 존재하지 않더라도, **최적의 경로(optimal path)**, 즉 A에서 C로 바로 가는 경로를 찾는 것입니다.

이것이 가능한 이유:

1. **궤적 꿰매기 (Stitching trajectories)**: 오프라인 RL은 (A, B) 궤적에서 A의 정보(Q-value 등)와 (B, C) 궤적에서 C의 정보를 동적 프로그래밍(Dynamic Programming)과 유사하게 조합하여, (A, C)라는 새로운 궤적이 더 좋다는 것을 추론해낼 수 있습니다.
2. **나쁜 데이터의 활용 (Leverage bad data)**: BC는 데이터에 나쁜 행동이 있으면 그것마저 모방하지만, 오프라인 RL은 보상 $r$을 보기 때문에 '이 행동은 보상이 낮으니 피해야겠다'라는 것까지 학습할 수 있습니다.

---

## 오프라인 RL의 도전과제

### 분포 불일치 문제

오프라인 RL이 데이터에 없는 경로도 찾을 수 있다는 것은 매우 강력한 능력이지만, 바로 이 지점이 오프라인 RL을 극도로 어렵게 만드는 이유입니다.

$\pi_{\beta}$가 수집한 학습 데이터의 분포를 고려해 봅시다. 우리 정책 $\pi_{\theta}$가 학습을 하다 보니, "데이터에 있는 이 행동 말고, 데이터에는 없지만 이런 행동을 해보면 어떨까?" 하고 시도해보고 싶어 합니다.

문제는 **이 행동이 좋은 행동일까요, 나쁜 행동일까요? 알 수가 없습니다.**

왜냐하면 우리는 **오프라인**이기 때문입니다. 환경과 상호작용이 금지되어 있으니, 그 행동을 실제로 해보고 결과를 피드백 받을 수가 없습니다. 온라인 RL이었다면 그냥 그 행동을 해보고, 보상이 높은지 낮은지 보면 그만입니다.

> **오프라인 RL은 데이터에 없는 '보지 못한 행동(unseen actions)'을 '안전하게(in a safe way)' 다루어야만 합니다. 동시에, 데이터에 있는 행동들보다 '더 잘해야(doing better than data)'하는 이중고를 겪습니다. 이것이 오프라인 RL의 핵심 딜레마입니다.**

---

### 적용 분야

오프라인 RL이 유용한 경우:

1. **온라인 데이터 수집이 위험하거나, 비싸거나, 비윤리적일 때**
    * **헬스케어**: 환자에게 잘못된 치료법을 '탐험'해볼 수 없습니다.
    * **자율 주행**: 실제 도로에서 사고를 내면서 학습할 수 없습니다.
    * **로보틱스**: 고가의 로봇이 망가지는 것을 감수하며 데이터를 수집하기 어렵습니다.
    * 이런 분야에서는 이미 수집된 안전한 로그 데이터를 가지고 최적의 정책을 학습해야 합니다.

2. **기존에 수집된 데이터를 재사용(Reuse)하고 싶을 때**
    * 과거에 수행했던 실험 데이터, 다른 프로젝트에서 나온 데이터, 심지어 다른 기관의 데이터 등, 이미 존재하는 방대한 데이터를 활용할 수 있다면 엄청난 자원 절약이 됩니다.

3. **데이터 수집 주체가 현재 정책이 아닐 때**
    * 예를 들어, **사람이 직접 시연한 데이터(human demonstration)**나 기존에 있던 레거시 시스템이 수집한 데이터를 활용하여 더 나은 정책을 만들고 싶을 때 사용됩니다.

"어차피 데이터 재사용하는 거, 기존의 off-policy 알고리즘 (DQN이나 SAC 같은) 그냥 쓰면 안 되나요?"라는 의문이 들 수 있습니다. 이 질문에 대한 답이 오프라인 RL의 핵심입니다.

---

### Q-value 과대평가 문제

**기존 off-policy 알고리즘을 그대로 가져다 쓰면 심각한 문제가 발생합니다.**

Q-러닝의 타겟 값 $y_i$는 다음과 같이 계산됩니다:

$$y_{i} \leftarrow r(s_{i}, a_{i}) + \gamma \max_{a'} Q_{\phi}(s_{i}', a_{i}')$$

이 수식은 벨만 최적 방정식(Bellman optimality equation)을 근사하기 위한 업데이트 룰입니다. (출처: Sutton & Barto, Reinforcement Learning: An Introduction, 2nd Ed., Chapter 6.5 Q-Learning). [외부 자료] 이 타겟 값 $y_i$는 다음 상태 $s_i'$에서 얻을 수 있는 최대의 미래 가치를 의미합니다. [외부 자료]

온라인 상황에서는 $\max_{a'}$ 연산이 큰 문제가 되지 않았습니다. 왜냐하면 우리가 어떤 $a'$에 대해 Q-value를 잘못 추정하더라도, 다음 스텝에서 그 $a'$를 직접 방문해서 $Q(s', a')$의 값을 수정할 기회가 있었기 때문입니다. [외부 자료]

하지만 지금은 오프라인입니다. 수정할 기회가 없습니다.

여기서 문제가 되는 부분이 바로 **$\max_{a'} Q_{\phi}(s_{i}', a_{i}')$** 항입니다. 이 $a'$는 다음 상태 $s_i'$에서 가능한 모든 행동 중에 Q-value를 가장 높게 만드는 행동입니다.

**그런데 이 $a'$가 만약 데이터셋 $D$에 없는 행동, 즉 OOD(Out-Of-Distribution) 행동이라면 어떻게 될까요?**

다음을 고려해 봅시다:
* x축은 행동 $a'$, y축은 Q-value입니다.
* 점선(True Q)이 실제 Q-value이고, 파란색 실선(Predicted Q)이 우리 Q-네트워크가 예측한 값입니다.
* 녹색으로 표시된 **data support**, 즉 학습 데이터가 존재하는 구간에서는 예측이 꽤 정확합니다. 실선과 점선이 비슷합니다.
* 하지만 데이터가 없는 **OOD 영역**을 보면, 예측된 Q-value가 실제 Q-value보다 **터무니없이 높게 추정(over-optimistic)**됩니다.

Q-함수는 데이터가 없는 영역에 대해서는 신뢰할 수 없는(unreliable) 값을 뱉어냅니다. 그런데 $\max$ 연산자는 바로 이 '터무니없이 높은' OOD 행동을 타겟 값으로 선택하게 만듭니다.

이 잘못된 타겟 값이 벨만 업데이트를 통해 계속 전파(propagate)되면, Q-value 전체가 심각하게 **과대평가(overestimated)**됩니다.

---

#### 과대평가의 실제 사례

이것은 단순한 이론이 아니라, 실제 실험으로도 명확히 드러납니다.

SAC라는 표준적인 off-policy 알고리즘을 크기 $n$의 정적 데이터셋(static data)에 돌렸을 때의 결과를 살펴봅시다:

* **AverageReturn 그래프**: 이것은 에이전트의 **실제 성능**입니다. 학습(TrainSteps)이 진행되어도 성능이 오르기는커녕 오히려 떨어지거나 매우 불안정합니다.

* **log(Q) 그래프**: 이것은 에이전트가 **스스로 생각하는 성능(Q-value)**입니다.
    * Q 추정치가 어떻게 됩니까? **폭발적으로 증가합니다(Q estimates explode!)**.
    * 특히 데이터가 적을수록($n=1000$) Q-value가 더 심하게 폭발합니다.

**에이전트는 '나는 지금 엄청 잘하고 있어!'라고 생각하지만 (Q-value 폭발), 실제 성능은 엉망진창인 (Return 하락) 최악의 상황입니다.**

이것이 바로 $\max$ 연산자로 인한 OOD 행동의 **과대평가(Overestimation)** 문제 때문입니다.

---

#### 문제 해결의 핵심 목표

우리의 당면 과제가 명확해졌습니다:

**"오프라인 RL에서 이 '과대평가(overestimation)' 문제를 어떻게 완화(mitigate)할 것인가?"**

이것이 사실상 오프라인 RL의 핵심 목표(core goal)라고 할 수 있습니다.

---

## 해결책 1: 정책 제약 (TD3+BC)

첫 번째 해결책은 매우 직관적인 아이디어입니다.

* **문제**: Q-함수가 OOD 행동(데이터에 없는 행동)에 대해 신뢰할 수 없다.
* **해결책**: 그럼 OOD 행동을 쿼리하지 않으면 된다. 즉, 우리 정책 $\pi$가 데이터에 있는 행동($\pi_{\beta}$가 했던 행동)들과 **가까운(close)** 행동들만 하도록 **제약(constrain)**을 걸자.

이 아이디어를 구현한 것이 **TD3+BC**입니다.

기존 TD3의 정책 업데이트 식은 $\pi = \arg \max_{\pi} \mathbb{E}_{s \sim D}[Q(s, \pi(s))]$로, 그냥 Q-value를 최대로 만드는 $\pi$를 찾습니다.

여기에 제약 항을 추가합니다. 바로 BC(Behavioral Cloning) 손실 항입니다.

> **TD3+BC 정책 업데이트:**
>
> $$\pi = \arg \max_{\pi} \mathbb{E}_{(s,a) \sim D}[\lambda Q(s, \pi(s)) - (\pi(s) - a)^2]$$

이 식의 의미:

1. **$\lambda Q(s, \pi(s))$**: Q-value를 최대화하라 (RL의 목표)
2. **$- (\pi(s) - a)^2$**: 우리 정책 $\pi(s)$가 뱉어낸 행동과, 데이터셋 $D$에 있는 실제 행동 $a$ ($\pi_{\beta}$의 행동) 사이의 거리(MSE)를 최소화하라 (BC의 목표)

결국 TD3+BC는 **Q-value도 높이면서 동시에 데이터에 있던 행동과 너무 멀어지지 말라**는 두 가지 목표를 동시에 달성하려는, 매우 미니멀한 접근법입니다.

---

## 해결책 2: 보수적 Q 추정 (CQL)

### CQL의 핵심 아이디어

TD3+BC 같은 정책 제약(policy constraint) 접근법은 직관적이지만 문제가 있습니다:

1. **너무 비관적 (Too pessimistic)**: 만약 $\pi_{\beta}$가 형편없는 랜덤 정책이었다면, 그 나쁜 정책을 따라 하라고 제약하는 꼴이 되니까요. 데이터보다 더 나은 정책을 찾기 힘들어집니다.
2. **충분히 비관적이지 않음 (Not pessimistic enough)**: 정책이 데이터에서 살짝만 벗어나도, Q-value 과대평가가 발생할 수 있습니다.

두 번째 해결책은 정책 $\pi$를 제약하는 대신, **Q-function 자체를 보수적으로 학습**시키자는 **보수적인 Q 추정(Conservative Q estimation)**입니다.

이 아이디어를 구현한 대표적인 알고리즘이 바로 **Conservative Q-Learning (CQL)**입니다.

* **CQL의 아이디어**: Q-function을 학습시킬 때, OOD 행동들이 절대 높은 값을 갖지 못하도록 훈련시킵니다.

---

#### 보수적 Q-함수 학습하기

CQL의 아이디어를 시각적으로 이해해 봅시다:

* **Naïve Q-function**:
    * 데이터가 있는 'Action support' 영역 밖(OOD)에서, Naïve Q가 Actual Q보다 훨씬 높게 과대평가되는 것을 볼 수 있습니다.

* **Conservative Q-function**:
    * 이것이 CQL이 만들고자 하는 Q-함수입니다.
    * 데이터가 없는 OOD 영역에서 Q-value를 **의도적으로 낮춥니다(underestimating)**. 억지로 눌러버리는 것입니다.
    * 반면, 데이터가 있는 'Action support' 영역 안에서는 실제 Q-value를 잘 따르도록 합니다.

이렇게 Q-function 자체를 보수적으로 만들면, 나중에 정책이 $\max Q$를 찾는다고 해도 OOD 행동을 선택할 위험이 줄어듭니다.

---

### CQL 수식

이 아이디어를 어떻게 수식으로 구현할까요? 바로 Q-function의 손실 함수(loss function)를 수정합니다.

전체적인 CQL의 손실 함수(Loss)는 세 부분으로 나뉩니다:

1. **Standard Critic Update (벨만 에러)**
    * $\mathbb{E}_{(s,a,s') \sim D}[(Q(s,a) - (r(s,a) + \gamma \mathbb{E}_{\pi}[Q(s',a')]))^2]$
    * 이것은 그냥 스탠더드한 벨만 에러입니다. Q-value가 타겟 값을 잘 맞추도록 하는 항입니다.

2. **OOD Q-value 억누르기 (Push down)**
    * $+ \alpha \mathbb{E}_{s \sim D, a \sim \mu(\cdot|s)}[Q(s,a)]$
    * $\mu$는 OOD 행동을 샘플링하는 분포(예: 랜덤 분포)입니다.
    * 즉, 데이터에 없을 법한 행동 $a$를 뽑아서, 그 행동의 Q-value $Q(s,a)$를 **최소화(minimize)**하라는 (전체 식이 $\min_Q$ 니까요) 정규화(regularization) 항입니다.
    * 이것이 바로 OOD 영역의 Q-value를 억누르는 역할을 합니다.

3. **In-Data Q-value 밀어 올리기 (Push up)**
    * $- \alpha \mathbb{E}_{(s,a) \sim D}[Q(s,a)]$
    * 2번처럼 OOD 영역만 억누르면, Q-function이 그냥 모든 값을 0으로 뱉어버리는 'trivial solution'에 빠질 수 있습니다.
    * 그래서 이 항을 추가합니다. 마이너스(-)가 붙어있습니다.
    * 샘플링을 $(s,a) \sim D$, 즉 **데이터셋 $D$ 안에서** 합니다.
    * 전체 $\min_Q$ 식에 마이너스가 붙었으니, 이 항은 $Q(s,a)$를 **최대화**하라는 의미가 됩니다.
    * 즉, 데이터에 있는(in-data) 행동 $(s,a)$에 대해서는 Q-value를 밀어 올리라는 것입니다.

**결론: CQL은 '데이터에 있는' 행동의 Q-value는 높이고, '데이터에 없는' 행동의 Q-value는 낮추도록 Q-함수를 학습시킵니다.**

---

### CQL 구현

실용적인 문제가 남았습니다. "데이터에 없을 법한 행동을 샘플링하는 분포 $\mu$를 어떻게 찾고, $\mathbb{E}_{a \sim \mu}[Q(s,a)]$ 이 항을 어떻게 계산할까요?"

여기서 수학적 트릭이 들어갑니다. (자세한 유도는 논문을 참고해야 합니다.) 결론만 말하면, $\mu$에 대한 엔트로피 정규화(max entropy regularizer) $R(\mu)$를 추가하면, 최적의 $\mu(a|s)$는 $\exp(Q(s,a))$에 비례하는 형태가 됩니다.

그리고 이 경우, 우리가 계산해야 했던 $\mu$에 대한 기댓값 항과 $R(\mu)$ 항이 깔끔하게 **$\mathbb{E}_{s \sim D}[\log \Sigma_{a} \exp(Q(s,a))]$** 라는 하나의 항으로 합쳐집니다.

* 이 $\log \Sigma_{a} \exp$ 형태를 **log-sum-exp**라고 부릅니다. 이는 $\max$ 연산의 'soft' 버전입니다.
* 행동 공간이 이산적(discrete)이라면 이 값을 정확히 계산할 수 있고,
* 연속적(continuous)이라면 $Q(s,a)$가 높은 영역에서 몇 개의 행동을 샘플링하여 근사(approximate)합니다.

---

#### CQL 요약

우리가 구현해야 할 **CQL의 최종 손실 함수 $L_{CQL}$**는 다음과 같습니다:

$$L_{CQL}(\phi) = L_{Bellman}(\phi) + \alpha \cdot \left( \mathbb{E}_{s \sim D}[\log \sum_{a} \exp(Q_{\phi}(s,a))] - \mathbb{E}_{(s,a) \sim D}[Q_{\phi}(s,a)] \right)$$

* $L_{Bellman}(\phi)$: 표준 벨만 에러
* $\mathbb{E}[\log \sum \exp(Q)]$ : OOD 행동 Q-value 억누르기 (push-down)
* $-\mathbb{E}[Q]$ : In-data 행동 Q-value 밀어 올리기 (push-up)

알고리즘 자체는 SAC 같은 기존 off-policy 알고리즘에서 Q-function의 손실 함수(critic loss)만 이 $L_{CQL}$로 교체하면 됩니다. 정책(policy) 업데이트는 그냥 SAC와 동일하게 합니다.

* **장점**: 아이디어가 단순하고 실제로 성능이 매우 잘 나옵니다.
* **단점**: 정규화 강도 $\alpha$ 값을 튜닝하는 것이 매우 중요하고, log-sum-exp를 근사하는 등 몇 가지 구현상의 핵(hack)이 필요합니다.

---

## 해결책 3: 보상 기반 접근법

### Filtered Behavioral Cloning

이 질문에 답하기 위해, 연구자들은 Q-러닝의 복잡한 세계에서 잠시 벗어나 가장 단순했던 **행동 복제(Behavioral Cloning, BC)**로 다시 눈을 돌렸습니다.

기존 BC의 문제는 데이터에 '좋은 궤적'과 '나쁜 궤적'이 섞여있을 때, BC는 보상 $r$을 보지 않기 때문에 이 둘을 구분하지 못하고 **똑같이 모방한다(Treat good and bad trajectories equally)**는 것입니다.

가장 단순한 해결책은 **'좋은 궤적만 골라서(Filtered) 모방하자'**는 것입니다. 이것이 바로 **Filtered Behavioral Cloning (K% BC)**입니다.

알고리즘은 지극히 단순합니다:

1. 데이터셋 $D$에 있는 모든 궤적 $\tau$에 대해 총 보상(return), 즉 $r(\tau) = \sum_t r(s_t, a_t)$를 계산합니다.
2. 이 총 보상을 기준으로 궤적들을 정렬(Rank)한 뒤, **상위 K%에 해당하는 궤적**만 남겨 새로운 데이터셋 $\tilde{D}$를 만듭니다.
3. 이 '좋은 데이터' $\tilde{D}$에 대해서만 표준적인 BC(Imitation Learning)를 수행합니다.

---

### Advantage-Weighted Regression (AWR)

Filtered BC도 여전히 문제가 있습니다:

1. **궤적 내 동일 취급**: '좋은 궤적'으로 필터링되었더라도, 그 궤적 안에는 상대적으로 '덜 좋은 행동(transition)'이 포함될 수 있습니다. 하지만 Filtered BC는 이들을 **모두 동등하게 모방**합니다.
2. **이진(Binary) 보상**: K% 안에 드느냐 마느냐, 즉 '좋다/나쁘다'의 이진적인 정보만 사용할 뿐, '얼마나' 좋은지는 활용하지 못합니다.

이 문제를 해결하기 위해, "각 행동(transition)이 '얼마나' 좋은지에 따라 가중치를 주어 모방하자!"는 아이디어가 나옵니다.

이때 '얼마나 좋은가'를 측정하는 가장 고전적이고 강력한 지표가 바로 **어드밴티지(Advantage)**입니다:

$$A(s, a) = Q(s, a) - V(s)$$

어드밴티지 함수 $A(s,a)$에 대해:
* $V(s)$는 상태 $s$에서의 평균적인 가치입니다.
* $Q(s, a)$는 상태 $s$에서 특정 행동 $a$를 했을 때의 가치입니다.
* 따라서 $A(s,a)$가 양수(+)라는 것은, 행동 $a$가 **평균보다 좋았다**는 뜻입니다.
* 반대로 $A(s,a)$가 음수(-)라는 것은, 행동 $a$가 **평균보다 나빴다**는 뜻입니다.

이 어드밴티지 개념은 강화학습 교과서(Sutton & Barto)의 정책 경사(Policy Gradient) 챕터에서 '베이스라인(baseline)'으로 V-function을 사용하는 Actor-Critic 알고리즘의 핵심입니다. (출처: Sutton & Barto, Reinforcement Learning: An Introduction, 2nd Ed., Chapter 13.4 Advantage Actor-Critic) [외부 자료]

이 어드밴티지를 가중치로 사용하는 회귀 분석, 이것이 바로 **Advantage-Weighted Regression (AWR)**입니다.

---

#### AWR의 알고리즘

AWR은 2단계로 작동합니다:

**1. V-function 학습 (Fit $\hat{V}^{\pi_{\beta}}$):**

먼저 데이터 수집 정책($\pi_{\beta}$)의 V-function $\hat{V}^{\pi_{\beta}}$를 근사해야 합니다. 데이터셋 $D$의 각 상태 $s_t$에서 궤적이 끝날 때까지 얻은 실제 총 보상(몬테카를로 리턴) $R_t(\tau)$가 있습니다. $\hat{V}(s_t)$가 이 $R_t(\tau)$와 비슷해지도록 지도 학습(L2 regression)을 합니다:

$$L_V = \mathbb{E}_{(s_t, a_t) \sim D}[(R_t(\tau) - V(s_t))^2]$$

**2. 정책 학습 (Train $\hat{\pi}$):**

이제 어드밴티지를 계산할 수 있습니다: $\hat{A}(s_t, a_t) = R_t(\tau) - \hat{V}^{\pi_{\beta}}(s_t)$

이 어드밴티지를 가중치로 사용하여 가중 BC(weighted BC)를 수행합니다:

$$\max_{\pi} \mathbb{E}_{(s_t, a_t) \sim D}[\log \pi(a_t|s_t) \cdot \exp(\beta \cdot \hat{A}(s_t, a_t))]$$

* $\log \pi(a_t|s_t)$는 스탠더드한 BC의 목적 함수입니다.
* $\exp(\beta \cdot \hat{A})$가 가중치입니다. $\beta$는 가중치의 세기를 조절하는 하이퍼파라미터입니다.
* $\exp$ 함수를 썼기 때문에 이 가중치는 항상 양수입니다.
* 어드밴티지가 큰(좋은) 행동 $a_t$는 $\log \pi$에 큰 가중치가 곱해져서 모방 확률이 높아지고, 어드밴티지가 작은(나쁜) 행동은 가중치가 0에 가까워져서 거의 모방되지 않습니다.

---

#### AWR의 장단점

* **장점**:
    * 매우 간단합니다(Simple).
    * **결정적으로, OOD 행동에 대한 쿼리가 전혀 없습니다!** 학습에 사용하는 모든 $(s, a)$ 쌍이 데이터셋 $D$에서 온 것들입니다.
* **단점**:
    * 어드밴티지를 계산할 때 몬테카를로 리턴 $R_t(\tau)$를 사용하는데, 이는 분산(variance)이 매우 커서 노이즈가 심합니다(noisy).
    * 우리가 계산한 $\hat{A}^{\pi_{\beta}}$는 데이터 수집 정책 $\pi_{\beta}$에 대한 어드밴티지입니다. 하지만 우리가 정말 원하는 것은 우리가 새로 학습하는 정책 $\pi_{\theta}$에 대한 어드밴티지 $\hat{A}^{\pi_{\theta}}$입니다. $\hat{A}^{\pi_{\beta}}$는 그보다 약한(weaker) 신호입니다.

---

## 해결책 4: Implicit Q-Learning (IQL)

### OOD 추정 없는 Q-러닝

AWR은 'OOD 쿼리를 피한다'는 아주 중요한 실마리를 제공했습니다. 그렇다면 이 아이디어를 Q-러닝에 다시 적용해 볼 수는 없을까요?

Q-러닝의 손실 함수를 다시 봅시다:

$$L_Q(\phi) = \mathbb{E}[(Q_{\phi}(s,a) - (r + \gamma \mathbb{E}_{a' \sim \pi}[Q_{\phi}(s', a')]))^2]$$

문제는 $\mathbb{E}_{a' \sim \pi}$ 이 부분이 OOD 쿼리를 발생시킨다는 것이었습니다.

**해결책: $a'$도 데이터셋 $D$에서 가져옵시다!** 이것은 Q-러닝이 아니라 **SARSA** 스타일의 업데이트입니다.

여기서 SARSA에 대해 잠시 짚고 넘어가겠습니다. SARSA는 $(S, A, R, S', A')$ 튜플을 사용하여 Q-function을 업데이트하는 방식입니다. (출처: Sutton & Barto, Reinforcement Learning: An Introduction, 2nd Ed., Chapter 6.4 Sarsa) [외부 자료]

$$Q(s,a) \leftarrow Q(s,a) + \alpha [r + \gamma Q(s', a') - Q(s,a)]$$

온라인 RL에서는 $a'$가 $\pi(s')$에서 샘플링된 다음 행동입니다. 오프라인 RL에서는 이 형태를 차용하여, 데이터셋 $D$에서 $(s, a, r, s', a')$ 5-튜플을 통째로 샘플링하여 손실 함수를 구성합니다:

$$L_Q(\phi) = \mathbb{E}_{(s,a,r,s',a') \sim D}[(Q_{\phi}(s,a) - (r + \gamma Q_{\phi'}(s', a')))^2]$$

이 식에는 $\max$ 연산자도, $\pi$에 대한 기댓값도 없습니다. **OOD 쿼리가 완벽히 사라졌습니다.**

---

### '최선'의 가치 함수 학습하기

하지만 이 단순 SARSA 접근법도 문제가 있습니다.

데이터셋 $D$에서 샘플링된 $a'$는 $\pi_{\beta}$가 했던 행동일 뿐입니다. 여기에는 **좋은 행동과 나쁜 행동이 모두 포함**되어 있습니다. 우리가 원하는 것은 $s'$에서 $\pi_{\beta}$가 했던 평균적인 행동 $a'$의 $Q(s', a')$ 값이 아니라, $\pi_{\beta}$가 했던 행동들 중에서 **가장 좋은(best)** 행동의 가치입니다.

즉, 타겟이 $r + \gamma Q(s', a')$ (평균적인 $a'$)이 아니라, $r + \gamma V^*(s')$ (최적의 $V$)가 되어야 합니다. 하지만 $V^*(s') = \max_{a'} Q(s', a')$를 계산하는 순간 다시 OOD 문제가 발생합니다.

여기서 **Implicit Q-Learning (IQL)**의 천재적인 아이디어가 나옵니다. $\max$ 연산을 명시적으로(explicitly) 하지 말고, $V(s)$ 함수 자체가 $Q(s, a)$의 최대값에 가깝도록 **암묵적으로(implicitly)** 학습시키자는 것입니다.

$s$에서 가능한 여러 $a$들에 대한 $Q(s,a)$ 값들의 분포(히스토그램)를 고려해 봅시다:

* **$\mathbb{E}_a[Q(s,a)]$ (평균)**: 일반적인 V-function 학습($L_2$ loss)은 $V(s)$를 이 평균값에 맞추도록 학습합니다.
* **$\max_a Q(s,a)$ (최대)**: 이것이 우리가 진짜 원하는 값입니다.
* **IQL의 V(s) (Expectile)**: IQL은 $V(s)$가 평균보다는 높고, 최대값보다는 약간 낮은, 이 분포의 **상위 분위수(expectile)**에 위치하도록 학습시킵니다.

---

### Expectile Regression

어떻게 이것이 가능할까요? 바로 **Expectile Regression(기대 회귀)**라는 기법을 사용합니다:

$$L_V(\psi) = \mathbb{E}_{(s,a) \sim D}[L_2^\tau(Q_{\phi'}(s,a) - V_\psi(s))]$$

여기서 $\tau$는 일반적인 L2 loss($\tau=0.5$)와 달리, 0과 1 사이의 비대칭성을 제어하는 값입니다. IQL은 $\tau \approx 0.9$ 같은 높은 값을 사용합니다. $\tau=0.9$는 $V_\psi(s)$가 $Q_\phi(s, \cdot)$의 상위 10% 정도에 해당하는 값들을 피팅하도록 만듭니다.

**Expectile Regression Loss $L_2^\tau(x)$**가 어떻게 작동하는지 수식을 봅시다. 여기서 $x = \text{target} - \text{prediction}$, 즉 $x = Q(s,a) - V(s)$ 입니다.

$$L_2^\tau(x) = \begin{cases} (1-\tau)x^2 & \text{if } x < 0 \\ \tau x^2 & \text{otherwise } (x \ge 0) \end{cases}$$

$\tau = 0.9$를 대입해 봅시다:

$$L_2^{0.9}(x) = \begin{cases} 0.1 x^2 & \text{if } x < 0 \quad (\text{즉, } V(s) > Q(s,a)) \\ 0.9 x^2 & \text{if } x \ge 0 \quad (\text{즉, } V(s) \le Q(s,a)) \end{cases}$$

이 비대칭적인 손실이 어떤 효과를 낳을까요?

* **$V(s)$가 타겟 $Q(s,a)$보다 클 때 ($x < 0$):**
    손실(loss)이 $0.1x^2$로 매우 작습니다. 즉, 경사 하강법이 $V(s)$를 $Q(s,a)$ 쪽으로 세게 끌어내리지 않습니다. $V(s)$가 높은 값을 유지하도록 용인해 줍니다.

* **$V(s)$가 타겟 $Q(s,a)$보다 작을 때 ($x \ge 0$):**
    손실이 $0.9x^2$로 매우 큽니다. $V(s)$는 $Q(s,a)$ 방향으로 강하게 끌어올려집니다.

결과적으로, $V(s)$는 대부분의 $Q(s,a)$ 값들보다는 위에 위치하고, $Q(s,a)$ 값들 중 높은 값들에 의해 그 위치가 결정됩니다. 즉, $V(s)$는 **높은 타겟(higher targets)들을 추종**하게 됩니다.

---

### IQL 알고리즘

이제 IQL의 전체 그림을 그릴 수 있습니다. IQL은 Q-function, V-function, 그리고 Policy(actor)를 **분리(decoupling)**하여 학습시킵니다.

**1. V-function 업데이트 (암묵적 최대값 학습):**

먼저 $V_\psi(s)$를 Expectile Regression을 통해 학습합니다:

$$L_V(\psi) = \mathbb{E}_{(s,a) \sim D}[L_2^\tau(Q_{\phi}(s,a) - V_{\psi}(s))]$$

* 이때 $Q_\phi$는 고정(frozen)된 타겟 Q-network입니다.
* 이 식은 $V_\psi(s)$가 $\mathbb{E}[Q_\phi(s, \cdot)]$가 아닌 $\text{expectile}_{\tau}[Q_\phi(s, \cdot)]$ (즉, 상위 $Q$ 값)를 배우도록 합니다.

**2. Q-function 업데이트 (OOD 없는 타겟 사용):**

$Q_\phi(s,a)$를 학습합니다. 이때 타겟으로 OOD 쿼리($\max Q$) 대신 방금 학습한 **$V_\psi(s')$**를 사용합니다:

$$L_Q(\phi) = \mathbb{E}_{(s,a,r,s') \sim D}[((r + \gamma V_{\psi}(s')) - Q_{\phi}(s,a))^2]$$

* 이것은 OOD 쿼리가 없는 단순한 L2 회귀입니다. $Q_\phi$는 $V_\psi$가 알려주는 암묵적 최대 가치를 향해 학습됩니다.

**3. 정책(Actor) 추출 (AWR 사용):**

마지막으로, 학습된 $Q_\phi$와 $V_\psi$를 이용해 정책 $\pi_\theta$를 학습시킵니다. AWR을 사용합니다:

$$\hat{A}(s,a) = Q_\phi(s,a) - V_\psi(s)$$

$$L_\pi(\theta) = \mathbb{E}_{(s,a) \sim D}[\log \pi_\theta(a|s) \cdot \exp(\beta \cdot \hat{A}(s,a))]$$

* $Q_\phi$는 (s,a)의 가치, $V_\psi$는 (s)의 암묵적 최대(평균 이상) 가치이므로, 이 $\hat{A}$는 매우 효과적인 어드밴티지 신호가 됩니다.

---

### 성능 비교

**IQL의 장단점:**

* **장점**:
    * Q-learning, V-learning, Policy learning 어느 단계에서도 **OOD 쿼리를 사용하지 않습니다**.
    * 정책은 오직 데이터에 있는 $(s,a)$ 쌍에 대해서만 학습됩니다.
    * Actor와 Critic 학습이 분리되어(decoupling) 계산적으로 빠르고(fast) 매우 안정적입니다.
    * 실전에서 성능이 매우 뛰어납니다(Work very well).
* **단점**:
    * Expectile $\tau$와 AWR $\beta$라는 두 개의 핵심 하이퍼파라미터를 튜닝하는 것이 중요합니다.

D4RL 벤치마크 결과:

* **antmaze-v0 total (미로찾기 총점)**:
    * BC: 100.2
    * TD3+BC: 163.8
    * CQL: 303.6
    * **IQL: 378.0**
    * 궤적 꿰매기(stitching)가 필수적인 복잡한 'antmaze' 태스크에서, IQL이 CQL마저 압도하는 것을 볼 수 있습니다. 이는 OOD 쿼리를 완전히 제거한 전략이 매우 효과적임을 보여줍니다.

* **total (locomotion + antmaze)**:
    * CQL: 1002.1
    * **IQL: 1070.4**
    * 전체 총점에서도 IQL이 가장 높은 성능을 보여줍니다.

* **runtime (학습 시간)**:
    * CQL: 80m (80분)
    * **IQL: 20m (20분)**
    * 성능은 더 좋은데, 학습 시간은 CQL의 1/4 수준입니다. Actor-Critic을 분리한 덕분에 매우 효율적입니다.

---

## 오프라인 RL 평가 방법

### D4RL 벤치마크

오프라인 RL 알고리즘들의 성능은 어떻게 평가할까요?

온라인 RL처럼 환경에 돌려보면 되는 것 아니냐고요? 안 됩니다. 오프라인 RL의 전제는 환경과의 상호작용 금지입니다. 무엇보다, 표준화된 데이터셋이 필요합니다.

그래서 오프라인 RL 평가를 위한 표준 벤치마크 데이터셋이 등장합니다. 바로 **D4RL(Datasets for Deep Data-Driven RL)**입니다.

D4RL은 다양한 품질의 데이터셋을 제공합니다. 예를 들어, MuJoCo (물리 시뮬레이션) 태스크에서, 1백만 개 샘플을 기준으로,

* `random`: 완전히 무작위 정책이 수집한 데이터. (최악의 데이터)
* `medium-replay`: 어느 정도 학습된 정책이 수집한 (좋고 나쁨이 섞인) 데이터.
* `medium-expert`: 중간 수준 정책과 전문가 정책이 수집한 데이터를 섞은 데이터.

알고리즘이 `random` 데이터만 보고도 `expert` 수준의 정책을 학습할 수 있다면 정말 대단하겠죠? D4RL은 이런 다양한 시나리오에서 알고리즘의 강건함(robustness)을 테스트할 수 있게 해줍니다.

---

### 벤치마크 결과

D4RL 벤치마크에서 여러 알고리즘의 성능을 비교한 결과입니다.

우리가 배운 알고리즘들을 봅시다: 'BC' (Behavioral Cloning), 'TD3+BC', 그리고 'CQL'입니다.

* **locomotion-v2 total (이동 작업 총점)**:
    * BC: 466.7
    * TD3+BC: 677.4
    * CQL: 698.5
    * BC 대비 TD3+BC와 CQL의 점수가 훨씬 높습니다. 이는 이 알고리즘들이 단순 모방(BC)을 넘어 데이터보다 더 나은 정책을 학습했다는 증거입니다.

* **antmaze-v0 total (미로 찾기 작업 총점)**:
    * BC: 100.2
    * TD3+BC: 163.8
    * CQL: 303.6
    * 'antmaze' 같이 궤적 꿰매기(stitching)가 필수적인 복잡한 작업에서는, BC나 TD3+BC보다 CQL의 성능이 압도적으로 높습니다. 이는 CQL이 OOD 문제를 더 잘 해결하면서 데이터에 없는 최적의 경로를 잘 찾아낸다는 것을 보여줍니다.

* **runtime (실행 시간)**:
    * BC (10m)나 TD3+BC (20m)에 비해 CQL (80m)이 훨씬 오래 걸리는 것을 볼 수 있습니다. 복잡한 log-sum-exp 계산과 정규화 항 때문에 계산 비용이 비싼 편입니다.

---

## 실제 응용 사례

### 로보틱스

로봇을 실제 환경에서 탐험시키며 학습시키는 것은 매우 위험하고 비쌉니다.

대신, 'Bridge Dataset' 같은 크고 다양한 오프라인 데이터(전문가의 시연, 랜덤 탐험 등)로 정책을 **사전 학습(Pre-training)**시킵니다. 그다음, 우리가 진짜 풀고 싶은 'Target dataset'에 대해 파인튜닝(Fine-tuning)을 합니다. 이후 실제 환경에서 약간의 온라인 파인튜닝을 결합하면 빠르고 안전하게 로봇을 학습시킬 수 있습니다.

---

### 핵융합 제어

이것은 DeepMind의 실제 연구 사례입니다. 토카막(TCV)이라는 핵융합 장치 내부의 플라즈마(plasma)를 제어하는 문제입니다.

실제 장치로 학습하는 것은 불가능에 가깝습니다. 대신, 매우 정교한 **시뮬레이션 환경(Simulated environment)** 안에서 정책을 학습시킵니다. 이 시뮬레이터에서 수집된 방대한 데이터를 오프라인 데이터셋(Replay buffer)으로 사용하여 정책을 학습(Learner)시킨 뒤, 학습된 정책(Control policy)을 **실제 장치(Deployment)**에 배포하여 플라즈마를 성공적으로 제어했습니다.

---

### 빌딩 냉각 시스템

빌딩의 에너지 효율을 최적화하는 문제입니다.

실제 빌딩 시스템(Real building system)에서 얻은 데이터를 바탕으로 정교한 빌딩 에너지 모델(Building energy model)이라는 시뮬레이터를 만듭니다 (Calibration). DRL 에이전트는 이 시뮬레이터와 상호작용하며(Training) 최적의 냉각 정책을 오프라인으로 학습합니다. 학습이 완료되면, 실제 빌딩에 배포(Deployment)하여 에너지를 절감합니다.

---

### 칩 디자인

반도체 칩의 회로(circuit)를 어떻게 배치하고 연결(Synthesis)할지 결정하는 문제입니다.

Q-network가 에이전트(PrefixRL Agent)가 되어 "add (3,2)" 같은 행동($a_t$)을 취합니다. 그러면 회로 합성(Circuit Synthesis)이라는 환경이 이 행동을 반영하여 실제 회로를 만들고($s_{t+1}$), 이전 회로($s_t$) 대비 칩의 면적(area)과 지연 시간(delay)이 얼마나 줄었는지 계산하여 보상($r_t$)을 줍니다. 이 sequential decision-making 문제를 오프라인 RL로 학습하여 칩 디자인을 최적화합니다.

---

## 요약

### 핵심 과제 재확인

* **오프라인 RL이란?** (Batch RL이라고도 불립니다)
    * 환경과의 상호작용 없이, 주어진 **정적 데이터셋(static dataset)**으로부터 정책을 학습하는 것입니다.

* **왜 유용한가?**
    * 실제 환경에서의 데이터 수집(online data)이 비싸거나, 위험하거나, 비윤리적일 수 있기 때문입니다. 헬스케어, 자율주행, 로보틱스가 대표적입니다. 따라서 기존 데이터를 재사용(Reusing)하는 것은 매우 가치 있는 일입니다.

* **왜 어려운가?**
    * 우리의 목표는 데이터($\pi_{\beta}$)보다 **더 나은 정책($\pi_{\theta}$)을 찾는 것**입니다.
    * 하지만 이 과정에서 필연적으로 데이터에 없는 **보지 못한 행동(unseen actions)**을 평가해야만 합니다.
    * 이때 **분포 이동(distribution shift)** 문제가 발생합니다.
    * 우리 Q-네트워크는 데이터가 있는 'data support' 영역에서는 비교적 정확하지만, 데이터가 없는 OOD 영역에서는 'True Q'와 상관없이 **터무니없이 높은 값(overestimation)**을 뱉어냅니다.

---

### 해결 방법

이 문제를 해결하기 위해 다양한 알고리즘들을 배웠습니다:

1. **TD3+BC**: 정책 $\pi$가 $\pi_{\beta}$에서 멀어지지 못하게 **명시적으로 제약**하는 방식. 매우 직관적(intuitive)이지만, 데이터가 좋지 않으면 성능의 한계가 명확(often too conservative)합니다.

2. **CQL (Conservative Q-Learning)**: **암묵적인 제약(Implicitly constrain)**. Q-function 자체를 보수적으로 학습시킵니다. 데이터에 없는 OOD 행동의 Q-value는 의도적으로 낮추고, 데이터에 있는 행동의 Q-value는 높이도록 정규화 항(regularizing term)을 추가합니다. 간단하면서도 실전에서 성능이 매우 잘 나왔습니다. 다만 하이퍼파라미터 $\alpha$ 튜닝이 매우 중요합니다.

3. **Filtered BC**: 궤적의 총 보상을 기준으로 **좋은 궤적만 모방**하는 방식.

4. **AWR**: 어드밴티지를 가중치로 사용하여 **좋은 행동(transition)만 가중 모방**하는 방식.

5. **IQL**: SARSA 스타일의 타겟과 Expectile Regression을 결합하여 **OOD 쿼리 자체를 원천 차단**하는 방식.

각각의 접근법이 어떤 문제를 해결하려 했고, 어떤 장단점을 갖는지 명확히 이해하는 것이 중요합니다.
