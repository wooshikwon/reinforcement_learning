# Lecture 02: Imitation Learning (모방 학습)

## 목차

- [강의 소개](#강의-소개)
- [강화학습 복습](#강화학습-복습)
  - [강화학습의 기본 개념](#강화학습의-기본-개념)
  - [기본 용어 정리](#기본-용어-정리)
  - [지도학습과 강화학습](#지도학습과-강화학습)
- [행동 복제](#행동-복제)
  - [행동 복제의 개념](#행동-복제의-개념)
  - [시연 데이터 수집 방법](#시연-데이터-수집-방법)
  - [구현체 차이 문제](#구현체-차이-문제)
- [모방 학습의 문제점](#모방-학습의-문제점)
  - [문제 1: 누적 오차](#문제-1-누적-오차)
  - [문제 2: 다중 모드 데이터](#문제-2-다중-모드-데이터)
  - [문제 3: 관찰 불일치](#문제-3-관찰-불일치)
- [해결 방법: 온라인 개입](#해결-방법-온라인-개입)
  - [DAgger 알고리즘](#dagger-알고리즘)
  - [Human-gated DAgger](#human-gated-dagger)
- [사례 연구: ALOHA와 ACT](#사례-연구-aloha와-act)
  - [ALOHA 하드웨어](#aloha-하드웨어)
  - [ACT 알고리즘](#act-알고리즘)
  - [ACT 결과](#act-결과)
  - [모바일 ALOHA](#모바일-aloha)
- [고급 기법](#고급-기법)
  - [디퓨전 정책](#디퓨전-정책)
  - [로봇 파운데이션 모델](#로봇-파운데이션-모델)
  - [대규모 데이터의 중요성](#대규모-데이터의-중요성)
- [모방 학습의 한계](#모방-학습의-한계)
- [요약](#요약)

---

## 강의 소개

안녕하세요, 여러분. GCB6206 강화학습 세 번째 시간 강의를 시작하겠습니다. 저는 이 강의를 맡은 이영운입니다.

오늘부터는 강화학습의 한 분야인 **'모방 학습 (Imitation Learning)'**에 대해 본격적으로 배워보겠습니다.

### 강의 참고 자료

수업을 시작하기에 앞서, 우리 강의의 많은 슬라이드와 과제들은 스탠포드 대학의 CS224r, UC 버클리 대학의 CS285, 그리고 구글 딥마인드의 강의 자료들을 참고하여 만들어졌다는 점을 말씀드립니다. 이 강의들은 딥러닝 시대의 강화학습을 선도하는 최고의 강의들로, 우리 수업 역시 최신 연구 동향과 핵심적인 이론들을 충실히 반영하고 있습니다.

### 오늘 수업의 흐름

오늘 우리는 다음과 같은 순서로 수업을 진행할 것입니다:

1. **복습 (Recap)**: 먼저 강화학습의 기본 개념들을 간단히 복습하며 시작하겠습니다.
2. **모방 학습 (Imitation Learning)**: 모방 학습이 무엇인지, 그리고 이 학습에 필수적인 데이터는 과연 어디서 오는 것인지에 대해 알아볼 것입니다.
3. **문제점 분석 (What can go wrong?)**: 모방 학습의 여러 문제점을 구체적인 시나리오를 통해 살펴보겠습니다.
4. **해결책 (Solutions)**: 온라인 개입을 통한 학습과 고급 기법들에 대해 배울 것입니다.
5. **사례 연구 (Case study)**: 실제 로봇 조작 분야에서 모방 학습이 어떻게 성공적으로 적용되었는지 최신 사례를 통해 확인하겠습니다.

자, 그럼 강화학습 복습부터 시작해볼까요?

---

## 강화학습 복습

### 강화학습의 기본 개념

강화학습의 기본 구조를 다시 한번 짚어보겠습니다. 강화학습에는 **에이전트 (agent)**와 **환경 (environment)**이라는 두 주인공이 등장합니다.

위키피디아의 정의에 따르면, "강화학습 (RL)은 지능적인 에이전트가 동적인 환경 속에서 누적 보상을 최대화하기 위해 어떻게 행동해야 하는지를 다루는 학문"입니다.

이 과정을 좀 더 풀어서 설명해 보겠습니다:

1. **에이전트**는 현재 **환경의 상태 (state)**를 관찰합니다.
2. 관찰한 상태를 바탕으로, 에이전트는 어떤 **행동 (action)**을 취할지 결정하고 실행합니다.
3. 에이전트의 행동은 환경에 영향을 미치고, 환경은 새로운 상태로 변하게 됩니다.
4. 이때 환경은 에이전트가 한 행동이 얼마나 좋았는지에 대한 피드백으로 **보상 (reward)**이라는 신호를 줍니다.

이러한 상호작용의 순환 고리를 계속 반복하면서, 에이전트는 어떤 상태에서 어떤 행동을 해야 더 많은 보상을 장기적으로 받을 수 있는지를 스스로 학습하게 됩니다.

#### 의사 결정 문제

강화학습의 목표는 결국 '최적의 의사 결정 규칙'을 찾는 문제입니다. 에이전트는 **상태 (State $s$)**를 입력으로 받아서, **행동 (Action $a$)**을 출력합니다.

이때 상태를 행동으로 매핑해주는 이 규칙, 즉 에이전트의 의사 결정 메커니즘을 우리는 **정책 (Policy)**이라고 부르며, 보통 그리스 문자 **파이 ($\pi$)**로 표기합니다. 수식 $\pi(a|s)$는 '상태 $s$가 주어졌을 때 행동 $a$를 할 확률'을 의미합니다.

현대 딥러닝 기반 강화학습에서는 이 정책을 $\pi_{\theta}$와 같이 파라미터 $\theta$를 가진 신경망으로 모델링합니다. 그리고 강화학습의 목표는 누적 보상을 최대화하는 최적의 정책, 즉 최적의 파라미터 $\theta$를 찾는 것이 됩니다.

---

### 기본 용어 정리

강화학습의 상호작용은 매 시간 스텝 (timestep) $t=0, 1, 2,...$ 마다 일어납니다. 앞으로 계속 사용될 핵심 용어들을 정리하겠습니다:

- **상태 ($s_t$)**: 시간 $t$에서의 세상의 모습입니다. 관찰 (observation)이라고도 합니다.
- **행동 ($a_t$)**: 상태 $s_t$에서 에이전트가 취하는 행동입니다. 제어 (control) 입력이라고도 합니다.
- **정책 ($\pi(a_t|s_t)$)**: 상태 $s_t$가 주어졌을 때 행동 $a_t$를 선택하는 규칙입니다. 에이전트 그 자체, 혹은 전략 (strategy), 제어기 (controller)라고도 불립니다.
- **전이 확률 ($P(s_{t+1}|s_t, a_t)$)**: 상태 $s_t$에서 행동 $a_t$를 했을 때, 다음 상태가 $s_{t+1}$이 될 확률입니다. 이는 세상이 어떻게 돌아가는지에 대한 규칙으로, 환경 모델 (environment model) 또는 동역학 (dynamics)이라고도 합니다.

이러한 상태와 행동의 연속적인 시퀀스, $(s_0, a_0, s_1, a_1,...)$를 우리는 **궤적 (trajectory)** 또는 **에피소드 (episode)**라고 부릅니다.

---

### 지도학습과 강화학습

우리가 이미 잘 알고 있는 지도학습과 강화학습은 근본적으로 어떤 차이가 있을까요?

**지도학습 (Supervised Learning)**은 문제에 대한 **'정답'**을 알고 있을 때 사용합니다. 마치 자전거 타는 법을 완벽하게 보여주는 '모범 답안' 영상이 있어서, 그 영상을 그대로 따라 배우는 것과 같습니다. 입력과 그에 대한 정답 (label) 쌍으로 이루어진 데이터셋으로부터 학습합니다.

**강화학습 (Reinforcement Learning)**은 문제에 대한 **'정답'**을 모를 때 사용합니다. 아무도 자전거 타는 법을 가르쳐주지 않습니다. 대신, 스스로 페달을 밟아보고 (행동), 넘어지면 아프다는 (음의 보상) 피드백을, 앞으로 나아가면 재미있다는 (양의 보상) 피드백을 받으며 **상호작용과 시행착오 (interaction & feedback)**를 통해 배우는 방식입니다.

#### 모방 학습의 등장

그런데 여기서 한 가지 재미있는 아이디어가 나옵니다. 만약 '정답', 즉 전문가의 시범 데이터를 구할 수 있다면 어떨까요? 굳이 어려운 시행착오를 거치는 강화학습 대신, 지도학습처럼 전문가를 그냥 흉내 내게 만들면 되지 않을까요?

바로 이 지점에서 오늘의 주제인 **'모방 학습'**이 시작됩니다.

---

## 행동 복제

### 행동 복제의 개념

모방 학습의 가장 기본적이고 직관적인 아이디어는 바로 **행동 복제 (Behavioral Cloning, BC)**입니다.

핵심 아이디어는 아주 간단합니다: **전문가의 행동 데이터를 이용해 정책을 지도학습 방식으로 훈련**시키는 것입니다.

#### 알고리즘

1. 먼저, 전문가가 특정 상황 (상태)에서 어떻게 행동하는지에 대한 데이터, 즉 **전문가 시연 (expert demonstrations)** 데이터셋 $\mathcal{D} = \{(s_0, a_0), (s_1, a_1),...\}$를 수집합니다.
2. 그리고 상태 $s$를 입력으로, 전문가의 행동 $a$를 출력 (정답)으로 하는 신경망 정책 $\pi_{\theta}$를 훈련시킵니다.
3. 이 훈련은 다음 손실 함수를 최소화하는 방향으로 진행됩니다.

$$\min_{\theta} - \mathbb{E}_{(s,a) \sim \mathcal{D}} [\log \pi_{\theta}(a|s)]$$

**수식 설명:**
- $\mathcal{D}$: 전문가의 시연 데이터셋을 의미합니다.
- $(s, a) \sim \mathcal{D}$: 데이터셋 $\mathcal{D}$에서 상태-행동 쌍 $(s, a)$를 하나 뽑는다는 의미입니다.
- $\pi_{\theta}(a|s)$: 우리의 신경망 정책입니다. 상태 $s$가 입력되었을 때, 전문가의 행동 $a$가 나올 확률을 출력합니다.
- $\log \pi_{\theta}(a|s)$: 이 확률값에 로그를 취한 것입니다. 이 값을 최대화하는 것이 우리의 목표입니다 (Maximum Likelihood Estimation).
- $\min_{\theta} - \mathbb{E}[\dots]$: 로그 확률을 최대화하는 것은, 여기에 마이너스를 붙인 값의 기댓값을 최소화하는 것과 같습니다.

결국 행동 복제는 상태를 '문제', 전문가의 행동을 '정답'으로 놓고 푸는 지도학습 문제와 완전히 동일합니다. 행동이 핸들 각도처럼 연속적인 값이면 **회귀 (Regression)** 문제로, '좌, 우, 직진'처럼 이산적인 값이면 **분류 (Classification)** 문제로 풀 수 있습니다.

1989년에 이미 Pomerleau가 ALVINN이라는 자율주행차에 이 아이디어를 적용한 바 있습니다.

---

### 시연 데이터 수집 방법

행동 복제라는 간단하고 강력한 방법을 배웠습니다. 그런데 아주 근본적인 질문이 남습니다. 이 학습에 필요한 '전문가 시연 데이터'는 대체 어디서 구하는 걸까요?

어떤 분야에서는 이미 데이터가 존재합니다. 예를 들어 자율주행차는 수많은 사람들의 주행 데이터를 기록하여 사용할 수 있습니다.

하지만 로보틱스 같은 분야에서는 새로운 작업을 가르칠 때마다 데이터를 직접 수집해야 합니다. 로봇 시연 데이터를 수집하는 대표적인 방법들은 다음과 같습니다:

#### 운동감각 교시 (Kinesthetic Teaching)

사람이 로봇팔을 직접 잡고 움직여서 작업을 가르치는 방식입니다. 매우 직관적이고 쉬운 인터페이스라는 장점이 있지만, 사람이 로봇과 함께 장면에 등장해야 한다는 단점이 있습니다.

#### 원격 조종 (Remote Controllers)

Xbox 컨트롤러나 3D 마우스 같은 장비로 로봇을 원격 조종하여 데이터를 수집하는 방법입니다.

#### 인형 조종술 (Puppeteering)

한쪽 로봇 (마스터)을 사람이 조종하면, 그 움직임이 다른 쪽 로봇 (슬레이브)에 그대로 복제되는 방식입니다. 이 역시 인터페이스는 쉽지만, 동일한 로봇 하드웨어가 2대나 필요하다는 단점이 있습니다.

물론, 사족보행 로봇의 균형 잡기처럼 사람이 시연 데이터를 만드는 것 자체가 불가능한 영역도 존재합니다.

---

### 구현체 차이 문제

그렇다면, 로봇으로 시연 데이터를 모으기 번거로우니, 그냥 사람이 하는 유튜브 영상 같은 것을 데이터로 사용하면 안 될까요?

이 접근법에는 아주 큰 난관이 존재하는데, 바로 **구현체 차이 (Embodiment Gap)**입니다.

#### 구현체 차이의 두 가지 측면

**외형의 차이 (Difference in Appearance)**
사람의 몸과 로봇의 몸은 생긴 것부터가 다릅니다.

**물리적 능력 및 자유도의 차이 (Difference in Physical Capabilities and Degrees of Freedom)**
더 근본적인 문제입니다. 사람은 유연한 손가락과 수많은 관절을 이용해 섬세한 작업을 하지만, 로봇은 보통 딱딱한 집게 (gripper)와 제한된 수의 관절을 가집니다. 움직일 수 있는 범위나 힘도 전혀 다릅니다.

이러한 근본적인 차이 때문에 사람이나 동물의 영상을 로봇이 직접 모방하여 행동을 학습하는 것은 매우 어렵습니다. 하지만, 불가능한 것은 아닙니다. 이러한 영상 데이터를 로봇이 어떤 행동을 시도해봐야 하는지에 대한 '탐색의 가이드'로 활용하려는 연구들이 활발히 진행되고 있습니다 (Peng et al., 2018).

---

## 모방 학습의 문제점

지금까지 모방 학습이 무엇인지, 그리고 학습에 필요한 데이터는 어떻게 얻는지에 대해 알아보았습니다. 이제는 이러한 모방 학습에 어떤 문제점들이 도사리고 있는지 자세히 살펴보겠습니다.

행동 복제는 전문가가 보여준 상태 ($s$)에서 전문가가 한 행동 ($a$)을 그대로 따라 하도록 학습합니다. 완벽한 전문가 데이터가 무한히 많다면 이론적으로는 문제가 없겠죠. 하지만 현실은 다릅니다. 데이터는 유한하고, 우리의 모델은 완벽하지 않습니다. 이 작은 불완전함이 연속적인 의사결정 과정 속에서 어떤 재앙을 불러일으키는지 지금부터 하나씩 살펴보겠습니다.

---

### 문제 1: 누적 오차

모방 학습의 가장 고질적이고 심각한 문제입니다. 바로 **누적 오차 (Compounding Errors)**, 또는 **연쇄 오차**입니다.

#### 지도학습과의 차이

일반적인 **지도학습**에서는 데이터 포인트 $x_1, x_2, x_3$가 서로 독립적 (i.i.d)이라고 가정합니다. 즉, 모델이 $x_1$에 대해 예측을 잘못하더라도, 그것이 $x_2$라는 입력 자체에 영향을 주지는 않습니다.

하지만 **모방 학습**의 상황은 완전히 다릅니다. 이것은 순차적 의사결정 문제입니다:

1. 에이전트는 상태 $s_1$에서 시작합니다. 여기서 정책 $\pi$는 행동 $\hat{a}_1$을 예측합니다. 그런데 이 예측이 전문가의 행동과 아주 미세하게 달랐다고 해봅시다.
2. 이 작은 오차 때문에 에이전트는 전문가가 갔을 상태 $s_2$가 아닌, 살짝 어긋난 상태에 도달하게 됩니다.
3. 문제는 여기서부터 심각해집니다. 이 새로운 상태는 전문가 데이터셋에는 없었을 가능성이 높습니다. 에이전트는 생전 처음 보는 상황에 놓인 것이죠. 이런 낯선 상태에서는 당연히 더 큰 실수를 할 확률이 높고, 그 결과 행동 $\hat{a}_2$는 전문가의 경로에서 더욱 벗어나게 만듭니다.

이렇게 한번의 작은 실수가 다음 상태를 바꾸고, 바뀐 상태에서는 더 큰 실수를 하고, 그 실수가 또 다음 상태를 더 망가뜨리는 이 연쇄 반응이 바로 '누적 오차'입니다.

#### 공변량 변화

이 현상을 기계학습 용어로는 **공변량 변화 (Covariate Shift)** 또는 **분포 불일치 (Distribution Mismatch)** 문제라고 설명할 수 있습니다. 즉, 학습할 때의 상태 분포 ($p_{expert}(s)$)와 실제 테스트 (실행)할 때 에이전트가 마주치는 상태 분포 ($p_{\pi}(s)$)가 달라지는 문제입니다.

에이전트는 전문가의 '안전한' 주행 데이터로 운전을 배웠는데, 막상 운전대를 잡으니 한번의 실수로 도로 밖, 낯선 오프로드로 나가버린 셈이죠. 당연히 오프로드에서는 어떻게 운전해야 할지 배운 적이 없으니 큰 사고로 이어질 수밖에 없습니다.

#### 해결 방향

이 누적 오차 문제를 어떻게 해결할 수 있을까요? 두 가지 접근법을 생각해 볼 수 있습니다:

1. **아주 많은 시연 데이터를 모아서 잘 되기를 바란다**: 이론적으로는 가능한 얘기입니다. 전문가가 겪을 수 있는 거의 모든 상황, 심지어 실수를 만회하는 상황까지 데이터로 다 모은다면, 에이전트가 어떤 낯선 상태에 가더라도 대처법을 알 수 있겠죠. 하지만 이건 엄청나게 비효율적이고 사실상 불가능에 가깝습니다.

2. **교정 행동 데이터를 수집한다**: 이것이 훨씬 더 똑똑하고 현실적인 접근법입니다. 이 아이디어는 "에이전트가 실수를 저질렀을 때, 그 상태에서 어떻게 하면 다시 올바른 경로로 돌아올 수 있는지"를 가르치는 데이터를 집중적으로 모으는 것입니다. 에이전트가 경로에서 이탈했을 때, 그 자리에서 전문가가 올바른 방향을 알려주는 것이죠.

이 '교정 데이터'라는 개념이 바로 뒤에서 배울 DAgger와 같은 고급 모방 학습 알고리즘의 핵심적인 아이디어로 이어지게 됩니다.

---

### 문제 2: 다중 모드 데이터

두 번째 문제입니다. 하나의 목표를 달성하는 방법이 여러 가지일 때 발생하는 **다중 모드 (Multimodality)** 문제입니다.

#### 평균내기의 함정

예를 들어, 주황색 원에서 시작해서 초록색 원으로 가야 하는데, 가운데 있는 가구를 피해서 왼쪽으로 돌아가는 경로도 있고, 오른쪽으로 돌아가는 경로도 있다고 가정해봅시다. 두 가지 모두 완벽하게 올바른 '정답'입니다.

자, 만약 이 두 가지 경로 데이터를 모두 모아서, 가장 단순한 손실 함수인 L2 손실 (평균 제곱 오차)로 정책을 학습시키면 어떤 일이 벌어질까요? 우리의 정책은 두 가지 '정답' 행동의 **평균 (average)**을 내려고 할 겁니다. 왼쪽으로 가는 행동과 오른쪽으로 가는 행동의 평균은? 바로 '직진'이겠죠. 결국 이 정책은 가구를 향해 돌진해서 부딪히는, 최악의 행동을 배우게 됩니다.

여러 전문가의 좋은 행동들을 어설프게 평균 내면 바보 같은 '프랑켄슈타인 정책'이 탄생하는 것이죠. 이런 문제는 여러 사람이 시연 데이터를 수집했을 때 특히 자주 발생합니다.

#### 해결책

정책이 단 하나의 행동만 출력하는 것이 아니라, 여러 가능성을 모두 표현할 수 있도록 **표현력이 풍부한 확률 분포를 사용**하는 것입니다. 예를 들어:

- 가우시안 혼합 모델 (Gaussian Mixture Model)
- VAE (Variational Auto-Encoder)
- 디퓨전 모델 (Diffusion Models)

이러한 모델들을 사용하면, 정책이 "왼쪽으로 갈 확률 50%, 오른쪽으로 갈 확률 50%"와 같이 여러 개의 유효한 행동 '모드'를 모두 학습할 수 있게 됩니다.

---

### 문제 3: 관찰 불일치

세 번째 문제입니다. 전문가와 에이전트가 세상을 바라보는 '정보의 양'이 다를 때 발생합니다.

#### 숨겨진 맥락의 문제

채팅 예시를 봅시다:

**상황 1:**
- 사용자: "안녕, 잘 지내?"
- 전문가: "응! 지난 주말 농구 경기 어땠어?"

**상황 2:**
- 사용자: "안녕, 잘 지내?"
- 전문가: "응, 좋아. 내일 점심 기대된다!"

에이전트가 볼 수 있는 관찰 (상태)은 "안녕, 잘 지내?"라는 문장으로 동일합니다. 그런데 왜 전문가는 완전히 다른 행동 (답변)을 했을까요? 그 이유는 전문가에게는 있지만 에이전트에게는 주어지지 않은 **숨겨진 맥락 (context)**이 있기 때문입니다.

첫 번째 상황에서 전문가는 상대방과 최근 농구에 대해 이야기했다는 사실을 기억하고 있었고, 두 번째 상황에서는 내일 점심 약속이 있다는 사실을 알고 있었던 것이죠.

이렇게 전문가가 에이전트보다 더 많은 정보를 가지고 의사결정을 내리는 상황에서는, 에이전트가 전문가의 행동을 정확히 모방하는 것이 원천적으로 불가능합니다.

#### 해결책

두 가지 방향으로 생각해볼 수 있습니다:

1. 에이전트에게 **최대한 많은 맥락 정보를 제공**해주는 것입니다. 채팅 예시라면, 현재 문장뿐만 아니라 전체 대화 기록을 상태로 입력해주는 것이죠.

2. 데이터를 수집하는 단계에서부터, **전문가가 에이전트와 동일한 정보만을 보도록 환경을 구성**하는 것입니다. 자율주행차를 예로 들면, 사람 운전자가 네비게이션 화면만 보고 운전하게 하는 것이 아니라, 실제 자율주행차가 사용할 카메라와 센서 데이터만 보고 운전하도록 시연 데이터를 수집하는 방식입니다.

---

## 해결 방법: 온라인 개입

지금까지 모방 학습의 세 가지 주요 난관에 대해 알아봤습니다. 이 문제들을 인지하는 것이 좋은 모방 학습 알고리즘을 설계하는 첫걸음입니다. 그럼 이제, 이 문제들 중 가장 치명적인 '누적 오차' 문제를 해결하기 위한 구체적인 알고리즘에 대해 배워보겠습니다.

### DAgger 알고리즘

'누적 오차' 문제를 해결하기 위한 가장 대표적이고 고전적인 알고리즘이 바로 **DAgger (Dataset Aggregation)**입니다. 이름 그대로 '데이터셋을 계속 쌓아나가는' 방식입니다.

앞서 '교정 데이터'를 모으는 것이 중요하다고 했는데, DAgger는 바로 그 아이디어를 체계적인 알고리즘으로 만든 것입니다.

#### DAgger의 작동 과정

1. **현재 정책 ($\pi_{\theta}$)을 실행합니다 (Roll out)**: 에이전트를 실제 환경에서 돌려봅니다. 그러면 에이전트는 자신이 자주 실수하는, '낯선' 상태들로 스스로 찾아가게 됩니다.

2. **전문가에게 물어봅니다 (Query expert)**: 에이전트가 방문했던 그 '낯선' 상태 ($s'$)들을 모아서 전문가에게 "이 상황에서는 어떻게 했어야 하나요?"라고 물어보고, 정답 행동 ($a^*$)을 받습니다. 에이전트의 실수가 발생한 지점에서 전문가가 올바른 행동을 알려주는 것입니다.

3. **데이터를 합칩니다 (Aggregate)**: 이렇게 새로 얻은 교정 데이터 $\{(s', a^*)\}$를 기존의 데이터셋 $\mathcal{D}$에 추가합니다.

4. **정책을 다시 학습합니다 (Update policy)**: 더 풍부해진 데이터셋으로 정책 $\pi_{\theta}$를 다시 학습시킵니다.

이 1~4번 과정을 계속 반복합니다. 그러면 정책은 점차 자신이 가보지 않았던 영역에 대한 대처법을 배우게 되고, 전문가의 행동 분포에 점점 더 가깝게 수렴하게 됩니다.

#### 장단점

DAgger는 **데이터 효율성이 매우 높다**는 큰 장점이 있습니다. 왜냐하면 정책이 가장 필요로 하는, 즉 가장 취약한 부분에 대한 데이터를 집중적으로 수집하기 때문입니다.

하지만 **실시간으로 전문가가 대기하며 에이전트의 모든 상태에 대해 라벨링을 해줘야 한다**는 점은 큰 단점이 될 수 있습니다.

---

### Human-gated DAgger

매 순간 전문가가 라벨링을 해주기 어렵다는 DAgger의 단점을 개선한, 좀 더 현실적인 변형 알고리즘이 있습니다. 바로 **Human-gated DAgger**, 또는 **공유 자율성 (Shared Autonomy)**이라고 불리는 방식입니다.

#### 작동 방식

1. 일단 에이전트가 스스로 임무를 수행하도록 둡니다.
2. 사람 전문가는 옆에서 지켜보기만 합니다. 그러다가 에이전트가 **결정적인 실수를 저지르려는 순간에 개입**해서 조종간을 빼앗습니다.
3. 그 시점부터는 **사람 전문가가 직접 임무를 끝까지 완수**합니다.
4. 이렇게 사람이 개입해서 수행한 부분의 궤적 전체가 새로운 전문가 시연 데이터가 되어 기존 데이터셋에 추가됩니다.
5. 이 데이터를 이용해 정책을 다시 학습합니다.

#### 장단점

이 방법은 전문가 입장에서 훨씬 편합니다. 계속 라벨링을 할 필요 없이, 필요할 때만 개입하면 되니까요.

하지만 자동차 주행처럼 매우 빠른 속도로 진행되는 작업에서는 사람이 실수를 제때 포착하고 개입하기 어려울 수 있다는 단점이 있습니다.

---

## 사례 연구: ALOHA와 ACT

지금까지 모방 학습의 문제점과 그것을 해결하기 위한 이론적인 알고리즘 (DAgger)에 대해 배웠습니다. 이제부터는 이러한 아이디어들이 실제 로봇 공학 분야에서 어떻게 적용되어 놀라운 성공을 거두었는지, 최신 사례 연구를 통해 살펴보겠습니다.

오늘 소개해드릴 시스템은 스탠포드와 UC 버클리에서 개발한 **ALOHA** 시스템과 **ACT**라는 모방 학습 알고리즘입니다. ALOHA는 저렴한 하드웨어로 구현한 양팔 로봇 원격 조종 시스템이고, ACT는 이 시스템으로 모은 데이터를 학습하는 알고리즘의 이름입니다.

로봇이 케이블 타이를 묶고, 배터리를 끼우고, 테이프를 붙이는 등 굉장히 정교한 양손 조작을 수행하는 것을 볼 수 있습니다. 이런 작업들은 기존의 로봇 공학에서는 매우 어려운 난제였습니다.

---

### ALOHA 하드웨어

먼저 하드웨어부터 살펴보겠습니다. 이 ALOHA 시스템의 가장 큰 특징 중 하나는 **2만 달러 미만의 저렴한 비용**으로 전체 시스템을 구축했다는 점입니다. 기성품 로봇 팔과 오픈소스로 공개된 부품, 코드를 활용하여 연구의 진입 장벽을 크게 낮췄습니다.

#### 시스템 구성

시스템은 '인형 조종술 (Puppeteering)' 방식으로 작동합니다:

- 사람이 '마스터' 로봇 팔 한 쌍을 움직이면, 그 관절 각도 움직임이 그대로 '슬레이브' 로봇 팔 한 쌍에 전달되어 작업을 수행합니다.
- 50Hz의 빠른 속도로 제어가 이루어져 반응성이 높습니다.
- 총 4대의 카메라 (전방, 상단, 양쪽 손목)가 촬영하는 RGB 이미지를 입력으로 사용합니다.

흥미로운 점은, 로봇이 물체와 상호작용할 때 느끼는 힘을 사람에게 전달해주는 **힘 피드백 (force feedback) 기능이 없다**는 것입니다. 이는 원격 조종을 더 어렵게 만들지만, 시스템을 더 저렴하고 단순하게 유지하는 비결이기도 합니다.

---

### ACT 알고리즘

이렇게 ALOHA 시스템으로 전문가의 양손 조작 데이터를 모았습니다. 이제 이 데이터로 모방 학습을 해야 합니다. 그런데 여기서 우리가 앞서 배웠던 문제들이 다시 등장합니다.

#### 도전 과제

- **도전 과제 1: 누적 오차** - 특히 50Hz라는 매우 빠른 제어 주기에서는 작은 오차가 순식간에 누적되어 걷잡을 수 없게 됩니다.
- **도전 과제 2: 다중 모드 데이터** - 사람이 시연할 때마다 매번 미세하게 다른 방식으로 작업을 수행하기 때문에 데이터에 다양한 '모드'가 존재합니다.

이 문제들 때문에, 기존의 평범한 모방 학습 (행동 복제) 방식으로 정책을 학습시켰더니 **성공률이 0%**가 나왔습니다. 잘 알려진 다른 모방 학습 기법들 (BeT, RT-1, VINN 등)을 적용해봐도 성공률이 거의 없거나 매우 낮았습니다.

#### 해결책 1: 액션 청킹

ACT 알고리즘의 첫 번째 핵심 아이디어는 바로 **액션 청킹 (Action Chunking)**입니다.

'누적 오차'는 매 순간 의사결정을 할 때마다 오차가 쌓이는 문제입니다. 그렇다면, 의사결정 빈도를 줄이면 어떨까요? 이것이 액션 청킹의 핵심입니다.

기존 방식처럼 1/50초마다 한번씩 행동을 결정하는 대신, ACT 정책은 한번에 **미래의 행동 묶음 (chunk), 예를 들어 약 60개의 행동 시퀀스 전체를 예측**합니다. 그리고 로봇은 이 60개의 행동이 끝날 때까지 중간에 새로운 관찰 (카메라 영상)을 보지 않고 예측된 행동을 그대로 수행합니다. 이를 **개루프 (open-loop)** 제어라고 합니다. 그리고 60 스텝이 지난 후에야 다시 주변을 둘러보고 다음 행동 묶음을 예측합니다.

이렇게 함으로써, 50Hz로 매우 자주 하던 의사결정을 약 0.8Hz의 느린 주기로 바꾸는 효과를 얻습니다. 이는 반응성은 약간 희생하지만, 오차가 빠르게 누적되는 것을 막아 훨씬 안정적인 제어를 가능하게 하는 똑똑한 트레이드오프 전략입니다.

#### 해결책 2: 절대 좌표 제어

행동을 '현재 위치에서 얼마만큼 움직여라' 같은 **상대적인 값**이 아니라, '목표 지점인 [x, y, z] 좌표로 가라'와 같은 **절대적인 관절 위치**로 예측합니다.

상대적 움직임은 오차가 계속 누적되지만, 절대적 목표 위치는 매번 오차를 리셋하는 자기 교정 효과가 있어 훨씬 안정적입니다.

#### 해결책 3: VAE를 통한 다중 모드 처리

**VAE (Variational Auto-Encoder)**라는 생성 모델을 정책에 도입합니다. VAE는 전문가의 다양한 시연 데이터 (다중 모드)를 잠재 공간 (latent space)이라는 압축된 형태로 학습합니다.

정책은 이 잠재 공간에서 샘플링을 통해 다양한 스타일의 성공적인 행동 시퀀스를 생성할 수 있게 됩니다. 이를 통해 '평균내기' 문제에 빠지지 않고, 여러 유효한 해결책 중 하나를 일관되게 따라 할 수 있게 됩니다.

#### 신경망 구조

이러한 아이디어를 구현한 ACT 정책의 신경망 구조는 **트랜스포머 (Transformer)**를 기반으로 하고 있습니다.

데이터의 흐름은 다음과 같습니다:

1. **입력**: 4개의 카메라 이미지와 현재 로봇의 관절 각도 정보가 입력됩니다.
2. **인코딩**: 각 이미지는 CNN을 통과해 시각적 특징 (feature)으로 변환됩니다.
3. **트랜스포머 인코더**: 이 시각 특징들과 관절 각도 정보가 트랜스포머 인코더로 들어갑니다. 트랜스포머의 '어텐션 (attention)' 메커니즘은 여러 입력 정보 간의 관계를 파악하여 현재 상황을 종합적으로 이해하는 데 매우 효과적입니다.
4. **트랜스포머 디코더**: 인코더가 상황을 이해한 결과를 바탕으로, 트랜스포머 디코더가 **미래 행동의 순차적인 묶음 (action sequence)**을 생성해냅니다.

---

### ACT 결과

이렇게 ACT는 트랜스포머 구조를 활용하여 시공간적인 정보를 효과적으로 처리하고, 액션 청킹과 VAE를 결합하여 모방 학습의 고질적인 문제들을 해결했습니다.

결과는 놀라웠습니다. 로봇이 아주 능숙하게 배터리를 집어서 좁은 슬롯에 정확하게 끼워 넣는 등의 작업을 수행했습니다. 이 어려운 작업을, 단 **50개의 전문가 시연 데이터만으로 학습**하여 **96%의 높은 성공률**로 해냈습니다. 다른 작업 역시 50개의 데이터로 84%의 성공률을 보였습니다.

성공률 0%에서 시작했던 것을 기억하면, 이는 정말 놀라운 발전입니다. 이는 ACT 알고리즘이 제안한 '액션 청킹'과 'VAE를 통한 다중 모드 모델링'이라는 아이디어가 실제로 매우 효과적이었음을 증명하는 결과입니다.

---

### 모바일 ALOHA

이전에 봤던 ALOHA 시스템은 고정된 테이블 위에서 정교한 양팔 조작을 수행했습니다. 연구자들은 여기서 한 걸음 더 나아갔습니다. "만약 이 양팔 로봇 시스템에 바퀴를 달아 움직이게 할 수 있다면 어떨까?" 바로 이 아이디어가 **모바일 ALOHA (Mobile ALOHA)**의 시작입니다.

#### 전신 원격 조종

이제 사람이 허리에 장비를 차고 직접 걸어 다니면서 로봇 전체, 즉 **몸체 (mobile base)와 양팔 모두를 동시에 원격 조종 (whole-body teleoperation)**할 수 있게 되었습니다. 이는 단순히 팔을 움직이는 것과는 차원이 다른, 훨씬 더 복잡하고 동적인 시연 데이터를 수집할 수 있게 되었다는 의미입니다.

#### 결과

학습된 정책을 통해 로봇이 주방을 돌아다니며 새우를 요리하고, 의자를 밀어 넣고, 캐비닛을 사용하는 등 훨씬 더 넓은 범위의 실생활 작업을 수행할 수 있게 되었습니다. 사람이 직접 화장실을 청소하도록 원격 조종하여 수집한 데이터로 학습한 로봇이 **스스로 (autonomous)**, 사람의 개입 없이 주방에서 새우를 요리하는 등의 작업을 자율적으로 수행했습니다.

이는 정적인 환경을 넘어, 이동과 조작이 결합된 복잡한 작업을 모방 학습만으로도 충분히 배울 수 있다는 가능성을 보여준 매우 중요한 연구 결과입니다.

---

## 고급 기법

지금까지 우리는 ACT 알고리즘이 '액션 청킹'과 'VAE'를 이용해 모방 학습의 문제들을 해결한 것을 봤습니다. 그런데 최근, 특히 이미지 생성 분야에서 엄청난 성공을 거둔 **디퓨전 모델 (Diffusion Models)**을 모방 학습에 적용하려는 시도가 큰 주목을 받고 있습니다.

### 디퓨전 정책

**디퓨전 정책 (Diffusion Policy)**은 디퓨전 모델을 로봇 제어에 적용한 최신 기법입니다.

#### 정책 모델링 방식의 비교

정책을 모델링하는 세 가지 방식을 비교해 보겠습니다:

**1. 명시적 정책 (Explicit Policy)**

우리가 지금까지 주로 다룬 방식입니다. 신경망 $F_{\theta}(o)$가 관찰 $o$를 입력받아 행동 $a$를 직접 출력합니다. 출력이 단일 값 (Regression)이면 다중 모드 문제를 해결하기 어렵고, 이를 보완하기 위해 가우시안 혼합 모델 (Mixture of Gaussians)이나 이산적인 행동 분포 (Categorical)를 사용하기도 합니다.

**2. 암시적 정책 (Implicit Policy)**

행동을 직접 출력하는 대신, **에너지 함수** $E_{\theta}(o, a)$를 학습합니다. 이 함수는 주어진 관찰 $o$에 대해 좋은 행동 $a$에는 낮은 에너지 (낮은 값)를, 나쁜 행동에는 높은 에너지를 부여합니다. 정책은 이 에너지 값을 최소화하는 행동을 찾는 최적화 과정 ($\text{arg min}_a E$)을 통해 결정됩니다. 표현력은 높지만, 행동을 결정할 때마다 최적화 과정을 거쳐야 해서 계산 비용이 비쌉니다.

**3. 디퓨전 정책 (Diffusion Policy)**

행동을 **생성 (generate)**하는 방식으로 접근합니다. 완전히 무작위적인 노이즈에서 시작해서, 여러 단계를 거치며 점진적으로 노이즈를 제거 (denoising)해 나감으로써 최종적으로 전문가의 행동과 유사한 그럴듯한 행동 (또는 행동 시퀀스)을 만들어냅니다.

신경망 $\epsilon_{\theta}(o, a)$는 각 단계에서 어떤 노이즈를 제거해야 하는지를 학습합니다. 이 방식은 매우 복잡하고 다양한 전문가 행동 분포 (다중 모드)를 아주 잘 모델링할 수 있다는 강력한 장점을 가집니다.

#### 디퓨전 정책의 장점

디퓨전 정책은 모방 학습의 주요 문제들에 대한 매우 효과적인 해결책을 제시합니다:

- **다중 모드 문제**: 디퓨전 모델 자체가 복잡한 데이터 분포를 학습하는 데 특화된 강력한 생성 모델이기 때문에, 여러 갈래의 전문가 행동을 뭉뚱그리지 않고 각각의 '모드'를 잘 살려서 행동을 생성할 수 있습니다.

- **누적 오차 문제**: 디퓨전 정책은 단일 행동이 아닌, 미래의 **행동 시퀀스 전체**를 일관성 있게 생성할 수 있습니다. 이렇게 시간적으로 일관된 계획을 세우면, 매 스텝마다 조금씩 잘못된 결정을 내리며 경로를 이탈하는 누적 오차 문제를 크게 완화할 수 있습니다.

#### 실제 성능

실험 결과, 로봇 팔이 T자 모양 블록의 외곽선을 따라 정교하게 움직이는 등, 디퓨전 정책이 시각적 입력을 바탕으로 매우 부드럽고, 시간적으로 일관되며, 정확한 행동 궤적을 생성해낼 수 있음을 보여줍니다.

---

### 로봇 파운데이션 모델

지금까지는 '하나의 로봇'이 '하나의 작업'을 배우는 상황을 주로 다뤘습니다. 하지만 최근 인공지능 분야의 가장 큰 화두는 단연 **파운데이션 모델 (Foundation Model)**입니다. 대규모 데이터로 미리 학습된 하나의 거대 모델을 가지고, 다양한 종류의 문제를 해결하는 패러다임이죠.

로보틱스 분야에도 이 패러다임이 적용되기 시작했습니다. 바로 **로봇 파운데이션 모델 (Robotic Foundation Model, RFM)**입니다.

#### RFM의 개념

RFM은 로봇의 관찰 ($s$)뿐만 아니라, 자연어 등으로 주어진 **작업/기술 ($z$)**을 함께 입력으로 받습니다. 예를 들어, "셔츠 개줘 (fold shirt)"라는 명령어를 입력하면, 모델 $\pi(a|s,z)$가 그에 맞는 행동 ($a$)을 출력하는 것이죠.

즉, 재학습 없이 명령어만 바꾸면 다양한 작업을 수행할 수 있는 **범용 (general-purpose) 로봇 제어 모델**을 만드는 것이 목표입니다.

#### 대규모 데이터셋의 필요성

하지만 이런 강력한 파운데이션 모델을 만들려면 무엇이 필요할까요? 바로 **엄청나게 방대하고 다양한 데이터**입니다.

**DROID 데이터셋**
다양한 환경에서 수집된 대규모 로봇 조작 데이터셋입니다. 13개 기관이 참여하여 500개가 넘는 실제 환경 (가정집, 연구실 등)에서 6만 5천 개 이상의 작업 에피소드를 수집했습니다. '야생 (in-the-wild)' 환경에서 얻은 데이터라는 점이 중요합니다.

**Open X-Embodiment (OXE) 데이터셋**
여기서 한 걸음 더 나아갑니다. 'X-Embodiment'는 'Cross-Embodiment', 즉 **서로 다른 형태의 로봇**을 교차하여 학습한다는 의미입니다. 34개 연구실이 협력하여, 무려 **22가지 종류의 다른 로봇**으로부터 수집한 **100만 개 이상의 에피소드**를 하나로 모은 거대한 데이터셋입니다.

이렇게 다양한 로봇, 다양한 환경, 다양한 작업 데이터를 하나의 모델에 학습시키면, 모델은 특정 로봇에 국한된 지식이 아닌, '물건을 잡는 법', '문을 여는 법'과 같은 **보편적이고 일반화된 조작 원리**를 학습하게 됩니다.

---

### 대규모 데이터의 중요성

이런 대규모 데이터셋으로 학습한 모델이 바로 **π_0 (Pi-Zero)**와 같은 **시각-언어-행동 (Vision-Language-Action, VLA) 모델**입니다. 이 모델을 학습시키는 데는 무려 **1만 시간** 분량의 로봇 데이터가 사용되었고, 128개의 최신 GPU로 2주 이상 학습시켜야 했습니다.

#### 언어 모델과의 비교

하지만 이 데이터의 양을 언어 모델의 세계와 비교해보면 어떨까요?

- 로보틱스 데이터 (OXE, π): 4천 ~ 1만 시간
- GPT-2 학습 데이터: 약 47만 5천 시간
- (참고) Llama 3 학습 데이터: 약 7억 9천만 시간

보시다시피, 로보틱스 데이터의 규모가 빠르게 커지고는 있지만, 언어 모델을 학습시키는 데 사용되는 데이터의 양에 비하면 아직은 아주 작은 수준입니다. 이는 역으로, 앞으로 로봇 데이터 수집 기술이 발전함에 따라 로봇 지능이 폭발적으로 성장할 잠재력이 엄청나게 많이 남아있다는 것을 의미합니다.

#### 데이터 수집 노력

이러한 데이터의 중요성을 알기에, 전 세계의 많은 기업들이 데이터 수집 규모를 늘리기 위해 사활을 걸고 있습니다. 테슬라가 휴머노이드 로봇인 옵티머스를 학습시키기 위해 수많은 사람들이 원격 조종으로 데이터를 쌓고 있으며, Figure AI의 로봇이 물류 창고 같은 실제 산업 현장에 투입되어 작업을 수행하고 있습니다. 이러한 노력들이 바로 미래의 로봇 파운데이션 모델을 위한 밑거름이 되고 있습니다.

---

## 모방 학습의 한계

지금까지 우리는 모방 학습이 얼마나 강력한지, 그리고 대규모 데이터와 결합했을 때 어떤 놀라운 일들을 할 수 있는지 살펴보았습니다. 모방 학습은 간단하면서도 매우 효과적인 행동 학습 프레임워크임이 분명합니다.

**하지만, 이것만으로 충분할까요?**

모방 학습에는 명확한 한계점들이 존재합니다:

1. **전문가 시연 데이터의 필요성**: 어떤 작업들은 전문가 시연 데이터를 모으는 것 자체가 매우 어렵거나 불가능합니다.

2. **성능 상한선**: 더 근본적인 한계는, **모방 학습으로 배운 정책은 절대 전문가보다 더 잘할 수 없다**는 것입니다. 항상 전문가의 실력에 의해 그 성능의 상한선이 정해져 있습니다.

3. **능동적 학습 불가**: 또한 모방 학습은 스스로의 경험이나 시행착오로부터 배우는 방법을 제공하지 않습니다. 그저 흉내 낼 뿐, 더 나은 방법을 스스로 창조해내지 못합니다.

그렇다면, 이 한계를 넘어, 에이전트가 전문가 없이도, **스스로의 실수로부터 배우고, 전문가를 뛰어넘는 새로운 전략을 발견하게 할 수는 없을까요?**

바로 이 질문에 대한 답을 찾는 여정이, 우리가 다음 시간부터 본격적으로 탐험하게 될 **강화학습 (Reinforcement Learning)**의 세계입니다.

---

## 요약

### 학습 내용 정리

이번 강의에서는 모방 학습의 기본 개념부터 최신 응용 사례까지 체계적으로 학습했습니다.

#### 1. 핵심 개념

**강화학습의 기본 구조**
- 에이전트 (Agent): 의사결정을 하는 주체
- 환경 (Environment): 에이전트가 상호작용하는 세계
- 상태 ($s_t$): 현재 환경의 상태
- 행동 ($a_t$): 에이전트가 선택하는 행동
- 정책 ($\pi(a|s)$): 상태를 행동으로 매핑하는 규칙
- 보상 (reward): 행동에 대한 피드백

**지도학습 vs 강화학습**
- 지도학습: 정답 (label)이 있는 데이터로 학습
- 강화학습: 시행착오와 보상 신호를 통해 학습
- 모방 학습: 전문가 시연 데이터를 이용한 지도학습 방식

**행동 복제 (Behavioral Cloning)**
- 전문가 시연 데이터셋 $\mathcal{D} = \{(s, a)\}$ 수집
- 손실 함수: $\min_{\theta} - \mathbb{E}_{(s,a) \sim \mathcal{D}} [\log \pi_{\theta}(a|s)]$
- 지도학습과 동일한 방식으로 정책 학습

---

#### 2. 시연 데이터 수집 방법

**로봇 데이터 수집 기법**
- 운동감각 교시 (Kinesthetic Teaching): 직접 로봇을 잡고 움직임
- 원격 조종 (Remote Controllers): 컨트롤러로 원격 조작
- 인형 조종술 (Puppeteering): 마스터-슬레이브 로봇 쌍 사용

**구현체 차이 (Embodiment Gap)**
- 외형의 차이
- 물리적 능력 및 자유도의 차이
- 사람의 영상을 직접 로봇 학습에 사용하기 어려운 이유

---

#### 3. 모방 학습의 문제점

**문제 1: 누적 오차 (Compounding Errors)**
- 작은 오차가 연쇄적으로 증폭
- 공변량 변화 (Covariate Shift): 학습 분포와 실행 분포의 불일치
- 해결 방향: 교정 데이터 수집

**문제 2: 다중 모드 데이터 (Multimodal Data)**
- 여러 정답 경로가 존재할 때 평균내기 문제 발생
- 해결책: 표현력 풍부한 확률 분포 사용 (GMM, VAE, Diffusion)

**문제 3: 관찰 불일치 (Observability Mismatch)**
- 전문가와 에이전트의 정보 비대칭
- 숨겨진 맥락 (context) 문제
- 해결책: 맥락 정보 제공 또는 동일한 정보만 사용하도록 제한

---

#### 4. 해결 방법

**DAgger (Dataset Aggregation)**
1. 현재 정책으로 환경 실행 (Roll out)
2. 전문가에게 교정 행동 질의 (Query expert)
3. 교정 데이터를 기존 데이터셋에 추가 (Aggregate)
4. 정책 재학습 (Update policy)
- 장점: 데이터 효율성 높음
- 단점: 실시간 전문가 라벨링 필요

**Human-gated DAgger (공유 자율성)**
- 에이전트가 실수할 때만 전문가 개입
- 전문가가 임무를 완수하며 교정 데이터 생성
- 장점: 전문가 부담 감소
- 단점: 빠른 작업에서는 개입 타이밍 어려움

---

#### 5. 사례 연구: ALOHA와 ACT

**ALOHA 하드웨어**
- 저렴한 비용 (2만 달러 미만)
- 인형 조종술 방식의 양팔 로봇
- 50Hz 제어 주기, 4대의 카메라
- 힘 피드백 없음

**ACT 알고리즘의 핵심 아이디어**

1. **액션 청킹 (Action Chunking)**
   - 한 번에 60개의 행동 시퀀스 예측
   - 개루프 제어로 의사결정 빈도 감소 (50Hz → 0.8Hz)
   - 누적 오차 완화

2. **절대 좌표 제어**
   - 상대적 움직임 대신 절대적 목표 위치 사용
   - 자기 교정 효과로 안정성 향상

3. **VAE를 통한 다중 모드 처리**
   - 잠재 공간에서 다양한 행동 스타일 학습
   - 평균내기 문제 해결

4. **트랜스포머 구조**
   - 인코더: 시각 정보와 관절 각도 정보 통합
   - 디코더: 미래 행동 시퀀스 생성

**ACT 결과**
- 50개 시연 데이터로 96% 성공률 달성
- 기존 BC 방법 (0% 성공률) 대비 극적인 개선

**모바일 ALOHA**
- 전신 원격 조종 (whole-body teleoperation)
- 이동과 조작이 결합된 복잡한 작업 수행
- 실생활 환경에서의 자율 작업 성공

---

#### 6. 고급 기법

**디퓨전 정책 (Diffusion Policy)**

정책 모델링 방식 비교:
- 명시적 정책: 직접 행동 출력 ($F_{\theta}(o) \rightarrow a$)
- 암시적 정책: 에너지 함수 최소화 ($\arg\min_a E_{\theta}(o, a)$)
- 디퓨전 정책: 노이즈 제거를 통한 행동 생성 ($\epsilon_{\theta}(o, a)$)

디퓨전 정책의 장점:
- 다중 모드 분포를 매우 잘 모델링
- 시간적으로 일관된 행동 시퀀스 생성
- 누적 오차 문제 완화

**로봇 파운데이션 모델 (RFM)**
- 정의: 다양한 작업을 수행할 수 있는 범용 로봇 제어 모델
- 입력: 관찰 ($s$) + 작업/명령 ($z$)
- 출력: 행동 ($a$)

**대규모 데이터셋**
- DROID: 13개 기관, 65,000+ 에피소드
- Open X-Embodiment (OXE): 34개 연구실, 22가지 로봇, 1,000,000+ 에피소드
- π_0 (Pi-Zero): 10,000시간 로봇 데이터로 학습

**데이터 규모 비교**
- 로보틱스: 4천 ~ 1만 시간
- GPT-2: 47만 5천 시간
- Llama 3: 7억 9천만 시간
→ 로봇 데이터는 여전히 성장 잠재력이 매우 큼

---

#### 7. 모방 학습의 한계

**근본적인 한계점**
1. 전문가 시연 데이터 수집의 어려움
2. 성능 상한선: 전문가를 뛰어넘을 수 없음
3. 능동적 학습 불가: 스스로 더 나은 전략 발견 불가

**다음 단계: 강화학습**
- 전문가 없이도 학습 가능
- 시행착오를 통한 능동적 학습
- 전문가를 뛰어넘는 전략 발견 가능

---

### 주요 알고리즘 비교

| 방법 | 데이터 효율성 | 전문가 부담 | 성능 | 적용 난이도 |
|------|-------------|------------|------|------------|
| **Behavioral Cloning** | 중간 | 낮음 | 낮음 | 쉬움 |
| **DAgger** | 높음 | 매우 높음 | 높음 | 중간 |
| **Human-gated DAgger** | 높음 | 중간 | 높음 | 중간 |
| **ACT (Action Chunking)** | 매우 높음 | 낮음 | 매우 높음 | 높음 |
| **Diffusion Policy** | 매우 높음 | 낮음 | 매우 높음 | 높음 |

---

### 중요 포인트

1. **모방 학습의 핵심**: 전문가 시연 데이터를 지도학습 방식으로 활용하여 정책 학습

2. **누적 오차의 심각성**: 순차적 의사결정에서 작은 오차가 연쇄적으로 증폭되는 문제

3. **데이터 품질의 중요성**: 다양한 상황, 특히 실수 복구 상황을 포함하는 교정 데이터가 핵심

4. **액션 청킹의 혁신**: 의사결정 빈도를 줄여 누적 오차를 완화하는 효과적인 전략

5. **디퓨전 모델의 강력함**: 복잡한 다중 모드 분포와 시간적 일관성을 동시에 해결

6. **대규모 데이터의 가능성**: 로봇 파운데이션 모델은 아직 초기 단계이며, 데이터 규모 확대에 따른 비약적 발전 기대

7. **모방 학습의 한계 인식**: 전문가를 뛰어넘을 수 없다는 근본적 한계를 이해하고 강화학습의 필요성 인식

---

### 다음 단계

모방 학습은 빠르게 시작할 수 있고 안정적인 결과를 제공하는 강력한 방법이지만, 전문가의 한계를 넘을 수 없다는 근본적인 제약이 있습니다. 다음 강의에서는 에이전트가 **스스로의 경험으로부터 학습**하고, **전문가 없이도** 최적의 행동을 발견할 수 있는 **강화학습 (Reinforcement Learning)**의 세계로 들어가게 됩니다.

강화학습은 시행착오를 통해 보상을 최대화하는 방향으로 스스로 학습하며, 때로는 전문가조차 생각하지 못한 창의적인 전략을 발견하기도 합니다. 오늘 배운 모방 학습의 개념과 한계점을 잘 기억해두시면, 다음 시간부터 배울 강화학습이 왜 필요한지, 그리고 어떤 문제를 해결하려 하는지 더 깊이 이해하실 수 있을 겁니다.

수고 많으셨습니다.
