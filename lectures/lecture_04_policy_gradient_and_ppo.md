# Lecture 04: Policy Gradient (정책 경사)

## 목차

- [강의 소개](#강의-소개)
  - [강의 개요](#강의-개요)
- [강화학습 기초 복습](#강화학습-기초-복습)
  - [강화학습 문제 정의](#강화학습-문제-정의)
  - [마르코프 결정 과정 (MDP)](#마르코프-결정-과정-mdp)
- [정책 경사 (Policy Gradient)](#정책-경사-policy-gradient)
  - [핵심 아이디어와 목적 함수](#핵심-아이디어와-목적-함수)
  - [정책 경사 정리](#정책-경사-정리)
  - [분산 문제](#분산-문제)
- [분산 감소 기법](#분산-감소-기법)
  - [핵심 수학 공식](#핵심-수학-공식)
  - [Reward-to-Go (인과성)](#reward-to-go-인과성)
  - [가치 함수 (Value Functions)](#가치-함수-value-functions)
  - [베이스라인 (Baseline)](#베이스라인-baseline)
  - [어드밴티지 (Advantage)](#어드밴티지-advantage)
- [Actor-Critic 알고리즘](#actor-critic-알고리즘)
  - [REINFORCE vs Actor-Critic](#reinforce-vs-actor-critic)
  - [남은 문제점](#남은-문제점)
- [고급 기법](#고급-기법)
  - [할인율 (Discount Factor)](#할인율-discount-factor)
  - [N-스텝 리턴](#n-스텝-리턴)
  - [일반화된 어드밴티지 추정 (GAE)](#일반화된-어드밴티지-추정-gae)
  - [중요도 샘플링 (Importance Sampling)](#중요도-샘플링-importance-sampling)
- [PPO (Proximal Policy Optimization)](#ppo-proximal-policy-optimization)
  - [클리핑 메커니즘](#클리핑-메커니즘)
  - [알고리즘 구조](#알고리즘-구조)
  - [장점과 한계](#장점과-한계)
  - [응용 사례](#응용-사례)
- [요약 및 다음 강의 예고](#요약-및-다음-강의-예고)

---

## 강의 소개

### 강의 개요

자, 그럼 오늘 우리가 어떤 여정을 떠나게 될지 한번 살펴보겠습니다.

1. **Recap (복습)**: 먼저 지난 시간에 배웠던 강화학습의 핵심 용어들을 빠르게 복습하면서 오늘의 학습을 위한 준비운동을 하겠습니다.

2. **Policy Gradient Method (정책 경사 방법)**: 오늘 강의의 핵심이죠. 정책 경사가 무엇인지, 그 기본적인 아이디어와 수식을 파헤쳐 보겠습니다.

3. **Reducing Variance (분산 줄이기)**: 정책 경사 방법은 강력하지만, 한 가지 큰 단점이 있습니다. 바로 학습 과정의 분산(variance)이 매우 크다는 점입니다. 이 문제를 해결하기 위한 여러 가지 기법들, 즉 **Reward-to-Go**, **Value Function (가치 함수)**, 그리고 **Baseline (기저선)**의 개념을 차례대로 배우게 될 겁니다.

4. **Advantage (어드밴티지)**: 위에서 배운 개념들을 종합하여 '어드밴티지'라는 더욱 정교한 개념을 도출하고, 이를 통해 정책 경사를 어떻게 더 안정적으로 만들 수 있는지 알아볼 겁니다.

5. **PPO**: 시간이 허락한다면, 오늘날 가장 널리 쓰이는 정책 경사 알고리즘 중 하나인 PPO (Proximal Policy Optimization)에 대해서도 맛보기로 다뤄보겠습니다.

---

## 강화학습 기초 복습

### 강화학습 문제 정의

자, 다시 기본으로 돌아가 봅시다. 강화학습이란 무엇을 하는 학문일까요?

가운데 **에이전트(Agent)**가 있습니다. 우리의 주인공이죠. 이 에이전트는 특정 시점 $t$의 **상태(State)** $s_t$를 관찰하고, 자신의 **정책(Policy)** $\pi(a_t|s_t)$에 따라 어떤 **행동(Action)** $a_t$를 취할지 결정합니다.

이 행동은 **환경(Environment)**에 영향을 미치고, 환경은 그 결과로 두 가지를 에이전트에게 돌려줍니다. 첫째는 행동에 대한 **보상(Reward)** $r_t$이고, 둘째는 다음 시점의 상태 $s_{t+1}$입니다. 이 과정이 계속 반복되면서 $(s_0, a_0, r_0), (s_1, a_1, r_1), \ldots$ 와 같은 경험의 연속, 즉 **궤적(Trajectory)**이 만들어집니다.

여기서 환경이 다음 상태와 보상을 주는 방식은 확률적일 수 있습니다. 이를 각각 상태 전이 확률 $p(s_{t+1}|s_t, a_t)$과 보상 확률 $p(r_t|s_t, a_t)$로 표현할 수 있죠. 이러한 환경의 규칙, 즉 MDP (Markov Decision Process)는 우리에게 주어져 있지만, 대부분의 경우 우리는 그 내부를 정확히 알지 못합니다.

이런 상황 속에서 강화학습의 궁극적인 목표는 무엇일까요? 바로, **누적 보상의 합 $\sum r_t$를 최대화하는 정책 $\pi(a_t|s_t)$를 학습하는 것**입니다. 즉, 어떤 상황에서 어떤 행동을 해야 가장 많은 보상을 받을 수 있는지를 배우는 것이죠.

### 마르코프 결정 과정 (MDP)

앞에서 언급한 강화학습의 수학적 틀을 좀 더 엄밀하게 정의해 보겠습니다. 이것이 바로 **마르코프 결정 과정(MDP)**입니다.

MDP는 다섯 가지 요소, 그리고 할인율을 포함한 튜플(tuple)로 정의됩니다:

$$
\langle S, A, p(s_0), p(s_{t+1}|s_t, a_t), r(s_t, a_t), \gamma \rangle
$$

각 요소의 의미는 다음과 같습니다:

* **$S$: 상태 공간(State Space)** - 에이전트가 존재할 수 있는 모든 가능한 상태들의 집합입니다.

* **$A$: 행동 공간(Action Space)** - 에이전트가 취할 수 있는 모든 가능한 행동들의 집합입니다.

* **$p(s_0)$: 초기 상태 분포** - 에피소드가 처음 시작될 때, 에이전트가 어떤 상태에서 시작할지에 대한 확률 분포입니다.

* **$p(s_{t+1}|s_t, a_t)$: 상태 전이 확률** - 현재 상태 $s_t$에서 행동 $a_t$를 했을 때, 다음 상태가 $s_{t+1}$이 될 확률을 의미합니다. 이것이 바로 환경의 '물리 법칙'이라고 할 수 있습니다.

* **$r(s_t, a_t)$: 보상 함수** - 상태 $s_t$에서 행동 $a_t$를 했을 때 받게 되는 즉각적인 보상입니다.

* **$\gamma$: 할인율(Discount Factor)** - $0$과 $1$ 사이의 값을 가지며, 미래의 보상을 현재 가치로 환산할 때 얼마나 할인할지를 결정합니다. $\gamma$가 $1$에 가까우면 미래의 보상을 중요하게 생각하는 것이고, $0$에 가까우면 당장의 보상에만 집중하는 근시안적인 에이전트가 됩니다.

이 MDP라는 틀 안에서, 우리는 최적의 정책 $\pi$를 찾아 나가는 여정을 떠나는 것입니다.

---

## 정책 경사 (Policy Gradient)

### 핵심 아이디어와 목적 함수

자, 드디어 오늘 강의의 핵심인 **정책 경사(Policy Gradient)**입니다.

우리의 목표는 누적 보상의 기댓값을 최대화하는 정책 $\pi_{\theta}$를 찾는 것이라고 했습니다. 여기서 $\theta$는 정책을 결정하는 파라미터, 예를 들어 뉴럴 네트워크의 가중치(weight)들을 의미합니다. 이 목표를 수식으로 표현하면 다음과 같은 **목적 함수(Objective Function)** $J(\theta)$를 최대화하는 문제가 됩니다.

$$
J(\theta) = \mathbb{E}_{\tau \sim \pi_{\theta}(\tau)}[\sum_t r(\tau)]
$$

이 수식을 풀어서 설명해볼까요? $\tau$는 하나의 궤적(trajectory)을 의미하고, $\pi_{\theta}(\tau)$는 정책 $\pi_{\theta}$를 따랐을 때 궤적 $\tau$가 나타날 확률입니다. 즉, 현재 정책 $\pi_{\theta}$를 따라서 수많은 궤적을 만들어보고, 그 궤적들에서 얻은 총 보상 $\sum_t r(\tau)$의 평균을 내겠다는 의미입니다.

그렇다면 이 $J(\theta)$를 어떻게 최대화할 수 있을까요? 바로 **경사 상승법(Gradient Ascent)**을 사용하면 됩니다. 즉, $J(\theta)$를 $\theta$에 대해 미분한 값, 즉 **그래디언트(Gradient)** $\nabla_{\theta}J(\theta)$를 구해서, 그 그래디언트 방향으로 $\theta$를 조금씩 업데이트하는 것이죠.

### 정책 경사 정리

여기서 아주 중요하고 신기한 수식이 등장합니다. 바로 **정책 경사 정리(Policy Gradient Theorem)**입니다.

$$
\nabla_{\theta}J(\theta) = \mathbb{E}_{\tau \sim \pi_{\theta}(\tau)}[\nabla_{\theta} \log \pi_{\theta}(\tau) r(\tau)]
$$

이 수식을 좀 더 풀어서 우리가 실제로 계산할 수 있는 형태로 바꾸면 다음과 같이 됩니다.

$$
\nabla_{\theta}J(\theta) \approx \frac{1}{N} \sum_{i=1}^{N} \left( \sum_{t=1}^{T} \nabla_{\theta} \log \pi_{\theta}(a_t^i | s_t^i) \right) r(\tau^i)
$$

이 수식의 의미를 직관적으로 파악하는 것이 매우 중요합니다.

* **$\log \pi_{\theta}(a_t | s_t)$**: 상태 $s_t$에서 행동 $a_t$를 할 로그 확률입니다. 여기에 $\nabla_{\theta}$를 취한 것은, '파라미터 $\theta$를 어떻게 바꿔야 이 행동을 할 확률이 높아지는가?'에 대한 방향을 의미합니다.

* **$r(\tau^i)$**: $i$번째 궤적에서 얻은 **총 보상**입니다.

이 둘을 곱한다는 것의 의미는 이렇습니다. 만약 어떤 궤적의 총 보상 $r(\tau^i)$이 높았다면(양수라면), 그 궤적에 포함된 모든 행동 $(a_t^i|s_t^i)$들이 나올 확률을 높이는 방향으로 $\theta$를 업데이트합니다. 반대로 총 보상이 낮았다면(음수라면), 그 행동들이 나올 확률을 낮추는 방향으로 업데이트하는 것이죠. 아주 직관적이고 합리적인 학습 방식이죠? '잘했으면 더 그렇게 하도록, 못했으면 덜 그렇게 하도록' 만드는 겁니다.

### 분산 문제

하지만 이 방식에는 큰 문제가 있습니다. 실제 세계에서는 모든 가능한 궤적을 다 경험해볼 수 없기 때문에, $N$개의 궤적을 샘플링해서 근사적으로 계산해야 합니다. 이 과정에서 발생하는 샘플링 오차 때문에 그래디언트의 **분산(Variance)이 매우 높아지는** 문제가 발생합니다.

#### 분산이 왜 중요한가?

앞서 정책 경사는 분산이 크다고 말씀드렸는데, 이게 왜 문제가 되는지 직관적으로 이해해 봅시다.

여기 두 개의 그림이 있다고 상상해보세요. 우리가 진짜 알고 싶은 값은 기댓값 $\mathbb{E}[X]$입니다. 하지만 우리는 전체 분포를 모르기 때문에, 샘플링을 통해 평균 $\bar{X}$를 구해서 $\mathbb{E}[X]$를 추정해야 합니다.

* **분산이 큰 경우**: 데이터 포인트들이 넓게 퍼져 있습니다. 만약 우리가 운 나쁘게 극단적인 샘플 몇 개만 얻었다면, 우리가 계산한 평균 $\bar{X}$는 실제 기댓값 $\mathbb{E}[X]$와 매우 큰 차이를 보이게 될 겁니다.

* **분산이 작은 경우**: 데이터 포인트들이 기댓값 주변에 옹기종기 모여 있습니다. 이 경우에는 어떻게 샘플링을 하더라도 우리가 계산한 평균 $\bar{X}$가 실제 기댓값 $\mathbb{E}[X]$에서 크게 벗어나지 않을 확률이 높습니다.

이를 정책 경사에 적용해 봅시다. 우리가 계산하는 그래디언트는 $N$개의 궤적을 샘플링해서 얻은 근사치입니다. 만약 이 그래디언트의 분산이 크다면, 어쩌다 얻은 한두 개의 '대박' 궤적이나 '쪽박' 궤적 때문에 그래디언트의 방향이 완전히 잘못된 곳을 가리킬 수 있습니다. 이렇게 되면 학습이 매우 불안정해지고, 제대로 된 정책을 찾기 어려워집니다. 반면 분산이 낮다면, 우리가 계산한 그래디언트는 실제 그래디언트와 유사할 가능성이 높고, 학습은 안정적으로 수렴하게 될 것입니다.

#### 분산을 어떻게 줄일 것인가?

자, 그럼 이 골치 아픈 분산 문제를 어떻게 해결할 수 있을까요? 두 가지 해결책이 있습니다.

1. **Solution 1: $N$을 매우 크게 하기**

가장 단순하고 무식한 방법입니다. 샘플의 개수 $N$을 아주 아주 크게 하면, 대수의 법칙(Law of Large Numbers)에 의해 샘플 평균은 실제 기댓값에 가까워질 겁니다. 하지만 이 방법은 엄청난 계산 비용을 요구합니다. 수많은 궤적을 생성해야 하니까요. 즉, **샘플 효율성(Sample Efficiency)**이 매우 떨어집니다.

2. **Solution 2: 분산이 낮은 추정량(Estimator) 사용하기**

이것이 우리가 앞으로 집중적으로 파고들 똑똑한 방법입니다. 현재의 정책 경사 추정량, 즉 $\frac{1}{N} \sum (\sum \nabla \log \pi) r(\tau)$를 수학적으로 변형하여, **기댓값은 동일하지만 분산은 더 낮은** 새로운 추정량을 만드는 것입니다.

이제부터 이 두 번째 방법을 구현하기 위한 여정을 떠나보겠습니다.

---

## 분산 감소 기법

### 핵심 수학 공식

분산을 줄이는 기법들을 배우기 전에, 아주 중요한 수학적 성질 하나를 짚고 넘어가야 합니다. 앞으로 우리가 분산을 줄이는 모든 기법의 이론적 기반이 될 내용이니 집중해서 들어주세요.

바로 이 공식입니다:

$$
\mathbb{E}_{x \sim p_{\theta}(x)}[\nabla_{\theta} \log p_{\theta}(x)] = 0
$$

어떤 확률 분포 $p_{\theta}(x)$가 있을 때, 그 분포의 로그 값에 그래디언트를 취한 것의 기댓값은 항상 $0$이라는 것입니다. 처음 이 수식을 봤을 때 정말 신기하다고 생각했습니다. 이게 왜 성립하는지 함께 증명해 봅시다.

#### 증명

1. 먼저 **로그 미분 트릭(Log-Derivative Trick)**을 사용합니다. $\log f(x)$를 미분하면 $f'(x)/f(x)$가 되죠. 따라서 다음이 성립합니다:

$$
\nabla_{\theta} \log p_{\theta}(x) = \frac{\nabla_{\theta} p_{\theta}(x)}{p_{\theta}(x)}
$$

2. 이 식의 양변에 $p_{\theta}(x)$를 곱하면, 다음의 유용한 관계식을 얻습니다:

$$
\nabla_{\theta} p_{\theta}(x) = p_{\theta}(x) \nabla_{\theta} \log p_{\theta}(x)
$$

3. 이제 우리가 증명하려는 식의 좌변, 즉 기댓값을 적분 형태로 써보겠습니다:

$$
\mathbb{E}_{x \sim p_{\theta}(x)}[\nabla_{\theta} \log p_{\theta}(x)] = \int p_{\theta}(x) \nabla_{\theta} \log p_{\theta}(x) dx
$$

4. 방금 위에서 유도한 관계식을 대입하면, 적분 안의 항은 $\nabla_{\theta} p_{\theta}(x)$가 됩니다:

$$
= \int \nabla_{\theta} p_{\theta}(x) dx
$$

5. 여기서 미분 기호 $\nabla_{\theta}$와 적분 기호 $\int$의 순서를 바꿀 수 있습니다:

$$
= \nabla_{\theta} \int p_{\theta}(x) dx
$$

6. $p_{\theta}(x)$는 확률 분포이므로, 모든 가능한 $x$에 대해 적분하면 그 값은 항상 $1$이 됩니다:

$$
= \nabla_{\theta} (1)
$$

7. 상수 $1$을 미분하면 $0$이 되죠:

$$
= 0
$$

이렇게 증명이 완료되었습니다. 이 '기댓값이 $0$이 되는 성질'을 이용해서, 우리는 분산을 줄이는 마법을 부리게 될 겁니다. 꼭 기억해두세요.

### Reward-to-Go (인과성)

자, 이제 첫 번째 분산 감소 기법인 **Reward-to-Go**를 배워봅시다. 이 아이디어의 핵심은 바로 **인과성(Causality)**입니다.

기존 정책 경사 수식을 다시 봅시다:

$$
\nabla_{\theta}J(\theta) = \mathbb{E}_{\tau \sim \pi_{\theta}(\tau)}\left[ \left( \sum_{t=1}^{T} \nabla_{\theta} \log \pi_{\theta}(a_t|s_t) \right) r(\tau) \right]
$$

여기서 $r(\tau)$는 $\sum_{t'=1}^{T} r(s_{t'}, a_{t'})$로, 궤적 전체의 보상 합입니다.

이 수식을 자세히 보면, 특정 시점 $t$의 행동 $(a_t|s_t)$에 대한 그래디언트를 계산할 때, 궤적 전체의 보상을 곱해주고 있습니다.

그런데 한번 상식적으로 생각해 봅시다. 시점 $t$에 취한 행동이, 그 이전인 $t-1, t-2, \ldots$ 시점의 보상에 영향을 줄 수 있을까요? 당연히 없습니다. 시간은 미래로만 흐르니까요. 시점 $t$의 정책 $\pi(a_t|s_t)$는 오직 현재와 미래의 보상 $(r_t, r_{t+1}, \ldots)$에만 영향을 미칩니다.

따라서 행동 $(a_t|s_t)$의 좋고 나쁨을 평가할 때, 그 행동과 아무 상관없는 과거의 보상까지 끌어와서 사용하는 것은 불필요한 노이즈, 즉 분산을 더하는 셈입니다. 우리는 오직 **"이 행동으로 인해 앞으로 받게 될 보상"**만을 고려해야 합니다.

#### 수식 증명

이 아이디어를 수식으로 증명해 보겠습니다.

1. 기존의 보상 항 $\sum_{t'=1}^{T}r(s_{t'}, a_{t'})$을 시점 $t$를 기준으로 과거 보상과 미래 보상으로 나눕니다:

$$
\sum_{t'=1}^{t-1}r(s_{t'}, a_{t'}) \quad (\text{Past Rewards}) \quad + \quad \sum_{t'=t}^{T}r(s_{t'}, a_{t'}) \quad (\text{Future Rewards})
$$

2. 이를 원래 정책 경사 수식에 대입하면, 기댓값 안의 항은 두 부분으로 나뉩니다:

$$
\mathbb{E} \left[ \sum_{t=1}^{T} \nabla_{\theta}\log\pi_{\theta}(a_t|s_t) \left( \sum_{t'=1}^{t-1}r_{t'} \right) \right] + \mathbb{E} \left[ \sum_{t=1}^{T} \nabla_{\theta}\log\pi_{\theta}(a_t|s_t) \left( \sum_{t'=t}^{T}r_{t'} \right) \right]
$$

3. 이제 왼쪽 항을 자세히 봅시다. 시점 $t$의 행동은 과거 보상과 독립적입니다. 따라서 기댓값을 분리할 수 있습니다 ($\mathbb{E}[AB] = \mathbb{E}[A]\mathbb{E}[B]$):

$$
\sum_{t=1}^{T} \mathbb{E}[\nabla_{\theta}\log\pi_{\theta}(a_t|s_t)] \cdot \mathbb{E}\left[ \sum_{t'=1}^{t-1}r_{t'} \right]
$$

4. 바로 여기서 우리가 앞서 증명한 마법 같은 공식이 등장합니다. $\mathbb{E}[\nabla_{\theta}\log\pi_{\theta}(a_t|s_t)]$는 $0$입니다! 따라서 왼쪽 항 전체가 $0$이 되어 사라집니다.

결론적으로, 정책 경사 수식에서 과거 보상 항을 제거해도 **기댓값은 변하지 않습니다.** 하지만 이 과거 보상 항은 기댓값이 $0$일지라도, 샘플링된 궤적마다 값이 다르기 때문에 분산을 가지고 있습니다. 따라서 이 불필요한 노이즈 덩어리를 제거함으로써, 우리는 기댓값은 그대로 유지하면서 **분산만 낮추는** 효과를 얻을 수 있습니다.

이렇게 해서 얻어진 새로운 정책 경사 추정량은 다음과 같습니다:

$$
\nabla_{\theta}J(\theta) = \mathbb{E}_{\tau \sim \pi_{\theta}(\tau)} \left[ \sum_{t=1}^{T} \nabla_{\theta} \log \pi_{\theta}(a_t|s_t) \left( \sum_{t'=t}^{T} r(s_{t'}, a_{t'}) \right) \right]
$$

여기서 $\sum_{t'=t}^{T} r(s_{t'}, a_{t'})$를 **Reward-to-Go**라고 부르며, 앞으로 $\hat{Q}_t$로 표기하기도 합니다.

#### 시각적 이해

차이점을 명확하게 보면:

* **기존 방식**: 모든 시점 $t=1, \ldots, T$에서 그래디언트를 계산할 때, 궤적 전체의 보상 합을 똑같이 사용합니다.

* **Reward-to-Go**: 시점 $t$의 그래디언트를 계산할 때는, 오직 그 시점부터 끝까지의 보상 합 $R_t(\tau)$만을 사용합니다. 시간이 지남에 따라 고려하는 보상의 범위가 점점 줄어드는 것을 볼 수 있습니다. 이것이 바로 인과성(Causality)을 제대로 반영한, 훨씬 더 합리적인 신호입니다.

### 가치 함수 (Value Functions)

Reward-to-Go를 사용해서 분산을 줄였지만, 여전히 한계가 있습니다. $\sum_{t'=t}^{T} r_{t'}$라는 값은 단 **하나의 궤적**에서 얻은 샘플 값일 뿐입니다. 이 역시 여전히 노이즈가 많을 수 있죠.

만약 우리가 "시점 $t$의 상태 $s_t$에서 앞으로 정책 $\pi$를 따랐을 때, 평균적으로 어느 정도의 보상을 받을 것으로 기대되는가?"를 알 수 있다면 어떨까요? 이 '평균적인 미래 보상의 기댓값'을 사용하면 단일 궤적의 노이즈를 크게 줄일 수 있을 겁니다. 이것이 바로 **가치 함수(Value Function)**의 역할입니다.

주로 두 가지 가치 함수가 사용됩니다.

#### 상태 가치 함수 (State-Value Function)

**정의**: 상태 $s_t$에서 시작하여 앞으로 정책 $\pi$를 따랐을 때 얻게 될 보상의 총합의 기댓값입니다.

**수식**:

$$
V^{\pi}(s_t) = \mathbb{E}_{\tau \sim \pi(\tau|s_0=s_t)}[\sum_{t'=t}^{T} r(s_{t'}, a_{t'})]
$$

**의미**: 상태 $s_t$에서 출발하면 정책 $\pi$에 따라 수많은 다른 궤적들이 뻗어나갈 수 있습니다. $V^{\pi}(s_t)$는 이 모든 가능한 미래 궤적들의 보상을 평균낸 값입니다.

#### 상태-행동 가치 함수 (State-Action Value Function)

**정의**: 상태 $s_t$에서 특정 행동 $a_t$를 **하고 난 후**, 그 다음부터 정책 $\pi$를 따랐을 때 얻게 될 보상 총합의 기댓값입니다. 흔히 **Q-가치(Q-value)**라고 부릅니다.

**수식**:

$$
Q^{\pi}(s_t, a_t) = \mathbb{E}_{\tau \sim \pi(\tau|s_t, a_t)}[\sum_{t'=t}^{T} r(s_{t'}, a_{t'})]
$$

**의미**: 상태 $s_t$에서 내가 행동 $a_t$를 '강제로' 선택합니다. 그 이후부터는 환경의 확률 ($s_{t+1}$이 결정되는 과정)과 나의 정책 ($a_{t+1}$부터의 선택)에 따라 궤적이 펼쳐지죠. $Q^{\pi}(s_t, a_t)$는 이 미래 궤적들의 보상을 평균낸 값입니다.

#### 가치 함수들의 관계

이 두 가치 함수는 서로 밀접한 관계를 가집니다.

* **$V$와 $Q$의 관계**:

$$
V^{\pi}(s_t) = \mathbb{E}_{a_t \sim \pi(a_t|s_t)}[Q^{\pi}(s_t, a_t)]
$$

상태의 가치는 그 상태에서 할 수 있는 모든 행동들의 Q-가치를 정책에 따라 가중 평균한 값과 같습니다.

* **벨만 방정식**:

$$
Q^{\pi}(s_t, a_t) = r(s_t, a_t) + \mathbb{E}_{s_{t+1} \sim p(s_{t+1}|s_t, a_t)}[V^{\pi}(s_{t+1})]
$$

행동의 가치는 즉각적인 보상과, 그 행동으로 인해 도달하게 될 다음 상태의 가치의 기댓값을 더한 것과 같습니다.

### 베이스라인 (Baseline)

분산을 줄이는 또 다른 강력한 기법, **베이스라인(Baseline)**입니다. 이것은 아주 일반적이면서도 강력한 기법입니다.

#### 베이스라인의 아이디어

아이디어는 이렇습니다. 어떤 행동의 좋고 나쁨을 평가할 때, 그 행동으로 얻은 보상의 절대적인 크기만 보는 것은 비합리적일 수 있습니다. 예를 들어, 어떤 게임에서 모든 행동이 양수의 점수를 준다고 해봅시다. 어떤 행동은 $+1$점, 다른 행동은 $+100$점을 줍니다. 이 경우, $+1$점을 받은 행동의 확률을 낮추고, $+100$점을 받은 행동의 확률을 높여야 합니다. 하지만 기존 방식대로라면 두 행동 모두 보상이 양수이므로 확률을 높이는 방향으로만 학습이 진행될 것입니다.

이 문제를 해결하기 위해, 각 상태마다 일종의 '기준점' 또는 '기대치'를 설정하고, 실제 얻은 보상이 이 기준점보다 높은지 낮은지를 비교하는 것입니다. 이 기준점이 바로 **베이스라인** $b(s_t)$입니다.

우리의 정책 경사 추정량에서 이 베이스라인을 빼봅시다:

$$
\nabla_{\theta}J(\theta) = \mathbb{E}_{\tau \sim \pi_{\theta}(\tau)} \left[ \sum_{t=1}^{T} \nabla_{\theta} \log \pi_{\theta}(a_t|s_t) \left( R_t(\tau) - b(s_t) \right) \right]
$$

여기서 아주 중요한 점은, 베이스라인 $b(s_t)$는 현재 상태 $s_t$에만 의존하고, 행동 $a_t$에는 의존하지 않아야 한다는 것입니다.

#### 베이스라인의 수학적 정당성

왜 그럴까요? 베이스라인을 뺀 항의 기댓값을 계산해 보면, 두 부분으로 나뉩니다:

$$
\mathbb{E}[\ldots (R_t(\tau))] - \mathbb{E}[\ldots (b(s_t))]
$$

두 번째 항, 즉 베이스라인과 관련된 항의 기댓값은 다음과 같습니다:

$$
\mathbb{E}_{\tau \sim \pi_{\theta}(\tau)} \left[ \sum_{t=1}^{T} \nabla_{\theta} \log \pi_{\theta}(a_t|s_t) b(s_t) \right]
$$

여기서 $b(s_t)$는 $a_t$와는 무관하므로 기댓값 밖으로 나갈 수 있고, 결국 우리는 $\mathbb{E}[\nabla_{\theta} \log \pi_{\theta}(a_t|s_t)]$ 항을 마주하게 됩니다. 우리가 강의 초반에 증명했듯이, 이 값은 $0$입니다!

따라서, 상태에만 의존하는 어떤 베이스라인을 빼더라도 정책 경사의 **기댓값은 변하지 않습니다.** 즉, 우리가 올바른 방향으로 가고 있다는 사실은 변하지 않습니다. 하지만 **분산은 바뀔 수 있습니다!**

#### 최적의 베이스라인

그렇다면 분산을 최소화하는 최적의 베이스라인은 무엇일까요?

분산은 결국 $(R_t(\tau) - b(s_t))$ 값의 제곱에 비례합니다. 이 값을 최대한 $0$에 가깝게 만들 수 있다면 분산을 줄일 수 있겠죠. 즉, 베이스라인 $b(s_t)$는 Reward-to-Go $R_t(\tau)$를 가장 잘 예측하는 값이 되어야 합니다.

상태 $s_t$가 주어졌을 때, $R_t(\tau) = \sum_{t'=t}^{T}r_{t'}$의 기댓값은 무엇일까요? 바로 **상태 가치 함수 $V^{\pi}(s_t)$**입니다!

$$
b(s_t) = \mathbb{E}[R_t(\tau) | s_t] = V^{\pi}(s_t)
$$

따라서 분산을 줄이기 위한 최적의 베이스라인은 바로 상태 가치 함수 $V^{\pi}(s_t)$입니다.

이것은 UC 버클리 CS285 강의에서 더 엄밀하게 증명되는데, 그래디언트 추정량의 분산을 $b(s_t)$에 대한 함수로 놓고 미분하여 $0$이 되는 지점을 찾으면 정확히 $b(s_t) = V^{\pi}(s_t)$가 나옵니다.

### 어드밴티지 (Advantage)

앞서 설명한 가치 함수들의 관계를 다시 한번 정리하고, 이를 바탕으로 매우 중요한 개념인 **어드밴티지(Advantage)**를 소개하겠습니다.

* **Q-가치 ($Q^{\pi}$)**: 상태 $s_t$에서 행동 $a_t$를 하는 것이 얼마나 좋은지를 나타내는 절대적인 값입니다.

* **V-가치 ($V^{\pi}$)**: 상태 $s_t$ 자체가 얼마나 좋은지를 나타내는, 그 상태에서 평균적으로 기대할 수 있는 값입니다.

* **어드밴티지 함수 (Advantage Function, $A^{\pi}(s_t, a_t)$)**

**정의**:

$$
A^{\pi}(s_t, a_t) = Q^{\pi}(s_t, a_t) - V^{\pi}(s_t)
$$

**의미**: 이 값이 바로 분산 감소의 다음 열쇠입니다. 어드밴티지는 특정 행동 $a_t$가 그 상태 $s_t$에서 평균적으로 하는 것 ($V^{\pi}(s_t)$)보다 **얼마나 더 좋은지 혹은 나쁜지**를 나타내는 '상대적인' 값입니다.

예를 들어, 어떤 상태에서 모든 행동이 다 좋은 결과를 낳는다고 해봅시다. Q-값들이 전부 $100, 101, 102$ 처럼 높은 값을 가질 수 있습니다. 하지만 이 값들을 그대로 정책 경사에 사용하면, 모든 행동의 확률을 높이려고만 할 겁니다. 어드밴티지를 사용하면, 평균(V-값, 예를 들어 $101$)을 빼주어 각각 $-1, 0, 1$이라는 값을 얻게 됩니다. 이제 우리는 어떤 행동이 '평균보다 나은지'를 명확하게 알 수 있고, 이를 통해 훨씬 더 정교하고 안정적인 학습 신호를 만들 수 있습니다.

#### 상태 가치 함수 학습

자, 그런데 이 V-함수나 Q-함수, 즉 $V^{\pi_{\theta}}(s)$와 $Q^{\pi_{\theta}}(s, a)$의 실제 값을 우리가 어떻게 알 수 있을까요? 보통은 알 수 없습니다. 그래서 우리는 이 가치 함수를 근사하는 또 다른 뉴럴 네트워크, 즉 **가치 네트워크(Value Network)**를 학습시킵니다.

상태 가치 함수 $V_{\phi}^{\pi_{\theta}}(s)$를 학습시키는 과정을 예로 들어봅시다. 여기서 $\phi$는 가치 네트워크의 파라미터입니다.

이것은 일종의 지도 학습(Supervised Learning) 문제입니다.

* **입력(Input)**: 상태 $s_t$
* **출력(Output)**: 가치 네트워크의 예측값 $V_{\phi}^{\pi_{\theta}}(s_t)$
* **정답(Target)**: 실제 가치 $V^{\pi_{\theta}}(s_t)$

문제는 '정답'인 실제 가치를 모른다는 것입니다. 그래서 우리는 '정답'을 추정해야 합니다.

가치 함수의 정의에 따라, $V^{\pi_{\theta}}(s_t) = \mathbb{E}[r_t + V^{\pi_{\theta}}(s_{t+1})]$입니다.

이 식을 이용해서, 우리가 수집한 데이터 $(s_t^i, r_t^i, s_{t+1}^i)$로 다음과 같이 타겟 값을 근사할 수 있습니다:

$$
y_t^i \approx r_t^i + V_{\phi}^{\pi_{\theta}}(s_{t+1}^i)
$$

이것을 **시간차 타겟(Temporal Difference Target, TD 타겟)**이라고 부릅니다. 즉, 실제 받은 즉각적인 보상 $r_t^i$에다가, 다음 상태 $s_{t+1}^i$의 가치에 대한 현재 우리의 '추정치' $V_{\phi}^{\pi_{\theta}}(s_{t+1}^i)$를 더해서 현재 상태의 가치에 대한 타겟으로 삼는 것입니다. 이를 '부트스트래핑(Bootstrapping)'이라고도 합니다.

이제 우리는 지도 학습 문제를 풀 수 있습니다. 손실 함수 $\mathcal{L}(\phi)$를 예측값과 타겟값 사이의 제곱 오차(Squared Error)로 정의하고, 이 손실을 최소화하도록 가치 네트워크의 파라미터 $\phi$를 업데이트하면 됩니다. 이 예측값과 타겟값의 차이를 **TD 에러(TD Error)**라고 합니다.

이렇게 가치 함수를 학습시키는 방법을 배웠습니다.

#### 가치 함수를 이용한 어드밴티지 계산

지난 시간에 우리는 어드밴티지 함수 $A^{\pi}(s, a) = Q^{\pi}(s, a) - V^{\pi}(s)$가 '평균보다 얼마나 더 좋은가'를 나타내는 아주 좋은 신호가 될 것이라고 배웠습니다. 그런데 여기서 한 가지 문제가 있습니다. 어드밴티지를 계산하려면 Q-함수와 V-함수를 둘 다 알아야 합니다. 그렇다면 우리는 이 두 개의 함수를 근사하기 위해 신경망을 두 개나 학습시켜야 할까요?

다행히도 그럴 필요가 없습니다. 아주 영리한 방법이 있습니다. 바로 Q-함수와 V-함수 사이의 관계를 이용하는 것입니다.

$Q^{\pi}(s_t, a_t)$의 정의를 다시 떠올려 봅시다. 상태 $s_t$에서 행동 $a_t$를 한 후의 보상 기댓값이죠. 이것은 즉각적인 보상 $r_t$와, 다음 상태 $s_{t+1}$의 가치 $V^{\pi}(s_{t+1})$의 기댓값의 합으로 표현할 수 있습니다:

$$
Q^{\pi_{\theta}}(s_t, a_t) = \mathbb{E}_{s_{t+1} \sim p(s_{t+1}|s_t, a_t)}[r_t + V^{\pi_{\theta}}(s_{t+1})]
$$

실제 데이터를 가지고 Q-값을 추정할 때는, 우리가 경험한 다음 상태 $s_{t+1}^i$를 사용하여 근사할 수 있습니다:

$$
Q^{\pi_{\theta}}(s_t^i, a_t^i) \approx r_t^i + V_{\phi}^{\pi_{\theta}}(s_{t+1}^i)
$$

여기서 $V_{\phi}^{\pi_{\theta}}$는 우리가 학습시킨 가치 네트워크입니다.

이제 이 근사된 Q-값을 원래 어드밴티지 수식에 대입해 봅시다:

$$
A_{\phi}^{\pi_{\theta}}(s_t^i, a_t^i) = Q_{\phi}^{\pi_{\theta}}(s_t^i, a_t^i) - V_{\phi}^{\pi_{\theta}}(s_t^i)
$$

$$
A_{\phi}^{\pi_{\theta}}(s_t^i, a_t^i) \approx \left( r_t^i + V_{\phi}^{\pi_{\theta}}(s_{t+1}^i) \right) - V_{\phi}^{\pi_{\theta}}(s_t^i)
$$

놀랍지 않습니까? 우리는 결국 V-함수를 근사하는 **단 하나의 신경망 ($V_{\phi}^{\pi_{\theta}}$)만 있으면** 어드밴티지를 계산할 수 있게 되었습니다! 이것은 계산적으로 매우 효율적인 방법이며, 실제로 많은 Actor-Critic 계열 알고리즘에서 표준적으로 사용되는 방식입니다.

---

## Actor-Critic 알고리즘

자, 이제 우리가 지금까지 논의한 모든 조각들을 하나로 합쳐봅시다.

1. 우리는 정책 경사의 가중치로 Reward-to-Go, $R_t(\tau)$를 사용하기로 했습니다 (Causality).

2. 분산을 더 줄이기 위해 베이스라인을 빼주기로 했습니다. 그리고 최적의 베이스라인은 $V^{\pi_{\theta}}(s_t)$라는 것을 알았습니다.
   * 따라서 가중치는 이제 $R_t(\tau) - V^{\pi_{\theta}}(s_t)$가 됩니다.

3. 한편, $R_t(\tau)$라는 단일 샘플 대신, 더 안정적인 추정치인 $Q^{\pi_{\theta}}(s_t, a_t)$를 사용하면 분산을 더 줄일 수 있다고 했습니다.

이 아이디어들을 모두 결합하면, 우리의 최종적인 정책 경사 가중치는 바로 이것이 됩니다:

$$
Q^{\pi_{\theta}}(s_t, a_t) - V^{\pi_{\theta}}(s_t)
$$

이것이 바로 우리가 이전에 정의했던 **어드밴티지 함수 $A^{\pi_{\theta}}(s_t, a_t)$**입니다!

최종적인 정책 경사 업데이트 규칙은 다음과 같습니다:

$$
\nabla_{\theta}J(\theta) = \mathbb{E}_{\tau \sim \pi_{\theta}(\tau)} \left[ \sum_{t=1}^{T} \nabla_{\theta} \log \pi_{\theta}(a_t|s_t) A^{\pi_{\theta}}(s_t, a_t) \right]
$$

이것이 바로 **Actor-Critic 알고리즘**의 핵심입니다.

* **Actor (배우, 정책 $\pi_{\theta}$)**: 어떤 행동을 할지 결정합니다.
* **Critic (비평가, 가치 함수 $V_{\phi}$ 또는 $Q_{\phi}$)**: 배우가 한 행동이 평균보다 얼마나 좋았는지(어드밴티지)를 평가하여 피드백을 줍니다.

Actor는 Critic의 긍정적인 피드백(높은 어드밴티지)을 받은 행동은 더 많이 하도록, 부정적인 피드백(낮은 어드밴티지)을 받은 행동은 덜 하도록 자신의 정책을 업데이트합니다. 오늘날 사용되는 대부분의 실용적인 정책 경사 알고리즘이 바로 이 Actor-Critic 구조를 기반으로 하고 있습니다.

### REINFORCE vs Actor-Critic

이제 알고리즘의 관점에서 이 차이를 명확히 해봅시다.

#### REINFORCE

가장 기본적인 몬테카를로 정책 경사 알고리즘입니다.

1. 현재 정책 $\pi_{\theta}$를 사용해 **하나의 완전한 궤적(Trajectory)**이 끝날 때까지 데이터를 수집합니다.
2. 그 궤적의 보상을 바탕으로 Reward-to-Go를 계산합니다. (또는 그냥 총 보상을 사용하기도 합니다)
3. 계산된 그래디언트를 사용해 정책 $\theta$를 **한 번** 업데이트합니다.
4. 다시 1번으로 돌아가 새로운 궤적을 수집합니다.

**단점**: 정책을 한 번 업데이트하기 위해 궤적 하나가 통째로 필요합니다. 만약 궤적의 길이가 1000 스텝이라면, 1000번의 상호작용이 일어나야 겨우 한 번 학습이 일어나는 셈이죠. 이는 매우 **샘플 비효율적(Sample Inefficient)**입니다.

#### Actor-Critic

1. 현재 정책(Actor) $\pi_{\theta}$를 사용해 정해진 N 스텝(예: 10 스텝)만큼만 데이터를 수집합니다.
2. 수집된 데이터 $(s_i, a_i, r_i, s_{i+1})$를 이용해 Critic ($V_{\phi}$)을 업데이트합니다. (TD 학습)
3. 업데이트된 Critic을 이용해 어드밴티지 $A(s_i, a_i)$를 계산합니다.
4. 이 어드밴티지를 이용해 Actor ($\pi_{\theta}$)를 업데이트합니다.

**장점**: 궤적이 끝날 때까지 기다릴 필요 없이, 소량의 데이터만으로도 Actor와 Critic을 계속해서 업데이트할 수 있습니다. REINFORCE가 1000 스텝마다 한 번 업데이트할 때, Actor-Critic은 10 스텝마다 업데이트하여 총 100번의 정책 업데이트를 수행할 수 있습니다. 훨씬 더 **샘플 효율적(Sample Efficient)**이죠.

### 남은 문제점

자, Actor-Critic이 REINFORCE보다 샘플 효율성 측면에서 훨씬 뛰어나다는 것을 알았습니다. 하지만 이것으로 모든 문제가 해결된 것일까요? 아쉽게도 그렇지 않습니다. 두 가지 주요 문제가 남아있습니다.

1. **어드밴티지 추정이 여전히 부정확하다 (Advantage Estimation is Not Accurate)**: 우리가 사용하는 어드밴티지, 즉 $r_t + \gamma V(s_{t+1}) - V(s_t)$는 Critic 네트워크 $V$의 추정치에 크게 의존합니다. 만약 Critic이 부정확하다면(학습 초기에는 당연히 부정확하겠죠), Actor에게 잘못된 학습 신호를 주게 됩니다. 이것이 바로 **편향(Bias)** 문제입니다.

2. **데이터가 단 한 번의 그래디언트 업데이트에만 사용된다 (Data Can Be Used for Only One Gradient Update)**: 지금까지 배운 알고리즘들은 모두 **온-폴리시(On-Policy)** 방식입니다. 즉, 현재 정책 $\pi_{\theta}$로 수집한 데이터는 오직 $\pi_{\theta}$를 업데이트하는 데에만 사용해야 합니다. 정책이 $\pi_{\theta'}$으로 약간이라도 업데이트되고 나면, 이전에 수집한 데이터는 '오래된 데이터(Off-Policy Data)'가 되어 버려야 합니다. 이 역시 데이터의 활용도를 떨어뜨리는 비효율을 낳습니다.

이 두 가지 문제를 해결하기 위해, 각각 **N-step Returns**와 **Importance Sampling**이라는 기법이 등장합니다.

---

## 고급 기법

### 할인율 (Discount Factor)

자, 이제 잠시 샛길로 빠져서 강화학습의 또 다른 기본 구성 요소인 **할인율(Discount Factor)**, 즉 감마 ($\gamma$)에 대해 이야기해 보겠습니다.

지금까지 우리는 목표를 총 보상의 합, $\sum r_t$를 최대화하는 것으로 설정했습니다. 그런데 여기에는 몇 가지 잠재적인 문제가 있습니다. 만약 에피소드가 끝나지 않고 무한히 계속된다면 어떻게 될까요? 보상의 합이 무한대로 발산해버릴 수 있습니다.

이를 방지하고, 또 다른 중요한 효과를 주기 위해 **할인율 $\gamma$** ($0$과 $1$ 사이의 값)를 도입합니다. 이제 우리의 목표는 할인된 보상의 합, $\sum_{t=1}^{T} \gamma^{t-1} r_t$를 최대화하는 것으로 바뀝니다.

현재 시점 ($t=1$)의 보상 $r_1$은 그대로 가치가 $1$이지만, 다음 시점의 보상 $r_2$는 $\gamma$만큼 할인되고, 그 다음 보상 $r_3$는 $\gamma^2$만큼 할인됩니다. 만약 $\gamma=0.9$라면, 다섯 단계 후의 보상 ($r_5$)의 현재 가치는 원래 가치의 $0.9^4 \approx 0.6$배밖에 되지 않습니다.

이 할인율 $\gamma$는 에이전트의 성격을 결정합니다.

* **$\gamma$가 $1$에 가까우면**: 미래의 보상을 현재 보상만큼이나 중요하게 생각하는, 멀리 내다보는 **장기적인(Long-term)** 관점의 에이전트가 됩니다.

* **$\gamma$가 $0$에 가까우면**: 당장의 보상에만 집중하고 미래는 거의 신경 쓰지 않는 **단기적인(Short-term)**, 근시안적인 에이전트가 됩니다.

#### 할인율을 사용하는 이유

그렇다면 왜 할인율을 사용할까요?

1. **수학적 편의성**: 무한한 길이의 에피소드에서 총 보상이 발산하는 것을 막아주어, 수렴성을 보장하고 분석을 용이하게 합니다.

2. **불확실성 모델링**: 미래는 불확실합니다. 당장 받는 보상이 나중에 받을 보상보다 더 확실하죠. 할인율은 이러한 미래의 불확실성을 반영하는 역할을 합니다. 마치 '에이전트가 매 스텝마다 $1-\gamma$의 확률로 소멸할 수도 있다'고 가정하는 것과 같습니다.

3. **현실 세계와의 유사성**: 경제학에서 현재의 돈이 미래의 돈보다 가치가 높다고 보는 '시간 선호' 개념과도 일맥상통합니다.

이 할인율은 우리의 목적 함수뿐만 아니라, 가치 함수를 정의하는 벨만 방정식에도 자연스럽게 녹아들어갑니다. 이제부터는 모든 가치 계산에 이 할인율 $\gamma$가 포함된다고 생각하시면 됩니다.

### N-스텝 리턴

첫 번째 문제, 즉 부정확한 어드밴티지 추정 문제를 해결하기 위한 N-스텝 리턴에 대해 알아봅시다. 이것은 강화학습에서 아주 중요한 **편향-분산 트레이드오프(Bias-Variance Tradeoff)**와 관련이 깊습니다.

우리가 Q-값을 추정하는 두 가지 극단적인 방법이 있습니다.

#### 몬테카를로(MC) 방식

$$
\hat{Q}_{MC}^{\pi}(s_t, a_t) = \sum_{t'=t}^{T} r_{t'}
$$

* 궤적이 끝날 때까지 실제로 받은 모든 보상을 더합니다.
* **장점**: 실제 보상 값을 사용하므로 **편향이 없습니다(No Bias)**.
* **단점**: 하나의 샘플 궤적에 전적으로 의존하므로 **분산이 매우 높습니다(High Variance)**.

#### 시간차(TD) 방식 (1-step Return)

$$
\hat{Q}^{\pi}(s_t, a_t) = r_t + \gamma V^{\pi}(s_{t+1})
$$

* 한 스텝의 실제 보상과, 그 다음 상태의 가치 함수 추정치 $V^{\pi}(s_{t+1})$를 사용합니다.
* **장점**: 여러 궤적의 정보가 압축된 가치 함수를 사용하므로 **분산이 낮습니다(Low Variance)**.
* **단점**: $V^{\pi}(s_{t+1})$라는 추정치에 의존하므로, 만약 이 추정치가 부정확하다면 결과적으로 편향된(Biased) 값을 얻게 됩니다. **편향이 높습니다(High Bias)**.

#### N-스텝 리턴

N-스텝 리턴은 이 두 극단 사이의 균형을 잡는 방법입니다. 1-스텝이 아닌, $n$-스텝만큼은 실제 보상을 사용하고, 그 이후의 가치는 가치 함수 추정치로 대체하는 것입니다.

$$
\hat{Q}_{n}^{\pi}(s_t, a_t) = \sum_{t'=t}^{t+n-1} \gamma^{t'-t} r_{t'} + \gamma^n V^{\pi}(s_{t+n})
$$

* $n=1$이면 TD 방식이 되고, $n=\infty$이면 몬테카를로 방식이 됩니다.
* $n$을 적절히 조절함으로써, 우리는 편향과 분산 사이의 스위트 스팟(Sweet Spot)을 찾을 수 있습니다.

물론, "최적의 $n$은 무엇인가?"라는 새로운 질문이 생깁니다. 이 질문에 대한 더 정교한 해답이 바로 다음에서 다룰 **일반화된 어드밴티지 추정(Generalized Advantage Estimation, GAE)**입니다.

### 일반화된 어드밴티지 추정 (GAE)

N-스텝 리턴의 문제는 어떤 $n$이 가장 좋은지 알기 어렵다는 것이었습니다. GAE는 "그렇다면 모든 $n$에 대한 어드밴티지 추정치를 전부 다 사용해서 평균을 내면 어떨까?"라는 아이디어에서 출발합니다. 다만 그냥 평균을 내는 것이 아니라, **지수 가중 평균(Exponentially-Weighted Average)**을 사용합니다.

$$
\hat{A}_{GAE}^{\pi}(s_{t},a_{t})=\sum_{n=1}^{\infty}w_{n}\hat{A}_{n}^{\pi}(s_{t},a_{t}) \text{, 여기서 } w_{n}\propto\lambda^{n-1}
$$

여기서 $\lambda$ (람다)라는 새로운 하이퍼파라미터가 등장합니다. 이 $\lambda$는 $0$과 $1$ 사이의 값을 가지며, 편향(Bias)과 분산(Variance) 사이의 트레이드오프를 조절하는 아주 중요한 역할을 합니다.

* **만약 $\lambda=0$이라면**: 가중치는 1-스텝 어드밴티지에만 부여되고 나머지는 모두 $0$이 됩니다. 즉, GAE는 정확히 1-스텝 TD 어드밴티지 추정치가 됩니다:

$$
\hat{A}_{GAE}^{\pi}(s_{t},a_{t})=r_{t}+\gamma V^{\pi}(s_{t+1})-V^{\pi}(s_{t})
$$

이것은 우리가 이전에 배운 것처럼, 분산은 낮지만 Critic의 추정치에 의존하기 때문에 편향이 높은 방식입니다.

* **만약 $\lambda=1$이라면**: 모든 $n$-스텝 어드밴티지에 동일한 가중치를 부여하게 되어, 결국 몬테카를로 방식의 어드밴티지 추정치와 같아집니다:

$$
\hat{A}_{GAE}^{\pi}(s_{t},a_{t})=\sum_{t'=t}^{\infty}\gamma^{t'-t}r_{t'}-V^{\pi}(s_{t})
$$

이것은 실제 보상을 끝까지 사용하므로 편향은 낮지만, 단일 궤적에 의존하여 분산이 매우 높은 방식입니다.

결국 이 $\lambda$라는 값을 $0$과 $1$ 사이에서 조절함으로써, 우리는 편향이 높은 TD 방식과 분산이 높은 몬테카를로 방식 사이의 스펙트럼을 부드럽게 오가며 문제에 맞는 최적의 지점을 찾을 수 있게 됩니다. 이것이 바로 GAE의 강력함입니다.

### 중요도 샘플링 (Importance Sampling)

자, GAE를 통해 우리는 어드밴티지 추정치를 훨씬 더 정교하게 만들었습니다. 이제 Actor-Critic의 두 번째 문제, 즉 **온-폴리시(On-Policy) 학습의 데이터 비효율성** 문제를 해결해 봅시다.

정책 경사 수식의 핵심은 바로 기댓값 $\mathbb{E}_{\tau \sim \pi_{\theta}(\tau)}$입니다. 이 기댓값은 **현재 정책 $\pi_{\theta}$를 따라서** 샘플링한 궤적 $\tau$에 대해서만 올바르게 계산됩니다.

그런데 우리가 경사 상승법으로 정책 파라미터 $\theta$를 $\theta'$으로 딱 한 번 업데이트하고 나면, 우리가 가지고 있던 데이터는 더 이상 새로운 정책 $\pi_{\theta'}$에서 나온 것이 아닌, 낡은 정책 $\pi_{\theta}$에서 나온 **오프-폴리시(Off-Policy) 데이터**가 되어버립니다. 그래서 기존 방식으로는 이 아까운 데이터를 버리고, 새로운 정책 $\pi_{\theta'}$으로 다시 궤적을 샘플링해야만 했습니다. 매우 비효율적이죠.

이 문제를 해결하기 위한 돌파구가 바로 통계학의 오래된 기법인 **중요도 샘플링(Importance Sampling)**입니다.

#### 중요도 샘플링의 수학적 원리

중요도 샘플링의 핵심 아이디어는 이렇습니다. 우리가 분포 $p(x)$에 대한 기댓값 $\mathbb{E}_{x \sim p(x)}[f(x)]$을 알고 싶은데, 샘플은 다른 분포 $q(x)$에서만 얻을 수 있는 상황이라고 해봅시다.

이때 간단한 수학적 트릭을 사용할 수 있습니다:

$$
\mathbb{E}_{x \sim p(x)}[f(x)] = \int p(x)f(x)dx = \int q(x) \frac{p(x)}{q(x)} f(x)dx
$$

이 마지막 항을 잘 보면, 이것은 분포 $q(x)$에 대한 $\frac{p(x)}{q(x)}f(x)$라는 새로운 함수의 기댓값과 같습니다:

$$
= \mathbb{E}_{x \sim q(x)}\left[\frac{p(x)}{q(x)} f(x)\right]
$$

여기서 $\frac{p(x)}{q(x)}$를 **중요도 가중치(Importance Weight)**라고 부릅니다. 이 가중치가 두 분포의 차이를 보정해주는 역할을 하는 것이죠.

이 원리를 우리의 정책 경사에 그대로 적용할 수 있습니다. 새로운 정책 $\pi_{\theta}$에 대한 목적 함수를, 낡은 정책 $\pi_{\theta_{old}}$에서 수집한 데이터를 이용해 계산하는 겁니다.

중요도 샘플링을 적용한 목적 함수를 $\theta$에 대해 미분하면, 중요도 가중치 $\frac{\pi_{\theta}(\tau)}{\pi_{\theta_{old}}(\tau)}$가 곱해진 새로운 형태의 정책 경사 추정량을 얻을 수 있습니다:

$$
\nabla_{\theta}J(\theta) = \mathbb{E}_{\tau \sim \pi_{\theta_{old}}(\tau)}\left[ \frac{\pi_{\theta}(\tau)}{\pi_{\theta_{old}}(\tau)} \nabla_{\theta} \log \pi_{\theta}(\tau) r(\tau) \right]
$$

이 식은 이론적으로는 완벽하지만, 궤적 전체의 확률 비율 $\frac{\pi_{\theta}(\tau)}{\pi_{\theta_{old}}(\tau)}$을 계산하는 것은 매우 복잡하고 불안정합니다. 그래서 실제로는 이 식을 근사하여, 각 타임스텝 $t$에서의 확률 비율의 곱으로 분해합니다. 만약 두 정책 $\pi_{\theta}$와 $\pi_{\theta_{old}}$가 매우 비슷하다면, 이 근사는 꽤 잘 작동합니다.

#### 중요도 샘플링을 적용한 Actor-Critic 알고리즘

중요도 샘플링을 도입함으로써 우리의 알고리즘은 이제 훨씬 더 효율적으로 데이터를 활용할 수 있게 됩니다.

1. 먼저 낡은 정책 $\pi_{\theta}$ (나중에 $\pi_{\theta_{old}}$가 될)를 이용해 $N$개의 데이터를 수집합니다.
2. 이 데이터로 Critic을 학습시키고 어드밴티지를 계산합니다.
3. 그리고 이제, **반복 루프**를 돕니다. 이 $N$개의 데이터를 버리지 않고, **여러 번 ($K$번) 재사용**하여 정책을 업데이트합니다.
4. 각 업데이트 스텝에서는 중요도 가중치 $\frac{\pi_{\theta^{(k)}}(a_i|s_i)}{\pi_{\theta_{old}}(a_i|s_i)}$를 곱하여 그래디언트를 계산하고, 정책을 $\theta^{(k)}$에서 $\theta^{(k+1)}$로 업데이트합니다.

하나의 데이터 묶음으로 여러 번의 학습이 가능해졌으니, 샘플 효율성이 크게 향상된 것입니다!

#### 중요도 샘플링의 함정

하지만 세상에 공짜 점심은 없죠. 중요도 샘플링은 강력하지만 아주 위험한 함정을 가지고 있습니다.

바로 중요도 가중치 $\frac{\pi_{\theta}(a|s)}{\pi_{\theta_{old}}(a|s)}$의 분산 문제입니다. 만약 업데이트를 여러 번 진행해서 새로운 정책 $\pi_{\theta}$가 낡은 정책 $\pi_{\theta_{old}}$와 너무 많이 달라지면 어떻게 될까요? 이 가중치 값이 어떤 샘플에서는 $0$에 가까워지고, 어떤 샘플에서는 폭발적으로 커질 수 있습니다. 이렇게 되면 그래디언트 추정량의 분산이 엄청나게 커져서 학습이 완전히 불안정해지고 망가져 버립니다.

따라서 중요도 샘플링을 안정적으로 사용하기 위한 핵심은 **"새로운 정책이 낡은 정책으로부터 너무 멀리 벗어나지 않도록 제약을 거는 것"**입니다.

이 제약을 거는 방식에 따라 여러 알고리즘이 파생됩니다.

* **TRPO (Trust Region Policy Optimization)**: 정책의 분포 사이의 거리(KL-Divergence)가 특정 반경(신뢰 영역, Trust Region)을 벗어나지 않도록 제약합니다. 효과는 좋지만 계산이 매우 복잡합니다.

* **PPO (Proximal Policy Optimization)**: TRPO의 아이디어를 훨씬 더 간단한 방식으로 구현한 알고리즘입니다. 복잡한 제약 조건 대신, **목적 함수 자체를 클리핑(Clipping)**하는 영리한 방법을 사용합니다.

---

## PPO (Proximal Policy Optimization)

PPO는 앞서 배운 Actor-Critic, GAE, 중요도 샘플링 기법들을 모두 모은 결정체와도 같습니다. 그리고 여기에 PPO만의 독창적인 아이디어인 **클리핑된 목적 함수(Clipped Objective Function)**가 추가됩니다.

### 클리핑 메커니즘

목표는 간단합니다. 중요도 비율 $ratio_t(\theta) = \frac{\pi_{\theta}(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}$가 너무 커지거나 작아지는 것을 막는 것입니다. PPO는 이 목표를 아주 직관적인 방법으로 달성합니다.

#### Case 1: 어드밴티지 ($A$)가 양수일 때 (잘한 행동)

* 우리는 이 행동의 확률을 높이고 싶으므로, $ratio_t(\theta)$를 키우는 방향으로 정책을 업데이트해야 합니다.
* 하지만 $ratio_t(\theta)$가 $1+\epsilon$ (예: $1.2$)보다 더 커지려고 하면, PPO는 목적 함수의 값을 $(1+\epsilon)A$로 **강제로 제한(Clip)**해버립니다. 더 이상 정책이 과도하게 변해도 추가적인 이득을 얻지 못하게 막는 것이죠.
* 수식: $\min(ratio_t(\theta)A, (1+\epsilon)A)$

#### Case 2: 어드밴티지 ($A$)가 음수일 때 (못한 행동)

* 우리는 이 행동의 확률을 낮추고 싶으므로, $ratio_t(\theta)$를 줄이는 방향으로 업데이트해야 합니다.
* 하지만 $ratio_t(\theta)$가 $1-\epsilon$ (예: $0.8$)보다 더 작아지려고 하면, PPO는 목적 함수의 값을 $(1-\epsilon)A$로 제한합니다. 정책이 한 번에 너무 비관적으로 변하는 것을 막아줍니다.
* 수식: $\max(ratio_t(\theta)A, (1-\epsilon)A)$

이 두 가지 경우를 하나의 식으로 합치면 다음과 같은 최종 PPO 목적 함수가 됩니다:

$$
J(\theta) = \mathbb{E} \left[ \min(ratio_t(\theta)A_t, \text{clip}(ratio_t(\theta), 1-\epsilon, 1+\epsilon)A_t) \right]
$$

이 간단한 `min`과 `clip` 연산을 통해 PPO는 정책이 급격하게 변하는 것을 막아 학습의 안정성을 크게 높입니다.

### 알고리즘 구조

지금까지 설명한 모든 것을 종합한 최종 PPO 알고리즘은 다음과 같습니다:

1. 현재 정책 $\pi_{\theta_{old}}$를 사용해 $N$개의 데이터 샘플 수집
2. 수집된 데이터로 Critic ($V_{\phi}$) 학습 및 어드밴티지 계산
3. $K$번의 에포크 동안 반복:
   - 중요도 비율 $ratio_t(\theta) = \frac{\pi_{\theta}(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}$ 계산
   - 클리핑된 목적 함수 $J(\theta) = \mathbb{E}[\min(ratio_t A_t, \text{clip}(ratio_t, 1-\epsilon, 1+\epsilon)A_t)]$ 최대화
   - 정책 파라미터 $\theta$ 업데이트
4. $\theta_{old} \leftarrow \theta$ (현재 정책을 낡은 정책으로 설정)
5. 1번으로 돌아가기

### 장점과 한계

#### 장점

그렇다면 PPO는 왜 이렇게 인기가 많을까요?

* **구현이 간단합니다**: 복잡한 2차 미분 계산이 필요한 TRPO와 달리, PPO는 비교적 간단한 코드로 구현할 수 있습니다.

* **성능이 뛰어납니다**: 단순함에도 불구하고 매우 다양한 환경에서 최고 수준의 성능을 보여줍니다.

* **확장성이 좋습니다**: 병렬 처리에 용이하여 대규모 학습이 가능합니다.

#### 한계

물론 단점도 있습니다. 롤아웃 길이 ($N$), 배치 사이즈, 클리핑 계수 ($\epsilon$) 등 여러 하이퍼파라미터에 민감하여 튜닝이 필요할 수 있습니다.

PPO는 중요도 샘플링을 통해 데이터 효율성을 크게 개선했지만, 여전히 근본적으로는 **온-폴리시 계열**의 알고리즘으로 분류됩니다. 왜냐하면 클리핑 제약 때문에 정책이 데이터를 수집한 낡은 정책에서 너무 멀리 벗어날 수 없기 때문입니다. 즉, 아주 오래된 데이터나, 완전히 다른 여러 정책에서 수집된 데이터를 자유롭게 활용하기는 어렵습니다.

### 응용 사례

이러한 PPO와 같은 강력한 정책 경사 알고리즘들은 이제 이론 속에만 머물러 있지 않습니다. 우리 주변의 놀라운 기술들을 가능하게 하고 있습니다.

* 복잡한 로봇 팔 제어를 통해 조립 라인을 자동화하거나, 어려운 물체 조작 작업을 수행합니다.

* 네 발 로봇이 파쿠르를 하거나, 인간형 로봇이 울퉁불퉁한 야지를 자유롭게 걷는 등, 동물과 인간의 움직임을 모사하는 놀라운 제어 능력을 보여줍니다.

* **그리고 아마도 가장 유명한 응용 사례일 것입니다. 바로 ChatGPT와 같은 대규모 언어 모델을 훈련시키는 RLHF (Reinforcement Learning from Human Feedback) 기술입니다.**

#### RLHF에서의 PPO

1. **1단계 (지도 학습)**: 먼저 사람이 직접 작성한 좋은 답변 예시들을 가지고 언어 모델을 지도 학습시킵니다 (Supervised Fine-Tuning).

2. **2단계 (보상 모델 학습)**: 모델이 생성한 여러 답변들을 사람이 보고 순위를 매깁니다. 이 데이터를 이용해 어떤 답변이 더 좋은지(사람이 선호하는지)를 예측하는 별도의 '보상 모델'을 학습시킵니다.

3. **3단계 (PPO를 이용한 강화학습)**: 이 보상 모델을 강화학습의 '환경'처럼 사용합니다. 언어 모델(정책)이 답변을 생성하면, 보상 모델이 그 답변에 대한 점수(보상)를 줍니다. 그리고 바로 이 과정에서 **PPO 알고리즘**을 사용하여, 보상 모델로부터 높은 점수를 받는 방향으로 언어 모델의 정책을 최적화합니다.

우리가 지금 사용하는 자연스러운 대화형 AI의 핵심에 바로 오늘 배운 PPO가 자리 잡고 있는 것입니다.

---

## 요약 및 다음 강의 예고

### 학습 내용 정리

오늘 우리는 정책 경사 (Policy Gradient)의 깊은 세계를 탐험했습니다. 핵심 내용을 정리하면:

1. **정책 경사의 기본 원리**:
   - 목적 함수 $J(\theta)$를 최대화하는 방향으로 정책 파라미터 $\theta$를 업데이트
   - 정책 경사 정리를 통한 그래디언트 계산
   - 분산 문제의 중요성

2. **분산 감소 기법**:
   - **Reward-to-Go**: 인과성을 고려하여 미래 보상만 사용
   - **가치 함수**: $V^{\pi}(s)$와 $Q^{\pi}(s,a)$를 통한 안정적인 추정
   - **베이스라인**: 상태 가치 함수를 기준점으로 사용
   - **어드밴티지**: $A^{\pi}(s,a) = Q^{\pi}(s,a) - V^{\pi}(s)$로 상대적 좋음을 측정

3. **Actor-Critic 알고리즘**:
   - Actor (정책)와 Critic (가치 함수)의 협력
   - REINFORCE보다 훨씬 높은 샘플 효율성

4. **고급 기법**:
   - **할인율 ($\gamma$)**: 미래 보상의 현재 가치 조절
   - **N-스텝 리턴**: 편향-분산 트레이드오프 조절
   - **GAE**: 모든 $n$-스텝 추정치의 지수 가중 평균
   - **중요도 샘플링**: 오래된 데이터 재사용

5. **PPO (Proximal Policy Optimization)**:
   - 클리핑 메커니즘으로 안정적인 정책 업데이트
   - 구현 간단, 성능 우수, 확장성 좋음
   - ChatGPT 등 실제 응용 사례

### 다음 시간 예고

이 한계를 극복하고, 수집된 모든 과거 데이터를 재활용하여 샘플 효율성을 극한으로 끌어올리는 방법이 바로 **오프-폴리시(Off-Policy) RL**입니다.

다음 시간에는 오프-폴리시 RL의 대표적인 알고리즘들인 DQN, DDPG, SAC에 대해 배우면서 강화학습의 또 다른 세계를 탐험해 보도록 하겠습니다.

감사합니다.
