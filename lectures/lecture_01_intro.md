# Lecture 01: 강화학습 입문 (Introduction to Reinforcement Learning)

## 목차
- [강의 소개](#강의-소개)
- [강화학습의 정의](#강화학습의-정의)
  - [서튼과 바르토의 정의](#서튼과-바르토의-정의)
  - [에이전트와 환경의 상호작용](#에이전트와-환경의-상호작용)
  - [강화학습의 세 가지 관점](#강화학습의-세-가지-관점)
- [강의 범위와 목표](#강의-범위와-목표)
  - [학습 범위](#학습-범위)
  - [커리큘럼](#커리큘럼)
  - [학습 목표](#학습-목표)
- [강의 운영](#강의-운영)
  - [선수과목](#선수과목)
  - [평가 정책](#평가-정책)
  - [주의사항](#주의사항)
  - [참고자료](#참고자료)
- [강화학습의 중요성](#강화학습의-중요성)
  - [최신 AI 기술의 발전](#최신-ai-기술의-발전)
  - [알파고의 의미](#알파고의-의미)
- [실제 응용 사례](#실제-응용-사례)
  - [반도체 설계 혁신: PrefixRL](#반도체-설계-혁신-prefixrl)
  - [ChatGPT의 핵심 기술](#chatgpt의-핵심-기술)
  - [이미지 생성 모델](#이미지-생성-모델)
  - [로보틱스 분야](#로보틱스-분야)
- [강화학습 vs 지도학습](#강화학습-vs-지도학습)
  - [근본적인 차이](#근본적인-차이)
  - [수학적 표현](#수학적-표현)
  - [학습 파이프라인](#학습-파이프라인)
- [다양한 문제 도메인](#다양한-문제-도메인)
- [요약](#요약)

---

## 강의 소개

안녕하세요, 여러분. GCB6206 강화학습 (Reinforcement Learning) 첫 번째 강의에 오신 것을 환영합니다. 저는 이번 학기 여러분과 함께 강화학습의 세계를 탐험할 이영운입니다. 오늘 이 시간에는 강화학습이 무엇인지, 우리가 왜 이 기술에 주목해야 하는지에 대한 큰 그림을 그려보는 시간을 갖도록 하겠습니다.

우리 강좌의 이름은 **GCB6206, 강화학습(Reinforcement Learning)**입니다. 이 수업을 통해 여러분은 강화학습의 핵심 이론부터 최신 딥러닝 기반의 알고리즘 구현까지, 폭넓은 지식과 실전 경험을 쌓게 될 것입니다.

### 오늘 다룰 내용

오늘 우리가 다룰 내용들을 간략하게 살펴보겠습니다.

* **Course overview (강의 개요)**: 이번 학기 전체의 로드맵을 그려볼 겁니다. 어떤 주제들을 어떤 순서로 배우게 될지 확인하면서 큰 그림을 이해하는 시간입니다.
* **Course goals (강의 목표)**: 이 수업을 마치고 났을 때, 여러분이 어떤 역량을 갖추게 될 것인지에 대한 목표를 명확히 할 것입니다.
* **Course logistics (강의 운영 방식)**: 성적 평가 기준, 과제, 시험 등 학기 운영에 대한 구체적인 정보를 안내해 드릴 겁니다.
* **Why study reinforcement learning? (왜 강화학습을 공부해야 하는가?)**: 아마 오늘 가장 중요한 파트가 될 텐데요, 인공지능의 여러 분야 중에서도 왜 지금 우리가 강화학습에 주목해야 하는지에 대한 동기를 부여하는 시간을 갖겠습니다.

이 네 가지 주제를 중심으로 오늘 수업을 진행하겠습니다. 그럼 본격적으로 첫 번째 주제부터 시작해볼까요?

---

## 강화학습의 정의

### 서튼과 바르토의 정의

"강화학습이란 무엇일까요?" 이 질문에 답하기 위해, 우리는 이 분야의 두 거장을 빼놓을 수 없습니다. **앤드류 바르토(Andrew Barto)와 리처드 서튼(Richard Sutton)** 교수입니다. 이들은 강화학습 분야를 개척하고 체계화한 선구자로, 이들이 함께 저술한 "Reinforcement Learning: An Introduction"이라는 책은 오늘날 강화학습을 공부하는 모든 학생과 연구자들에게 '바이블'과도 같은 필독서로 여겨집니다.

그들의 업적을 기리기 위해, 2024년에는 컴퓨터 과학 분야의 노벨상이라 불리는 **ACM 튜링상(Turing Award)**을 수상했습니다. 이는 강화학습이 현대 AI와 컴퓨터 과학에서 얼마나 중요한 위치를 차지하는지를 상징적으로 보여주는 사건이라 할 수 있습니다.

그렇다면 이 두 대가는 강화학습을 어떻게 정의했을까요? 그들의 정의를 함께 읽어보겠습니다.

> **"강화학습이란, 주어진 상황(situations)에서 어떤 행동(actions)을 해야 할지를 학습하여, 숫자 형태의 보상 신호(numerical reward signal)를 최대로 만드는 것이다."**

이 정의에는 강화학습의 핵심 요소가 모두 담겨 있습니다.

* **'학습(learning)'**: 정해진 규칙에 따라 움직이는 것이 아니라, 경험을 통해 스스로 최적의 행동 방식을 찾아가는 과정입니다.
* **'상황을 행동에 매핑(map situations to actions)'**: 특정 상태(state)에서 어떤 행동(action)을 취할지 결정하는 '정책(policy)'을 배우는 것을 의미합니다.
* **'숫자 형태의 보상 신호를 최대로(maximize a numerical reward signal)'**: 강화학습의 유일한 목표입니다. '보상'이라는 명확한 기준을 통해 학습의 방향을 설정하고, 장기적으로 가장 많은 보상을 얻는 것을 목표로 합니다.

### 에이전트와 환경의 상호작용

앞서 본 서튼과 바르토의 정의가 조금 추상적으로 느껴질 수 있습니다. 그래서 이번에는 좀 더 직관적인 그림과 함께 강화학습을 이해해 보겠습니다. 강화학습의 가장 기본적인 상호작용 구조를 살펴보겠습니다.

강화학습에는 두 가지 핵심 주체가 등장합니다. 바로 **에이전트(Agent)**와 **환경(Environment)**입니다.

1. **에이전트 (Agent)**: 학습의 주체이자 의사결정자입니다. 에이전트는 주변 환경을 관찰하고, 그 관찰 결과를 바탕으로 어떤 행동을 취할지 결정합니다.

2. **환경 (Environment)**: 에이전트를 제외한 모든 것을 의미합니다. 에이전트가 상호작용하는 대상이죠.

이 둘의 상호작용은 다음과 같은 순환 구조를 통해 이루어집니다.

* **상태(State)와 보상(Reward)**: 매 순간, 환경은 에이전트에게 자신의 **상태(state)** 정보와 함께 **보상(reward)** 신호를 전달합니다. 상태는 '현재 상황'에 대한 정보이고, 보상은 직전 행동이 얼마나 좋았는지를 알려주는 숫자 값입니다.
* **행동(Action)**: 에이전트는 환경으로부터 받은 상태 정보를 바탕으로 다음에 취할 **행동(action)**을 결정합니다.
* **상호작용의 반복**: 에이전트가 행동을 취하면, 환경의 상태가 변하게 되고, 환경은 그 결과로 새로운 상태와 보상을 다시 에이전트에게 전달합니다. 이 과정이 계속해서 반복됩니다.

이러한 상호작용의 고리 속에서 에이전트는 "어떻게 행동해야 누적 보상(cumulative reward)을 최대로 만들 수 있을까?"를 스스로 학습하게 됩니다. 위키피디아의 정의도 이와 맥락을 같이 합니다: "강화학습(RL)은 지능적인 에이전트가 동적인 환경에서 누적 보상을 최대화하기 위해 어떻게 행동해야 하는지에 대한 것이다."

### 강화학습의 세 가지 관점

지금까지 강화학습의 기본적인 정의와 구조를 살펴봤습니다. 이제 강화학습이라는 용어가 학계와 산업계에서 어떤 의미로 사용되는지를 세 가지 관점에서 정리하겠습니다.

1. **순차적 의사결정 문제 (Sequential Decision Making Problems)**: 강화학습은 근본적으로 '시간의 흐름에 따라 연속적으로 결정을 내려야 하는' 문제들을 다룹니다. 체스를 둘 때 한 수를 두고, 상대의 수를 보고, 다시 다음 수를 두는 것처럼, '관찰하고, 행동하고, 다시 관찰하고, 행동하는' 과정이 반복되는 모든 문제가 여기에 해당합니다.

2. **의사결정 학습을 위한 접근법 (Approach for Learning Decision Making)**: 이러한 순차적 의사결정 문제를 풀기 위한 다양한 방법론들을 통칭하여 강화학습이라고 부르기도 합니다. 모방 학습(Imitation Learning), 모델-프리(Model-free) RL, 모델-기반(Model-based) RL, 온라인(Online) RL, 오프라인(Offline) RL 등 수많은 기법들이 존재하며, 우리는 이 수업에서 이들 중 중요한 몇 가지를 깊이 있게 다룰 것입니다.

3. **연구 분야 (Research Field)**: 마지막으로, 강화학습은 위에서 언급한 순차적 의사결정 문제와 그것을 풀기 위한 학습 방법론을 모두 아우르는 하나의 거대한 연구 분야 그 자체를 의미합니다.

결국 이 세 가지는 모두 연결되어 있습니다. 강화학습이라는 '연구 분야'는 '순차적 의사결정 문제'를 풀기 위한 다양한 '학습 접근법'을 연구하는 학문이라고 종합적으로 이해하시면 되겠습니다.

---

## 강의 범위와 목표

### 학습 범위

강화학습이 굉장히 광범위한 분야라는 것을 이제 아셨을 겁니다. 따라서 한 학기 동안 이 모든 것을 다루는 것은 불가능합니다. 그래서 우리 수업에서는 몇 가지 핵심적인 내용에 집중하고자 합니다.

우리 수업의 초점은 다음과 같습니다.

* **강화학습의 핵심 개념 (Key Concepts in RL)**
* **딥 강화학습 방법론 및 구현 (Deep RL Methods and Implementations)**
* **로보틱스 및 제어 분야의 예시 (Examples in Robotics and Control)**
* **고급 강화학습 주제에 대한 간략한 소개 (Very Brief Introduction to Advanced Topics in RL)**

우리가 다룰 문제들의 예시로는 땅을 기어가는 법을 배우는 에이전트, '벽돌깨기' 게임을 플레이하는 에이전트, 그리고 로봇 팔이 물체를 조작하는 문제가 있습니다. 이처럼 다양한 문제에 적용되는 강화학습의 원리를 배우게 될 것입니다.

### 커리큘럼

구체적인 커리큘럼은 다음과 같습니다.

* **초반부**: 모방 학습(Imitation Learning)과 모델-프리(Model-free) RL의 대표적인 알고리즘들(PPO, DQN, SAC)을 배울 것입니다.
* **중간고사 이후**: 오프라인(Offline) RL, 모델-기반(Model-based) RL, 그리고 역강화학습(Inverse RL)과 같은 조금 더 심화된 주제들을 다루게 됩니다.
* **프로그래밍 과제**: 각 주제에 맞춰 총 6개의 프로그래밍 과제(HW)가 주어질 예정입니다.

### 학습 목표

이 과정을 모두 마치면 여러분은 다음과 같은 역량을 갖추게 될 것입니다.

* 강화학습 분야의 주요 개념과 전문 용어에 익숙해지고
* 최신 딥 강화학습 알고리즘을 파이토치(PyTorch)로 직접 구현할 수 있으며
* 최신 강화학습 연구 동향을 이해하고 따라갈 수 있는 능력을 갖추게 될 것입니다

---

## 강의 운영

### 선수과목

이제 강의의 행정적인 부분들에 대해 안내해 드리겠습니다. 이 부분은 여러분의 학점과 직결되니 잘 들어주시기 바랍니다.

먼저 선수과목(Prerequisites)입니다.

* **머신러닝**과 **딥러닝**에 대한 기본적인 지식이 필요합니다. 미적분, 확률론, 선형대수학은 물론, 역전파(Back-propagation), SGD, CNN, RNN 등에 대한 개념을 숙지하고 있다고 가정하고 수업을 진행할 것입니다.
* 과제는 **파이토치(PyTorch)**를 이용해 딥러닝 모델을 훈련하는 것을 요구합니다.
* 또한, 모든 과제 보고서는 **LaTeX**을 이용해 작성해야 합니다. 템플릿은 제공될 예정입니다.

### 평가 정책

성적 평가 정책(Grading Policy)은 다음과 같습니다.

* **과제(Homework) 30%, 중간고사(Midterm) 30%, 기말고사(Final) 30%, 출석(Attendance) 10%**로 구성됩니다.
* 과제는 총 6번이지만, 그중 **가장 높은 점수를 받은 3개의 과제 성적만 반영**할 것입니다. 부담을 덜어주기 위한 정책이니 잘 활용하시기 바랍니다.
* 중간고사와 기말고사는 **오픈북(Open-book)**으로 진행됩니다.

### 주의사항

몇 가지 중요한 경고(Warnings)입니다.

* **과제는 반드시 미리 시작하세요!** 강화학습 모델 훈련은 생각보다 많은 시간이 걸리고, 수많은 반복과 디버깅을 필요로 합니다.
* 과제에 대한 협업은 가능하지만, 보고서에는 **모든 협업자의 이름을 명시**해야 하며, **솔루션 코드는 반드시 직접 작성**해야 합니다.
* ChatGPT와 같은 생성형 AI의 도움을 받는 것은 좋지만, **너무 맹신하지는 마세요**. 종종 틀린 정보를 제공하기도 합니다.

### 참고자료

강의 정보 및 참고자료(Information & Resources)입니다.

* **LearnUs**를 통해 모든 공지사항과 강의자료가 전달될 것이니, 정기적으로 확인해주세요.
* 과제 수행을 위해 **VESSL AI** 사용을 권장하며, Colab도 좋은 대안입니다.
* 주요 참고문헌으로는 앞서 소개한 **서튼과 바르토의 교재**, 그리고 스탠포드와 UC 버클리의 유명한 강화학습 강의자료들을 추천합니다.

---

## 강화학습의 중요성

### 최신 AI 기술의 발전

지금까지는 우리 수업의 행정적인 부분들에 대해 이야기했습니다. 이제 다시 본론으로 돌아와서, '왜 우리가 강화학습을 공부해야 하는가?'라는 근본적인 질문에 대해 더 깊이 파고들어 보겠습니다.

"우리는 인공 일반 지능(AGI)에 얼마나 가까워졌는가?"라는 도발적인 질문을 던져봅시다.

최근 AI 기술의 발전은 눈부십니다.

* **ChatGPT**는 인간과 자연스러운 대화를 나누고
* **CoPilot**은 코드를 작성해주며
* **Stable Diffusion**은 텍스트 설명만으로 놀라운 이미지를 생성하고
* **Sora**는 동영상을 만들어냅니다

이러한 기술들은 대부분 방대한 데이터를 기반으로 학습하는 **지도 학습(Supervised Learning)**의 성공 사례라고 볼 수 있습니다. 이처럼 지도 학습이 엄청난 성과를 보여주고 있는데, 그렇다면 왜 우리는 굳이 강화학습을 공부해야 할까요?

### 알파고의 의미

그 답에 대한 강력한 힌트를 알파고(AlphaGo)가 보여주고 있습니다.

2016년, 전 세계를 충격에 빠뜨렸던 이세돌 9단과 구글 딥마인드의 **알파고(AlphaGo)**의 대국을 기억하십니까? 많은 분들이 기억하시겠지만, 이 대국에서 알파고는 인간 최고수를 꺾으며 인공지능의 새로운 가능성을 열었습니다.

특히 주목해야 할 것은 바로 '신의 한 수'라고도 불렸던 **알파고의 37번째 수**입니다. 이 수는 당시 경기를 지켜보던 모든 프로 기사들이 전혀 예상하지 못했던, 인간의 기보(기존 데이터)에서는 찾아볼 수 없었던 창의적인 수였습니다.

이것이 바로 강화학습의 힘입니다. 지도 학습이 주어진 데이터(정답)를 '모방'하고 '패턴을 학습'하는 데 강점이 있다면, 강화학습은 **스스로 탐험하고 상호작용하며 기존에 없던 새로운 해법이나 전략을 '발견'**할 수 있는 능력이 있습니다. 알파고의 37번째 수는 강화학습이 인간의 지능을 뛰어넘는 창의성을 발휘할 수 있음을 보여준 상징적인 사건이었습니다.

---

## 실제 응용 사례

지난 시간에 이어서 '왜 우리가 강화학습을 공부해야 하는가?'에 대한 이야기를 계속 나눠보겠습니다. 우리는 알파고의 예를 통해, 강화학습이 인간의 데이터를 넘어서는 창의적인 해답을 찾아낼 수 있다는 점을 확인했습니다. 이번에는 강화학습이 게임을 넘어, 훨씬 더 복잡하고 실용적인 문제들을 어떻게 해결하고 있는지 최신 사례들을 통해 살펴보겠습니다.

### 반도체 설계 혁신: PrefixRL

첫 번째 사례는 바로 반도체 설계 분야입니다. 복잡한 회로도는 '병렬 접두사 회로(Parallel Prefix Circuit)'라는 것의 일부입니다. 이 회로는 반도체 칩 안에서 덧셈과 같은 기본적인 연산을 매우 빠르게 처리하는 데 사용되는 핵심 부품입니다. 전통적으로 이러한 회로는 EDA(Electronic Design Automation), 즉 전자 설계 자동화라는 정교한 소프트웨어 도구를 통해 설계되었습니다.

그런데 2021년 ACM DAC라는 저명한 학회에서 발표된 "PrefixRL"이라는 연구는 이 분야에 큰 파장을 일으켰습니다. 연구팀은 강화학습 에이전트를 학습시켜 이 회로를 설계하도록 했습니다. **PrefixRL**이 설계한 회로는 기존의 **EDA Tool**이 설계한 결과물보다 더 작고, 더 빠르며, 전력 소모도 적었습니다.

결과는 놀라웠습니다. 이는 알파고가 바둑에서 인간의 정석을 넘어선 것과 마찬가지로, 강화학습이 수십 년간 축적된 인간의 공학적 지식과 설계 원칙을 뛰어넘는, 더 효율적인 해결책을 찾아낼 수 있음을 보여준 또 하나의 강력한 증거입니다. 정해진 규칙 없이, '더 나은 성능'이라는 보상만을 가지고 탐색한 결과가 인간 전문가 시스템을 능가한 것입니다.

### ChatGPT의 핵심 기술

이제 아마 여러분이 가장 흥미로워할 주제일 겁니다. 바로 ChatGPT와 같은 거대 언어 모델(LLM)입니다. 놀랍게도, 여러분이 매일 사용하는 ChatGPT의 자연스러운 대화 능력 뒤에는 강화학습이 핵심적인 역할을 하고 있습니다. 훈련 과정은 크게 세 단계로 나뉩니다.

1. **사전학습 (Pretraining)**: 이 단계에서는 인터넷에 있는 수조 개의 단어로 이루어진 방대한 텍스트 데이터를 사용하여 모델을 학습시킵니다. 모델의 목표는 아주 단순합니다. "다음에 올 단어를 예측하라." 이 과정을 통해 모델은 언어의 문법, 구조, 그리고 세상의 방대한 지식을 학습하게 됩니다. 이 단계는 전통적인 지도학습에 가깝습니다.

2. **지도 미세조정 (Supervised Finetuning, SFT)**: 사전학습된 모델은 지식은 많지만, 우리가 원하는 '대화형 비서'처럼 행동하는 법은 모릅니다. 그래서 이 단계에서는 사람이 직접 작성한 고품질의 '질문과 이상적인 답변' 쌍 수만 개를 이용해 모델을 추가로 학습시킵니다. 이를 통해 모델은 사용자의 지시를 따르고, 정중하며, 유용한 방식으로 답변하는 법을 배우게 됩니다.

3. **보상 모델링 및 강화학습 (Reward Modeling & Reinforcement Learning)**: 여기가 바로 강화학습이 등장하는, 가장 중요한 단계입니다.
   * **보상 모델링(Reward Modeling)**: 먼저, SFT 모델에게 동일한 질문에 대해 여러 가지 다른 답변을 생성하도록 합니다. 그리고 사람이 이 답변들을 보고 어떤 것이 더 좋은지 순위를 매깁니다. 이 수십만 개의 인간 선호도 데이터를 학습하여, '어떤 답변이 좋은 답변인지'를 점수로 예측하는 **보상 모델(Reward Model)**을 만듭니다. 이 보상 모델이 바로 강화학습의 '환경'이자 '심판' 역할을 하게 됩니다.
   * **강화학습(Reinforcement Learning)**: 이제 SFT 모델을 '에이전트'로 삼아, 보상 모델로부터 높은 점수를 받는 방향으로 정책을 업데이트합니다. 즉, 에이전트(SFT 모델)가 어떤 답변(action)을 생성하면, 보상 모델(environment)이 그 답변이 얼마나 좋은지 점수(reward)를 줍니다. 에이전트는 이 보상을 최대로 만드는 방향으로 자신의 파라미터를 계속해서 조정해 나갑니다. 이 과정을 **인간 피드백을 통한 강화학습(Reinforcement Learning from Human Feedback, RLHF)**이라고 부르며, ChatGPT가 그저 정보를 나열하는 것을 넘어 인간과 자연스럽고 유용한 대화를 할 수 있게 만드는 핵심 기술입니다.

### 이미지 생성 모델

강화학습의 활약은 언어 모델에만 그치지 않습니다. 이미지 생성 모델인 '확산 모델(Diffusion Model)'에 강화학습을 적용한 사례를 살펴봅시다. "미적 품질(Aesthetic Quality)"이나 "프롬프트와 이미지의 일치도(Prompt-Image Alignment)"와 같은 기준은 수학적인 손실 함수로 정의하기 매우 애매합니다. 하지만 RLHF와 유사하게, 인간의 선호도를 보상으로 사용하여 모델을 학습시키면, RL 훈련을 거칠수록 점점 더 고품질의, 그리고 사용자의 의도에 더 부합하는 이미지를 생성해낼 수 있습니다.

### 로보틱스 분야

이제 물리적인 세계로 넘어가 볼까요? 로보틱스 분야에서 강화학습의 놀라운 성과를 보여줍니다.

* 강화학습을 통해 복잡한 **파쿠르(Parkour)** 동작을 학습한 사족보행 로봇이 있습니다. 기존의 제어 방식으로는 프로그래밍하기 거의 불가능했던 역동적이고 민첩한 움직임을, 시뮬레이션 환경에서의 수많은 시행착오와 보상 학습을 통해 스스로 터득해냈습니다.
* 더 나아가, 강화학습으로 훈련된 드론이 인간 세계 챔피언을 꺾고 **드론 레이싱** 대회에서 우승하는 사례도 있습니다. 시속 100km가 넘는 속도로 장애물을 피하며 최적의 경로를 찾아 비행하는 능력은, 강화학습이 인간의 반응 속도와 제어 능력을 뛰어넘는 '초인적인' 제어 정책을 학습할 수 있음을 증명합니다.

---

## 강화학습 vs 지도학습

자, 지금까지 다양한 예시를 통해 강화학습의 잠재력을 확인했습니다. 그 내용을 다시 한번 요약해보겠습니다.

* 세상에는 **순차적 의사결정 문제**가 가득합니다.
* 무엇이 '정답'인지 모를 때, 강화학습은 스스로 답을 찾아냅니다.
* 때로는 인간을 뛰어넘는 성능을 보여주기도 합니다 (예: 알파고).
* 인간과 상호작용하는 AI 시스템을 만드는 데 필수적입니다 (예: ChatGPT).
* 미분 불가능하거나 복잡한 목표(인간의 피드백, 미적 감각 등)를 최적화할 수 있습니다.

### 근본적인 차이

그렇다면, 이 강력한 강화학습은 우리가 이미 잘 알고 있는 지도학습(Supervised Learning, SL)과 근본적으로 어떻게 다를까요?

자전거 타는 아이의 비유는 이 차이를 아주 직관적으로 설명해 줍니다.

* **지도학습 (Supervised Learning)**은 마치 전문가가 자전거 타는 영상을 수천 시간 보면서 '어떤 상황에서 핸들을 몇 도로 꺾고, 페달을 어떤 힘으로 밟아야 하는지'에 대한 **정답 데이터**를 암기하는 것과 같습니다. 즉, '모범 답안'을 통한 학습입니다.

* **강화학습 (Reinforcement Learning)**은 직접 자전거에 올라타서 부딪혀보는 것과 같습니다. 넘어지면 '아프다'는 부정적인 보상을 받고, 균형을 잡으면 '재미있다'는 긍정적인 보상을 받습니다. 이처럼 명확한 정답 없이, **상호작용과 피드백(보상)**을 통해 스스로 최적의 방법을 터득해 나가는 과정입니다.

### 수학적 표현

이제 이러한 개념을 조금 더 형식적이고 수학적인 언어로 표현해 보겠습니다.

의사결정 문제를 도식화하면:

* 에이전트는 **상태(State) $s$**를 입력으로 받습니다.
* 그리고 **행동(Action) $a$**를 출력합니다.
* 이때, 상태 $s$가 주어졌을 때 행동 $a$를 결정하는 에이전트의 의사결정 메커니즘을 **정책(Policy) $\pi(a|s)$**라고 부릅니다. 정책은 현재 상태에서 각 행동을 할 확률을 나타내는 함수이며, 강화학습의 목표는 결국 '최적의 정책'을 찾는 것입니다.

이 정책을 학습하는 방식에서 지도학습과 강화학습의 차이는 다음과 같습니다.

* **지도학습**: 전문가의 데이터 $\{(s_0, a_0), (s_1, a_1), ...\}$가 주어집니다. 학습 목표는 주어진 상태 $s_i$에서 전문가가 했던 행동 $a_i$가 나올 확률, 즉 $\pi(a_i|s_i)$를 최대화하는 것입니다. 이는 전문가를 그대로 모방하는 학습입니다.

* **강화학습**: 전문가 데이터 대신, 행동의 결과로 주어지는 **보상(Reward) $r$**이 있습니다. 학습 목표는 정책 $\pi$를 따랐을 때 얻게 되는 보상의 총합, 즉 $\sum r_i$를 최대화하는 것입니다. 어떤 행동이 정답인지는 모르지만, 더 많은 보상을 가져다주는 행동을 하도록 정책을 개선해 나갑니다.

### 학습 파이프라인

강화학습 과정이 어떻게 순환적으로 이루어지는지를 보여주는 파이프라인은 다음과 같습니다.

1. 현재 정책 $\pi_\theta$를 사용하여 환경과 **상호작용**하고 데이터를 수집합니다 (이를 '롤아웃'이라고 합니다).
2. 수집된 데이터 $D = \{(s_t, a_t, r_t)\}$를 사용하여, 보상의 합 $\sum r_t$를 최대화하는 더 나은 정책 $\pi_{\theta'}$를 찾습니다. 이것이 바로 **강화학습 알고리즘**이 하는 일입니다.
3. 찾아낸 더 나은 정책으로 기존 정책을 **업데이트**합니다 ($\pi_\theta \leftarrow \pi_{\theta'}$).
4. 이 과정을 계속 반복하면, 정책은 점진적으로 최적의 정책에 가까워집니다.

---

## 다양한 문제 도메인

(상태, 행동, 보상) 프레임워크는 세상의 수많은 문제에 적용될 수 있습니다.

* **동물**: 상태(시각, 후각 등), 행동(근육 움직임), 보상(음식/고통)
* **알파고**: 상태(바둑판), 행동(다음 수), 보상(승리+1/패배-1)
* **리그 오브 레전드**: 상태(게임 화면), 행동(키보드/마우스 입력), 보상(승리, 킬, 타워 파괴 등 복합적인 신호). 이처럼 중간 목표에 보상을 주는 것을 **보상 설계(Reward Shaping)**라고 하며, 복잡한 문제에서 학습을 효율적으로 만드는 중요한 기술입니다.
* **로봇**: 상태(카메라, 관절 센서), 행동(모터 토크), 보상(임무 성공/실패)
* **챗봇**: 상태(대화 기록), 행동(다음 응답 텍스트), 보상(인간의 선호도)

이러한 문제들은 우리 실생활 곳곳에 있습니다. 가정용 로봇, 유튜브 추천 시스템, 자율주행차, 헬스케어, 가상 비서, 게임 AI 등, 순차적인 의사결정이 필요한 모든 곳에 강화학습이 적용될 잠재력이 있습니다.

다음 시간에는 지도학습과 강화학습의 중간 다리 역할을 하는 **모방 학습(Imitation Learning)**에 대해 배워보겠습니다. 전문가의 데이터를 일단 모방하는 것에서부터 시작하여 점차 완전한 강화학습으로 나아가는 여정의 첫걸음이 될 것입니다.

---

## 요약

### 학습 내용 정리

오늘 우리는 강화학습이 무엇인지, 왜 중요한지, 그리고 어떤 원리로 동작하는지에 대한 큰 그림을 그려보았습니다. 핵심 내용을 정리하면 다음과 같습니다.

1. **강화학습의 정의**:
   - 서튼과 바르토: "주어진 상황에서 어떤 행동을 해야 할지를 학습하여, 숫자 형태의 보상 신호를 최대로 만드는 것"
   - 에이전트(Agent)와 환경(Environment)의 상호작용을 통한 학습
   - 상태(State), 행동(Action), 보상(Reward)의 순환 구조

2. **강화학습의 세 가지 관점**:
   - 순차적 의사결정 문제 (Sequential Decision Making Problems)
   - 의사결정 학습을 위한 접근법 (Learning Approaches)
   - 연구 분야 (Research Field)

3. **강의 범위 및 목표**:
   - 핵심 개념, 딥 RL 방법론, 로보틱스 예시
   - PPO, DQN, SAC, Offline RL, Model-based RL, Inverse RL
   - PyTorch 구현 능력 및 최신 연구 동향 이해

4. **강화학습의 중요성**:
   - 정답이 없는 문제 해결 (알파고의 37번째 수)
   - 인간을 뛰어넘는 창의성
   - 실제 응용: 반도체 설계(PrefixRL), ChatGPT(RLHF), 이미지 생성, 로보틱스

5. **강화학습 vs 지도학습**:
   - 지도학습: 전문가 데이터 모방, 정책 $\pi$가 전문가 행동 $a_i$ 예측
   - 강화학습: 상호작용을 통한 학습, 누적 보상 $\sum r_t$ 최대화
   - 강화학습 파이프라인: 상호작용 → 데이터 수집 → 정책 업데이트 → 반복

6. **다양한 응용 도메인**:
   - 게임 AI, 로보틱스, 추천 시스템, 자율주행, 헬스케어
   - 순차적 의사결정이 필요한 모든 문제에 적용 가능

### 핵심 개념

* **강화학습 (Reinforcement Learning, RL)**: 정답이 없는 세상에서, 시행착오와 보상을 통해 스스로 최적의 전략을 찾아가는 학습 패러다임
* **에이전트 (Agent)**: 학습 및 의사결정의 주체
* **환경 (Environment)**: 에이전트가 상호작용하는 대상
* **상태 (State)**: 현재 상황에 대한 정보
* **행동 (Action)**: 에이전트가 취하는 결정
* **보상 (Reward)**: 행동의 좋고 나쁨을 알려주는 신호
* **정책 (Policy)**: 상태를 행동으로 매핑하는 함수 $\pi(a|s)$
* **누적 보상 (Cumulative Reward)**: 장기적으로 얻게 되는 총 보상 $\sum r_t$

### 다음 강의 예고

다음 시간에는 **모방 학습(Imitation Learning)**에 대해 배우겠습니다. 전문가의 행동을 모방하는 것에서 시작하여, 점차 강화학습의 세계로 나아가는 첫걸음을 내딛게 될 것입니다.

오늘 수업은 여기까지입니다. 수고 많으셨습니다.
