# Lecture 06: Q-Learning과 Deep Q-Network (DQN)

## 목차

- [복습: REINFORCE와 PPO](#복습-reinforce와-ppo)
  - [REINFORCE의 한계: On-Policy 데이터](#reinforce의-한계-on-policy-데이터)
  - [Off-Policy를 위한 중요도 샘플링](#off-policy를-위한-중요도-샘플링)
  - [PPO (Proximal Policy Optimization)](#ppo-proximal-policy-optimization)
- [Q-Learning으로의 전환](#q-learning으로의-전환)
  - [정책 경사 알고리즘 요약](#정책-경사-알고리즘-요약)
  - [새로운 접근: 가치 반복](#새로운-접근-가치-반복)
  - [가치 함수 정의](#가치-함수-정의)
  - [Fitted Value Iteration의 문제점](#fitted-value-iteration의-문제점)
  - [벨만 방정식과 Q-러닝](#벨만-방정식과-q-러닝)
  - [Fitted Q-Iteration 알고리즘](#fitted-q-iteration-알고리즘)
  - [알고리즘 비교 요약](#알고리즘-비교-요약)
  - [Q-러닝의 장단점](#q-러닝의-장단점)
- [Deep Q-Network (DQN)](#deep-q-network-dqn)
  - [Q-러닝의 3가지 문제점](#q-러닝의-3가지-문제점)
  - [문제 1: Correlated Samples](#문제-1-correlated-samples)
  - [해결책 1: Replay Buffer](#해결책-1-replay-buffer)
  - [문제 2: Moving Targets](#문제-2-moving-targets)
  - [해결책 2: Target Network](#해결책-2-target-network)
  - [문제 3: Exploration](#문제-3-exploration)
  - [DQN의 추가 기법들](#dqn의-추가-기법들)
  - [DQN의 성과](#dqn의-성과)
- [DQN 개선 기법](#dqn-개선-기법)
  - [Double DQN (DDQN)](#double-dqn-ddqn)
  - [N-step Returns](#n-step-returns)
  - [Prioritized Experience Replay (PER)](#prioritized-experience-replay-per)
  - [Distributional RL](#distributional-rl)
  - [Rainbow: 모든 개선안의 집합체](#rainbow-모든-개선안의-집합체)
- [요약](#요약)

---

## 복습: REINFORCE와 PPO

### REINFORCE의 한계: On-Policy 데이터

정책 경사 (Policy Gradient)의 가장 기본이 되는 알고리즘은 **REINFORCE**입니다.

목표는 $J(\theta)$, 즉 정책 $\pi_{\theta}$를 따랐을 때 얻는 총 보상 $r$의 기댓값을 최대화하는 것입니다.

$$J(\theta)=\mathbb{E}_{\tau\sim\pi_{\theta}(\tau)}\sum_{t}r(s_{t},a_{t})$$

이를 위해 $\theta$에 대해 미분, 즉 정책 경사 (gradient)를 구해야 합니다.

$$\nabla_{\theta}J(\theta)=\mathbb{E}_{\tau\sim\pi_{\theta}(\tau)}[(\sum_{t=1}^{T}\nabla_{\theta}\log~\pi_{\theta}(a_{t}|s_{t}))(\sum_{t=1}^{T}r(s_{t},a_{t}))]$$

여기서 아주 중요한 점은, 이 기댓값 $\mathbb{E}_{\tau\sim\pi_{\theta}(\tau)}$이 **현재 정책 $\pi_{\theta}$에서 샘플링된 궤적 (trajectory) $\tau$**를 사용한다는 것입니다. 즉, 경사도를 계산하려면 *반드시* 현재 정책 $\pi_{\theta}$로 데이터를 수집해야 합니다.

이것이 바로 **On-Policy (온-폴리시, 정책 내)** 방식의 핵심입니다. 정책을 한 번 업데이트, 즉 $\theta' \leftarrow \theta + \alpha\nabla_{\theta}J(\theta)$ 이렇게 $\theta$가 $\theta'$로 바뀌고 나면, 이전에 $\pi_{\theta}$로 수집했던 모든 데이터 $\tau \sim \pi_{\theta}(\tau)$는 더 이상 $\pi_{\theta'}$에 대한 경사도를 계산하는 데 쓸모가 없어집니다. 모두 버리고 $\pi_{\theta'}$로 처음부터 다시 데이터를 샘플링해야 하죠.

이는 데이터 효율성 (sample efficiency) 면에서 매우 비효율적입니다.

### Off-Policy를 위한 중요도 샘플링

이 비효율성을 극복하기 위해 "Off-Policy (오프-폴리시, 정책 외)" 방식을 도입하려 했고, 그때 사용된 통계적 기법이 바로 **중요도 샘플링 (Importance Sampling)**입니다.

기본 원리는 간단합니다. $p(x)$ 분포의 기댓값을 $q(x)$ 분포에서 뽑은 샘플로 계산하고 싶다면, $f(x)$에 $\frac{p(x)}{q(x)}$라는 비율 (ratio)을 곱해서 보정해주면 됩니다.

$$\mathbb{E}_{x\sim p(x)}[f(x)]=\mathbb{E}_{x\sim q(x)}[\frac{p(x)}{q(x)}f(x)]$$

이를 우리 목표 $J(\theta)$에 적용하면, **새 정책 $\pi_{\theta}$**의 기댓값을 **이전 정책 $\pi_{\theta_{old}}$**에서 뽑은 데이터로 계산할 수 있게 됩니다.

$$J(\theta)=E_{\tau\sim\pi_{\theta_{old}}(\tau)}[\frac{\pi_{\theta}(\tau)}{\pi_{\theta_{old}}(\tau)}r(\tau)]$$

이 식을 $\theta$로 미분하면, '로그-도함수 트릭' ($\nabla_{\theta}p_{\theta}(x)=p_{\theta}(x)\nabla_{\theta}\log~p_{\theta}(x)$)을 이용해 최종적으로 이런 형태의 경사도를 얻게 됩니다.

$$\nabla_{\theta}J(\theta) = E_{\tau\sim\pi_{\theta_{old}}(\tau)}[\frac{\pi_{\theta}(\tau)}{\pi_{\theta_{old}}(\tau)}\nabla_{\theta}\log~\pi_{\theta}(\tau)r(\tau)]$$

여기서 $\frac{\pi_{\theta}(\tau)}{\pi_{\theta_{old}}(\tau)}$는 궤적 전체의 비율이라 불안정하니, 실제로는 각 타임스텝 $t$에서의 비율의 곱으로 나타내고, 종종 $\pi_{\theta}$와 $\pi_{\theta_{old}}$가 매우 가깝다는 가정 하에 근사 (approximation)를 사용하기도 합니다.

이 중요도 샘플링을 Actor-Critic 구조에 적용하면 다음과 같은 알고리즘이 나옵니다.

1. 먼저, 현재 정책 $\pi_{\theta}$로 $N$개의 샘플 $\{s_i, a_i, r_i\}$를 수집합니다.
2. 수집한 데이터로 Critic을 업데이트 (예: $V_{\phi}^{\pi_{\theta}}(s)$)하고, Advantage $A_{\phi}^{\pi_{\theta}}(s, a)$를 계산합니다.
3. 현재 정책 $\theta$를 $\theta_{old}$로 저장해둡니다.
4. 이제 $K$번의 내부 루프를 돕니다.
5. $N$개의 샘플을 *재사용*하면서, 중요도 샘플링 비율 $\frac{\pi_{\theta^{(k)}}(a_i|s_i)}{\pi_{\theta_{old}}(a_i|s_i)}$을 곱한 정책 경사도를 계산하여 $\theta$를 $K$번 업데이트합니다.

즉, 데이터를 한 번 뽑아서 $K$번 '재활용'하여 정책을 업데이트하는 것입니다. 데이터 효율성이 높아졌죠.

### PPO (Proximal Policy Optimization)

하지만 중요도 샘플링 비율 $\frac{\pi_{\theta}}{\pi_{\theta_{old}}}$이 너무 커지거나 작아지면 업데이트가 매우 불안정해지는 문제가 있습니다.

이 문제를 아주 영리하게 해결한 알고리즘이 바로 **PPO (Proximal Policy Optimization)**입니다. PPO는 Actor-Critic 알고리즘의 일종으로, 다음과 같은 핵심 기법들을 사용합니다.

- 분산을 줄이기 위해 Advantage 함수 $A$를 사용합니다. (보통 GAE라는 더 정교한 추정치를 씁니다.)
- 중요도 샘플링을 사용해 데이터를 재활용하며 여러 번의 경사도 스텝을 밟습니다.
- **가장 중요한 것:** 정책이 $\pi_{\theta_{old}}$로부터 너무 멀리 벗어나지 않도록, 목적 함수 자체를 **클리핑 (Clipping)**하여 제약을 겁니다.

#### PPO: 클리핑 메커니즘

PPO의 핵심인 '클리핑'이 어떻게 동작하는지 봅시다.

먼저, 정책 비율을 $ratio_t(\theta) = \frac{\pi_{\theta}(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}$라고 정의합니다. 이 비율이 1보다 크면 새 정책이 그 행동을 할 확률이 높아진 것이고, 1보다 작으면 낮아진 것입니다.

PPO는 두 가지 상황을 나눠서 생각합니다.

1. **$A > 0$ (행동이 예상보다 좋았을 때):**
   - 우리는 이 행동의 확률 $\pi_{\theta}(a_t|s_t)$을 높이고 싶습니다. 즉, $ratio_t(\theta)$를 키우고 싶습니다.
   - 하지만 $ratio_t(\theta)$가 $1+\epsilon$ (예: 1.2)보다 더 커지면, PPO는 이 값을 $1+\epsilon$으로 '잘라버립니다 (clip)'.
   - 즉, $J(\theta)$의 증가폭을 $\min(ratio_t(\theta)A, (1+\epsilon)A)$로 제한합니다. 정책이 한 번에 너무 과하게 좋아지려는 것을 막는 것입니다.

2. **$A < 0$ (행동이 예상보다 나빴을 때):**
   - 우리는 이 행동의 확률 $\pi_{\theta}(a_t|s_t)$을 낮추고 싶습니다. 즉, $ratio_t(\theta)$를 줄이고 싶습니다.
   - 하지만 $ratio_t(\theta)$가 $1-\epsilon$ (예: 0.8)보다 더 작아지면, PPO는 이 값을 $1-\epsilon$으로 '잘라버립니다'.
   - 즉, $J(\theta)$의 감소폭 (실제로는 $A$가 음수이므로 증가폭)을 $\max(ratio_t(\theta)A, (1-\epsilon)A)$로 제한합니다. ($A$가 음수이므로 max가 맞습니다. $\min(ratio_t(\theta)|A|, (1-\epsilon)|A|)$와 같습니다.) 정책이 한 번에 너무 과하게 나빠지는 것도 막는 것입니다.

#### PPO: 목적 함수와 알고리즘

이 두 가지 경우를 하나의 식으로 합친 것이 바로 PPO의 'Clipped Objective Function'입니다.

$$J(\theta) \approx \sum_{t=1}^{T} \min( ratio_t(\theta) A_t, \text{clip}(ratio_t(\theta), 1-\epsilon, 1+\epsilon) A_t )$$

$\text{clip}(ratio_t(\theta), 1-\epsilon, 1+\epsilon)$는 $ratio$를 $1-\epsilon$와 $1+\epsilon$ 사이로 강제하는 함수입니다. $A_t$가 양수일 때는 $\min(ratio \cdot A, (1+\epsilon)A)$가 되고, $A_t$가 음수일 때는 $\min(ratio \cdot A, (1-\epsilon)A)$가 되어 (음수이므로 실제로는 max), 정확히 우리가 원했던 동작을 수행합니다.

그래서 PPO 알고리즘은 다음과 같이 동작합니다.

1. 정책 $\pi_{\theta}$로 $N$개의 샘플을 수집합니다.
2. Advantage $A_{GAE}$를 계산합니다.
3. $\theta_{old} \leftarrow \theta$로 정책을 고정합니다.
4. $K$번의 내부 루프 (epoch) 동안,
5. 미니배치를 뽑아 위에서 정의한 'Clipped Objective' $J(\theta^{(k)})$ (그리고 탐험을 위한 엔트로피 보너스 $H$)를 이용해 경사 상승법으로 $\theta$를 업데이트합니다.
6. $K$번의 업데이트가 끝나면 $\theta \leftarrow \theta^{(k+1)}$로 최종 업데이트하고, 다시 1번으로 돌아가 새 데이터를 수집합니다.

#### PPO가 좋은 이유

PPO가 왜 이렇게 널리 쓰일까요?

- **구현이 단순합니다.** 복잡한 신뢰 영역 (Trust Region) 계산 (TRPO)이 필요 없습니다.
- **이산 행동 (discrete)과 연속 행동 (continuous) 공간 모두에서 잘 작동합니다.**
- 데이터 수집과 학습이 병렬화되기 쉬워 **확장성 (scalable)**이 좋습니다.

물론 단점도 있습니다. 하이퍼파라미터, 예를 들어 롤아웃 길이 ($N$), 배치 크기, 엔트로피 계수 등에 꽤 **민감하게 반응**하는 편입니다.

---

## Q-Learning으로의 전환

### 정책 경사 알고리즘 요약

여기까지가 PPO를 포함한 **정책 경사 (Policy Gradient)** 계열 알고리즘들의 복습이었습니다.

이 계열의 흐름을 다시 한 번 정리해봅시다.

1. **정책 실행 (Roll-out):** 현재 정책 $\pi_{\theta}$로 환경과 상호작용하며 궤적 (데이터)을 수집합니다.
2. **반환값 추정 / 모델 피팅:** 수집한 데이터로 $V_{\phi}^{\pi_{\theta}}$나 $Q_{\phi}^{\pi_{\theta}}$ 같은 가치 함수 (Critic)를 학습시킵니다.
3. **정책 개선 (Improve policy):** 이 가치 함수 (주로 Advantage $A_{\phi}^{\pi_{\theta}}$)를 이용해 정책 경사도 $\nabla_{\theta}J(\theta)$를 계산하고, 정책 $\pi_{\theta}$를 직접 업데이트합니다.

핵심은, 이 알고리즘들은 "어떤 행동이 현재 정책보다 얼마나 더 좋은가?"를 기준으로 **정책 $\pi_{\theta}$ 자체를 직접 학습**한다는 것입니다.

### 새로운 접근: 가치 반복

이제 완전히 다른 접근법을 생각해 봅시다.

방금 본 것이 '정책 반복 (Policy Iteration)'의 흐름입니다. 정책 $\pi$를 평가 (Value $V$ or $Q$)하고, 경사도를 이용해 $\pi$를 개선 ($\theta' \leftarrow \theta + ...$)합니다.

그런데 만약, 우리가 어떤 정책 $\pi$에 대한 **완벽한 상태-행동 가치 함수 $Q^{\pi}(s, a)$를 알고 있다면** 어떨까요? $Q^{\pi}(s, a)$는 상태 $s$에서 행동 $a$를 했을 때 앞으로 받을 총 보상의 기댓값입니다.

그렇다면 우리는 어떤 상태 $s$에서든, 그냥 $Q^{\pi}(s, a)$를 가장 크게 만드는 행동 $a$를 선택하면 되지 않을까요?

$$\pi'(s) = \arg\max_{a} Q^{\pi}(s, a)$$

이 $\pi'$는 $\pi$보다 항상 같거나 더 좋은 정책이 됨이 보장됩니다. (Policy Improvement Theorem)

이 아이디어에서 '가치 반복 (Value Iteration)' 패러다임이 나옵니다.

- 정책 $\pi$를 직접 학습하는 대신, **최적의 가치 함수 $Q$를 학습하는 데 집중합니다.**
- 최적의 $Q$ 함수 ($Q^*$)를 일단 찾기만 하면, 최적의 정책 $\pi^*$는 $Q^*$에서 $\arg\max$를 취함으로써 **자동으로 (implicitly)** 결정됩니다.

이것이 바로 Q-러닝의 핵심 철학입니다.

### 가치 함수 정의

이해를 돕기 위해 가치 함수들을 다시 정의해봅시다.

- **상태 가치 $V^{\pi}(s_t)$**: 상태 $s_t$에서 시작해서 정책 $\pi$를 따랐을 때의 총 보상 기댓값입니다.
- **상태-행동 가치 $Q^{\pi}(s_t, a_t)$**: 상태 $s_t$에서 행동 $a_t$를 *먼저* 하고, 그 이후부터 정책 $\pi$를 따랐을 때의 총 보상 기댓값입니다.

만약 우리가 최적의 정책 (greedy policy) $\pi(s) = \arg\max_a Q(s, a)$를 따른다면, 상태 $s_t$의 가치 $V(s_t)$는 그 상태에서 할 수 있는 *가장 좋은 행동의 Q-가치*와 같아집니다.

$$V(s_t) = \max_{a_t} Q^{\pi}(s_t, a_t)$$

### Fitted Value Iteration의 문제점

그럼 $V$ 함수를 직접 학습하는 'Fitted Value Iteration' 알고리즘을 생각해 봅시다. 데이터를 모으고, $V_{\phi}(s)$를 학습시킵니다. 이때 $V_{\phi}(s)$의 타겟값은 무엇일까요? 바로 '벨만 최적 방정식 (Bellman Optimality Equation)'입니다.

$$V(s) = \max_a (r(s,a) + \mathbb{E}_{s' \sim p(s'|s,a)}[V(s')])$$

이 식을 보세요. 타겟을 계산하려면 $r(s,a)$ (보상 함수)뿐만 아니라, **$p(s'|s,a)$ (상태 전이 확률, 즉 동역학)**를 알아야만 기댓값 $\mathbb{E}_{s'}$을 계산할 수 있습니다.

하지만 아타리 게임이나 로봇 제어에서 우리가 $p(s'|s,a)$를 어떻게 알 수 있을까요? **이는 환경의 모델 (model)을 알아야 함을 의미**하며, 대부분의 복잡한 문제에서는 불가능합니다. 이것이 $V$ 대신 $Q$를 배우려는 핵심 이유입니다.

### 벨만 방정식과 Q-러닝

다시 Q-함수로 돌아와 봅시다. $Q^{\pi}(s_t, a_t)$는 "현재 보상 $r_t$ + 다음 상태 $s_{t+1}$의 가치 $V^{\pi}(s_{t+1})$의 기댓값"으로 분해할 수 있습니다.

$$Q^{\pi}(s_t, a_t) = \mathbb{E}_{s_{t+1}\sim p(\cdot|s_t, a_t)}[r_t + V^{\pi}(s_{t+1})]$$

그런데 아까 $V^{\pi}(s_{t+1}) = \max_{a_{t+1}} Q^{\pi}(s_{t+1}, a_{t+1})$라고 했죠? 이걸 위 식에 대입하면, $Q$에 대한 재귀적인 식, 즉 **벨만 방정식**을 얻습니다.

$$Q^{\pi}(s_t, a_t) = \mathbb{E}_{s_{t+1}\sim p(\cdot|s_t, a_t)}[r_t + \max_{a_{t+1}} Q^{\pi}(s_{t+1}, a_{t+1})]$$

이 식이 왜 중요할까요? $V$를 학습할 때와 달리, 이 식은 $Q$를 업데이트하기 위해 $p(s'|s,a)$를 알 필요가 없습니다. 기댓값 $\mathbb{E}_{s_{t+1}}$은 그냥 환경에서 **샘플링**을 통해 얻은 *다음 상태 $s'$* 하나로 근사 (approximate)할 수 있기 때문입니다.

### Fitted Q-Iteration 알고리즘

드디어 **Q-러닝** 알고리즘입니다.

1. **데이터 수집:** 환경과 상호작용하며 $(s_i, a_i, r_i, s_i')$ 형태의 트랜지션 (transition)을 수집합니다.
   - **중요:** 이때 *어떤 정책*을 쓰든 상관없습니다. 탐험을 위해 무작위 행동을 섞어 써도 됩니다. 왜냐하면 우리는 최적의 $Q^*$를 배우는 것이지, 데이터를 수집하는 정책을 배우는 것이 아니기 때문입니다. 이것이 Q-러닝이 **Off-Policy (오프-폴리시)**인 이유입니다.

2. **타겟 설정:** 수집한 샘플 $(s_i, a_i, r_i, s_i')$에 대해 '타겟 Q-가치' $y_i$를 계산합니다.
   - 벨만 방정식을 샘플링 버전으로 바꾼 것입니다.
   - $y_i = r_i + \max_{a_i'} Q_{\phi}(s_i', a_i')$
   - 즉, "실제로 받은 보상 $r_i$" + "다음 상태 $s_i'$에서 할 수 있는 최선의 행동 ($\max_{a_i'}$)에 대한 현재 Q-함수의 예측치 $Q_{\phi}(s_i', a_i')$" 입니다.

3. **Q-함수 피팅:** 이제 이걸 지도학습 문제처럼 풉니다.
   - **입력:** $s_i, a_i$
   - **타겟:** $y_i$
   - **손실 함수:** $L(\phi) = \sum_i (Q_{\phi}(s_i, a_i) - y_i)^2$ (Mean Squared Error)
   - 이 손실을 최소화하도록 $\phi$ (예: 뉴럴넷의 가중치)를 업데이트합니다.

Q-러닝에서는 정책 $\pi$를 위한 파라미터 $\theta$가 따로 없습니다. 정책 $\pi$는 그저 현재 $Q_{\phi}$ 함수에 대해 $\arg\max$를 취하는 행동으로 **암묵적으로 (implicitly) 정의**될 뿐입니다.

가장 큰 장점은? 정책 경사 (PG) 방식에서 우리를 괴롭혔던 **분산 (variance)이 큰 경사도 계산이 없습니다!**

### 알고리즘 비교 요약

지금까지 배운 세 가지 접근법을 요약해봅시다.

1. **정책 반복 (Policy Iteration, 예: PPO):** Advantage $A^{\pi}$를 평가하고, 정책 $\pi$를 *직접* 업데이트합니다.
2. **Fitted Value Iteration:** $V_{\phi}$를 업데이트합니다. 하지만 타겟 $y_i$를 계산하기 위해 환경의 동역학 ($p(s'|s,a)$)이 필요합니다.
3. **Fitted Q-Iteration (Q-Learning):** $Q_{\phi}$를 업데이트합니다. 타겟 $y_i$를 $r_i + \max_{a'} Q_{\phi}(s_i', a')$로 계산하므로, **동역학 (모델)이 필요 없습니다 (Model-free).**

Q-러닝은 강화학습 분야의 거대한 돌파구였습니다. 저명한 교과서인 Sutton & Barto의 "Reinforcement Learning: An Introduction" (2018)에 따르면, Q-러닝은 **'오프-폴리시 시간차 제어 (Off-policy TD Control)'** 알고리즘의 대표적인 예입니다. (Chapter 6.5)

- **'오프-폴리시'**란, 우리가 배우려는 최적의 정책 (greedy policy)과 상관없이, 탐험을 포함하는 다른 정책 (예: $\epsilon$-greedy)이 생성한 데이터를 가지고 학습할 수 있다는 의미입니다.
- **'시간차 (Temporal-Difference, TD)'**란, 타겟 $y_i$를 계산할 때 전체 궤적의 보상을 기다리는 것이 아니라, 한 스텝 후의 보상 $r_i$와 다음 상태의 가치 추정치 ($\max Q_{\phi}(s_i', a_i')$)를 이용해 업데이트한다는 의미입니다.
- **'제어 (Control)'**란, 단순히 정책을 '평가 (evaluation)'하는 것을 넘어 '최적의 정책을 찾는 것'을 의미합니다.

### Q-러닝의 장단점

Q-러닝은 정말 강력하지만, 완벽하지는 않습니다.

**장점 (Pros):**
- 정책 경사 (PG) 방식의 **높은 분산 (high-variance)** 문제를 피할 수 있습니다.
- **오프-폴리시 (Off-policy)**이므로, 이전에 수집한 모든 데이터를 '경험 리플레이 버퍼 (Replay Buffer)'에 쌓아두고 재사용할 수 있습니다. 이는 **샘플 효율성 (sample efficient)**을 극단적으로 높여줍니다.
- 학습해야 할 함수가 $Q$ 함수 하나뿐입니다. (정책 함수가 따로 필요 없음)

**단점 (Cons):**
- $Q$ 함수를 (선형 함수가 아닌) 비선형 함수, 즉 딥러닝 (Deep Neural Network)으로 근사 (approximation)할 경우, **수렴성 (convergence)이 보장되지 않습니다.**
- 왜 불안정할까요? **'부트스트래핑 (Bootstrapping)'** 때문입니다. 우리가 학습하려는 타겟 $y_i = r + \max_{a'} Q_{\phi}(s', a')$이, 우리가 지금 업데이트하고 있는 $Q_{\phi}$에 의존합니다. 즉, $Q_{\phi}$가 업데이트되면 타겟 $y_i$도 따라서 변합니다. 이는 마치 **'움직이는 과녁 (Moving Target)'**을 맞추려는 것과 같아서 학습이 매우 불안정해질 수 있습니다.
- **연속적인 행동 공간 (continuous action space)**에서는 사용하기 거의 불가능합니다. $\max_a Q_{\phi}(s, a)$를 계산하려면 모든 $a$에 대해 $Q$값을 비교해야 하는데, $a$가 연속적이면 (무한대) 이를 계산할 수 없기 때문입니다.

---

## Deep Q-Network (DQN)

### Q-러닝의 3가지 문제점

이 Q-러닝을 (선형 함수가 아닌) 딥 뉴럴 네트워크와 결합하여 **Deep Q-Network (DQN)**를 만들려고 할 때, 학습이 매우 불안정해지는 심각한 문제들이 발생합니다.

딥러닝과 Q-러닝을 단순 결합할 때 발생하는 3가지 핵심 문제는 다음과 같습니다.

1. **Correlated samples (샘플 간의 상관관계):** 강화학습 데이터는 시간 순서대로 수집되므로, 연속된 샘플들 ($s_t$와 $s_{t+1}$)이 매우 강한 상관관계를 가집니다. 이는 딥러닝 학습의 기본 가정인 'i.i.d. (독립 항등 분포)' 가정을 위배하여 학습을 불안정하게 만듭니다.

2. **Moving targets (움직이는 타겟):** Q-러닝은 '타겟' ($y_i$)을 계산할 때 현재 'Q-네트워크' ($Q_{\phi}$) 자신을 사용합니다. 즉, $Q_{\phi}$가 업데이트되면 타겟 $y_i$도 따라서 변해버립니다. 이는 마치 움직이는 과녁을 맞추려는 것과 같아 학습이 수렴하기 어렵습니다.

3. **Exploration (탐험):** 최적의 Q-가치를 학습하려면 환경을 충분히 탐험하여 다양한 $(s, a)$ 쌍에 대한 정보를 모아야 합니다.

DQN은 이 3가지 문제에 대해 각각 **Replay buffer (리플레이 버퍼)**, **Target network (타겟 네트워크)**, 그리고 **Epsilon greedy (엡실론 그리디)**라는 영리한 해결책을 제시했습니다.

이제 이 문제들과 해결책을 하나씩 자세히 뜯어보겠습니다.

### 문제 1: Correlated Samples

'Online Q-learning'이란, 에이전트가 환경으로부터 $(s_i, a_i, s_i', r_i)$라는 경험 (transition)을 *하나* 얻을 때마다, *즉시* $Q_{\phi}$ 네트워크를 업데이트 (학습)하는 방식입니다.

딥러닝 모델 (예: 이미지 분류기)을 학습시킬 때를 떠올려보세요. 우리는 전체 데이터셋에서 미니배치를 '무작위로 (randomly)' 샘플링하여 학습합니다. 이는 데이터가 **i.i.d. (independent and identically distributed)**, 즉 '독립적이고 동일한 분포에서 추출되었다'는 것을 전제로 합니다.

하지만 강화학습의 궤적 (trajectory)을 생각해 봅시다. 상태 $s_t$와 $s_{t+1}$이 과연 독립적일까요? 전혀 아니죠. $s_{t+1}$은 $s_t$와 $a_t$에 의해 거의 결정됩니다. 즉, 연속된 샘플들은 매우 **강한 상관관계 (strong correlations)**를 가집니다.

이런 상관관계가 높은 데이터를 순서대로 학습에 사용하면, 네트워크는 방금 경험한 특정 궤적이나 상태 (예: 게임의 특정 구역)에 과적합 (overfitting)되기 쉽습니다. 이는 학습을 편향시키고 불안정하게 만듭니다.

### 해결책 1: Replay Buffer

이 '샘플 상관관계' 문제를 해결하기 위한 아이디어가 바로 **'리플레이 버퍼 (Replay Buffer $\mathcal{B}$)'**입니다. (이는 사실 1993년 Lin의 연구에서 '경험 리플레이 (Experience Replay)'라는 이름으로 처음 제안되었습니다. DQN은 이를 딥러닝 스케일에 성공적으로 적용한 것입니다. - *Sutton & Barto, "Reinforcement Learning: An Introduction", 2018, Section 16.6*)

알고리즘은 다음과 같습니다.

1. 에이전트가 $(s_i, a_i, s_i', r_i)$ 경험을 얻으면, 이것으로 바로 학습하지 않고, 일단 '버퍼 $\mathcal{B}$'라는 거대한 저장소에 **저장**합니다.
2. 학습 (업데이트)을 할 때는, 이 버퍼 $\mathcal{B}$에 쌓여있는 *수많은 과거의 경험들* 중에서 **무작위로 (randomly) 미니배치를 샘플링**합니다.
3. 이 무작위로 추출된 미니배치를 이용해 $Q_{\phi}$를 업데이트합니다.

이 기법의 장점은 명확합니다.

- **상관관계 해소:** 버퍼에서 무작위로 샘플링하므로, 연속된 샘플들이 하나의 배치에 포함될 확률이 매우 낮아집니다. 즉, i.i.d. 가정을 만족시켜 학습이 안정화됩니다.
- **데이터 효율성:** 한 번 경험한 데이터를 버리지 않고, 여러 번의 학습 (epoch)에 **재사용 (reuse)**할 수 있습니다. 이는 데이터 수집 비용이 비싼 강화학습에서 샘플 효율성을 극도로 높여줍니다.
- **배치 학습:** 딥러닝의 표준인 '배치 학습'이 가능해져, 그래디언트의 분산이 줄어들고 안정적인 업데이트가 가능해집니다.

### 문제 2: Moving Targets

첫 번째 문제를 리플레이 버퍼로 해결했습니다. 이제 두 번째 문제인 **'Moving targets (움직이는 타겟)'**을 살펴보겠습니다.

이 상황을 비유하자면, 아이들 (Q-네트워크 파라미터 $\phi$)이 잔디밭 위의 토끼 (타겟 $y_i$)를 잡으려고 하는데, 아이들이 토끼를 향해 달려가면 (학습), 토끼도 그만큼 도망가버리는 (타겟 변경) 상황입니다.

왜 이런 일이 발생할까요? Q-러닝의 손실 함수 (Loss function)와 타겟 ($y_i$) 정의를 다시 봅시다. 우리는 Q-네트워크의 예측값 $Q_{\phi}(s_i, a_i)$를 타겟 $y_i$에 근접하도록 학습시킵니다.

$$L(\phi) = \mathbb{E}_{(s,a,r,s') \sim \mathcal{B}} [ (Q_{\phi}(s, a) - y_i)^2 ]$$

그런데 이 타겟 $y_i$가 어떻게 계산되죠?

$$y_i = r_i + \gamma \max_{a'} Q_{\phi}(s_i', a')$$

문제는 바로 이 $\max_{a'} Q_{\phi}(s_i', a')$ 항입니다. 타겟 $y_i$를 계산하는 데, **우리가 지금 막 업데이트하려는 파라미터 $\phi$가 그대로 사용됩니다!**

경사 하강법을 통해 $\phi$를 $\phi_{new}$로 한 스텝 업데이트했다고 합시다. 그러면 우리가 다음 스텝에서 맞춰야 할 타겟 $y_i$ 역시 $r_i + \gamma \max_{a'} Q_{\phi_{new}}(s_i', a')$로 **같이 변해버립니다.**

이는 지도학습에서 정답 (label)이 매 스텝마다 바뀌는 것과 같습니다. 당연히 학습이 불안정하고 수렴하기 어렵습니다.

### 해결책 2: Target Network

DQN은 이 '움직이는 과녁' 문제를 해결하기 위해 아주 영리한 두 번째 기법, **'타겟 네트워크 (Target Network)'**를 도입합니다.

아이디어는 간단합니다. Q-값을 계산하는 네트워크를 두 개로 복제하는 것입니다.

1. **메인 네트워크 (Main Network):** 파라미터 $\phi$를 가지며, 리플레이 버퍼에서 샘플링된 배치로 *매 스텝* 학습 (업데이트)됩니다.
2. **타겟 네트워크 (Target Network):** 파라미터 $\phi'$를 가지며, 오직 '타겟 $y_i$'를 계산하는 데만 사용됩니다.

알고리즘은 다음과 같이 동작합니다.

1. 학습 시작 시, 타겟 네트워크 파라미터 $\phi'$를 메인 네트워크 파라미터 $\phi$와 동일하게 초기화합니다: $\phi' \leftarrow \phi$.
2. 이후 일정 주기 (예: $L=10,000$ 스텝) 동안, 메인 네트워크 $\phi$는 계속 학습되지만, 타겟 네트워크 $\phi'$는 **업데이트하지 않고 파라미터를 고정 (frozen)**시킵니다.
3. 데이터를 수집하고 ($N$ 스텝), 버퍼에서 샘플링하여 ($K$ 스텝) $Q_{\phi}$를 업데이트할 때, 타겟 $y_i$는 **고정된 $\phi'$**를 사용해 계산합니다:

$$y_i \leftarrow r_i + \gamma \max_{a'} Q_{\phi'}(s_i', a')$$

4. $L$ 스텝이 지나면, 다시 $\phi' \leftarrow \phi$로 타겟 네트워크를 메인 네트워크의 최신 버전으로 동기화합니다.

이렇게 하면, $L$ 스텝의 내부 루프 동안 $\phi$가 아무리 변해도, 타겟 $y_i$는 고정된 $\phi'$에 의해 계산되므로 **움직이지 않습니다.** 이제 토끼 ($\phi'$)가 멈춰 섰으니, 아이들 ($\phi$)이 제대로 토끼를 향해 달려갈 수 있게 된 것이죠.

### 문제 3: Exploration

마지막 세 번째 문제, '탐험 (Exploration)'입니다.

Q-러닝의 목적은 최적의 $Q^*$를 찾는 것입니다. 만약 우리가 항상 현재 $Q_{\phi}$가 가장 높다고 예측하는 행동 ($a_t = \arg\max_a Q_{\phi}(s_t, a)$)만 선택한다면 (이를 **활용 (exploitation)**이라고 합니다), 우연히 초반에 $Q$값이 낮게 추정된, 하지만 실제로는 더 좋은 보상을 줄 수도 있는 다른 행동들을 시도해 볼 기회 (이를 **탐험 (exploration)**이라고 합니다)를 영원히 잃게 됩니다.

이 '탐험-활용 트레이드오프 (Exploration-Exploitation Trade-off)'를 다루는 가장 고전적이고 간단한 방법이 **'Epsilon-greedy ($\epsilon$-greedy)'**입니다.

- $\epsilon$ (엡실론)이라는 작은 확률값 (예: 0.1)을 정합니다.
- **$\epsilon$의 확률로 (With probability $\epsilon$):** $Q$값과 상관없이 **무작위 행동 (random action)**을 선택합니다 (탐험).
- **$1-\epsilon$의 확률로 (Otherwise):** 현재 $Q_{\phi}$가 가장 높다고 말하는 행동 $a_t = \arg\max_a Q_{\phi}(s_t, a)$를 선택합니다 (활용).

일반적으로 학습 초반에는 $\epsilon$을 1.0 (100% 무작위 탐험)으로 높게 설정하여 환경에 대한 다양한 정보를 수집하고, 학습이 진행됨에 따라 $\epsilon$ 값을 0.1이나 0.01 같은 작은 값으로 서서히 줄여나갑니다 (annealing 또는 scheduling).

### DQN의 추가 기법들

지금까지 설명한 3가지 핵심 요소 (리플레이 버퍼, 타겟 네트워크, $\epsilon$-greedy)가 DQN의 이론적 기반입니다. 이 외에, 2015년 네이처 논문에서 아타리 (Atari) 게임이라는 고차원 시각적 환경에 DQN을 적용하기 위해 사용한 두 가지 중요한 전처리 기법이 있습니다.

1. **Frame Skip (프레임 스킵):** 아타리 게임은 초당 60프레임 (FPS)으로 동작합니다. 1/60초마다 에이전트가 행동을 결정하는 것은 너무 비효율적이고, 사실 그럴 필요도 없습니다. 따라서 4프레임마다 한 번씩만 $\epsilon$-greedy로 행동을 선택하고, 그 선택한 행동 (예: '오른쪽으로 이동')을 4프레임 동안 유지합니다.

2. **Frame Stack (프레임 스택):** 아타리 게임의 스크린샷 *한 장* (하나의 프레임)만 봐서는 공이 어느 방향으로 움직이는지, 적이 다가오는 속도가 얼마인지 알 수 없습니다. 즉, 정적인 이미지 하나는 완전한 '상태 (state)'가 아닙니다. 이 문제를 해결하기 위해, 현재 프레임과 직전 3개의 프레임을 합쳐, **총 4개의 연속된 프레임을 묶어서 (stacking)** 하나의 '상태'로 간주하여 Q-네트워크 (CNN)의 입력으로 사용합니다. 이렇게 하면 네트워크가 이미지의 동적인 정보 (속도, 가속도 등)를 파악할 수 있게 됩니다.

### DQN의 성과

DQN은 아타리 57개 게임 (원 논문은 49개)에서 다음과 같은 성과를 거두었습니다:

- 이전의 선형 함수 기반 강화학습보다 훨씬 뛰어난 성능을 보였습니다.
- 전문 인간 테스터의 평균 점수를 기준으로, '비디오 핀볼', '복싱', '브레이크아웃 (벽돌깨기)' 등 많은 게임에서 인간을 아득히 초월했습니다.
- '퐁', '스페이스 인베이더' 등 다수의 게임에서 인간 수준을 달성하거나 근접했습니다.
- 물론 '몬테주마의 복수'처럼 장기적인 탐험과 계획이 필요한 게임에서는 0점을 받으며 한계를 보이기도 했습니다.

---

## DQN 개선 기법

DQN은 '엔드-투-엔드 (end-to-end)' 강화학습의 시작을 알린 기념비적인 알고리즘입니다. 하지만 DQN 역시 완벽하지 않았습니다.

DQN의 단점들을 보완하기 위해 이후에 제안된 주요 개선 기법들은 다음과 같습니다:

1. **Q-가치 과대추정 (overestimation) 방지:** DQN은 Q-값을 실제보다 높게 추정하는 경향이 있습니다. → **Double DQN**
2. **더 나은 Q-타겟 추정:** 1-스텝 타겟 ($r + \gamma \max Q'$)은 편향 (bias)이 큽니다. → **n-step Q-value**
3. **유용한 데이터로 학습:** 리플레이 버퍼에서 균일하게 (uniformly) 샘플링하는 것은 비효율적입니다. → **Prioritized Experience Replay (PER)**
4. **Q-가치의 분포 학습:** Q-가치를 '기댓값 (하나의 숫자)'이 아닌 '보상의 분포' 자체로 학습합니다. → **Distributional RL**

### Double DQN (DDQN)

#### 문제: Q-가치 과대추정

"DQN이 예측하는 Q-가치가 정확할까요?"

DQN 원 논문의 그래프를 보면, 학습이 진행됨에 따라 에이전트가 받는 실제 '평균 보상'은 일정 수준에서 수렴하는데, 네트워크가 예측하는 '평균 Q-가치'는 계속해서 치솟는 것을 볼 수 있습니다. 이는 DQN이 Q-가치를 **심각하게 과대추정 (overestimate)**하고 있음을 보여줍니다.

Double DQN 논문의 그래프를 보면 더 명확합니다. 네 개의 게임 모두에서, DQN estimate가 DQN true value (실제 보상)보다 **훨씬** 위에 있는 것을 볼 수 있습니다. 반면 Double DQN estimate는 true value에 거의 근접해 있죠.

#### 원인: $\max$ 연산자

왜 이런 과대추정이 발생할까요? **범인은 바로 $\max$ 연산자입니다.**

DQN의 타겟 식을 다시 봅시다: $y_i = r_i + \gamma \max_{a'} Q_{\phi'}(s_i', a')$

이 $\max_{a'}$ 항이 문제입니다. $Q_{\phi'}(s_i', a')$는 '참값'이 아니라 노이즈 (추정 오차)가 낀 '추정치'입니다. 만약 여러 행동 $a_1, a_2, ...$ 에 대한 추정치 $X_1, X_2, ...$ 가 있다고 할 때, $\max$ 연산자는 이 추정치들 중 가장 큰 값을 선택합니다.

통계적으로, 여러 개의 랜덤 변수들 중 최댓값의 기댓값은, 기댓값들의 최댓값보다 항상 크거나 같습니다 (젠센의 부등식).

$$\mathbb{E}[\max(X_1, X_2, ...)] \ge \max(\mathbb{E}[X_1], \mathbb{E}[X_2], ...)$$

쉽게 말해, $Q_{\phi'}$가 여러 행동 중 하나에 대해 *실수로* (긍정적인 오차로 인해) Q-값을 높게 추정했다면, $\max$ 연산자는 **항상 그 긍정적인 오차를 선택하게 된다**는 것입니다. 이 작은 오차들이 매 업데이트마다 누적되면서 Q-가치가 계속 부풀려지는 것입니다.

이 문제가 발생하는 근본적인 이유는 DQN이 $\max$를 취할 때, 행동을 **선택 (selection)**하는 것과 그 행동의 가치를 **평가 (evaluation)**하는 것을 *동일한* 타겟 네트워크 $Q_{\phi'}$로 수행하기 때문입니다.

#### 해결책: Double DQN (DDQN)

**Double DQN (DDQN)**은 이 '선택'과 '평가'를 분리함으로써 문제를 해결합니다.

우리는 이미 두 개의 네트워크, 즉 메인 네트워크 ($\phi$)와 타겟 네트워크 ($\phi'$)를 가지고 있습니다. DDQN은 이 둘의 역할을 나눕니다.

1. **행동 선택 (Selection):** 다음 상태 $s'$에서 *어떤 행동이* 가장 좋은지는, 현재 학습 중인 **메인 네트워크 ($\phi$)**를 이용해 결정합니다.

$$a^* = \arg\max_{a'} Q_{\phi}(s', a')$$

2. **행동 평가 (Evaluation):** 그렇게 선택된 행동 $a^*$의 가치가 *얼마인지*는, **타겟 네트워크 ($\phi'$)**를 이용해 평가합니다.

$$\text{Value} = Q_{\phi'}(s', a^*)$$

이 둘을 합친 것이 Double DQN의 타겟 $y$입니다:

$$y = r + \gamma Q_{\phi'}(s', \arg\max_{a'} Q_{\phi}(s', a'))$$

DQN의 타겟과 비교해보세요. $\arg\max$ 안에는 $\phi$를 쓰고, 바깥쪽 $Q$에는 $\phi'$를 씁니다. 이렇게 두 네트워크의 역할을 분리하면, 메인 네트워크가 실수로 어떤 행동 $a'$를 과대추정하더라도, 타겟 네트워크 $\phi'$는 그 행동 $a'$에 대해 독립적인 (다른 오차를 가진) Q-값을 반환하므로, 과대추정이 상쇄되는 효과가 있습니다. 구현도 매우 간단하면서 효과는 아주 강력합니다.

### N-step Returns

DQN 개선의 두 번째 주제는 **'n-step Q-value'**입니다.

지금까지 우리가 사용한 Q-러닝 타겟 $y_t = r_t + \gamma \max_{a_{t+1}} Q_{\phi'}(s_{t+1}, a_{t+1})$은 **1-스텝 (1-step) TD 타겟**입니다. 즉, 딱 한 스텝의 실제 보상 $r_t$만 사용하고, 그 이후의 모든 미래 보상은 $Q_{\phi'}$라는 추정치로 대체 (bootstrapping)합니다.

- 1-스텝 TD의 장점: $Q_{\phi'}$가 비교적 안정된 추정치이므로 **분산 (variance)이 낮습니다.**
- 1-스텝 TD의 단점: (특히 학습 초반에) $Q_{\phi'}$가 부정확한 추정치일 수 있으므로 **편향 (bias)이 높습니다.**

이 반대쪽 극단에는 **몬테카를로 (MC) 타겟**이 있습니다.

MC 타겟: $\hat{Q}_{MC} = \sum_{t'=t}^{T} r_{t'}$ (에피소드가 끝날 때까지 받은 모든 실제 보상의 합)

- MC의 장점: 100% 실제 보상이므로 **편향이 없습니다 (No bias).**
- MC의 단점: 한 번의 궤적 (샘플)에 너무 의존하므로 **분산이 매우 높습니다.**

**N-step return**은 이 1-step TD (낮은 분산, 높은 편향)와 MC (높은 분산, 낮은 편향) 사이의 **절충안 (trade-off)**입니다.

아이디어는 $N$ 스텝 (예: N=3) 동안의 **실제 보상**은 그대로 사용하고, $N$ 스텝 이후의 가치만 $Q_{\phi'}$로 추정하는 것입니다.

$$y_t = \sum_{t'=t}^{t+N-1} \gamma^{t'-t} r_{t'} + \gamma^N \max_{a_{t+N}} Q_{\phi'}(s_{t+N}, a_{t+N})$$

$N=1$이면 1-step TD (Q-러닝)가 되고, $N \to \infty$ (에피소드 끝)이면 MC가 됩니다. $N$은 이 둘 사이의 편향-분산 트레이드오프를 조절하는 하이퍼파라미터입니다.

#### N-step return의 장단점

**장점 (Pros):**
- 1-step TD보다 $N$ 스텝만큼의 실제 보상 신호를 더 많이 반영하므로, **편향이 더 적습니다.**
- 실제 보상 신호가 네트워크에 더 빠르고 깊게 전파되므로, **학습 속도가 (특히 초반에) 더 빠른 경향**이 있습니다.

**단점 (Cons) 및 고찰:**
- 이 타겟 $y_t$를 계산하려면, 리플레이 버퍼에 $(s_t, a_t, r_t, ..., s_{t+N})$까지 $N$ 스텝의 궤적을 저장해야 합니다.
- **이론적 문제:** 1-step TD ($y_t = r_t + ...$)는 $s_t, a_t$만 있으면 그 이후는 $\max$로 처리하므로 완벽한 오프-폴리시 (off-policy)입니다. 하지만 N-step 타겟은 $a_t$부터 $a_{t+N-1}$까지의 행동이 필요합니다. 이 행동들은 *현재* 우리가 학습하려는 정책 ($\epsilon$-greedy)이 아닌, *과거의* 정책에서 나온 것일 수 있습니다. 따라서 N-step 타겟은 엄밀히 말해 **온-폴리시 (on-policy)** 타겟입니다.
- **실제:** 하지만, 이러한 이론적인 부정확성에도 불구하고, N-step 타겟을 (오프-폴리시인) DQN 리플레이 버퍼와 함께 **그냥 사용해도 실제로는 성능이 매우 잘 나옵니다!** 그래서 DQN의 성능을 높이는 표준적인 기법으로 널리 사용됩니다.

### Prioritized Experience Replay (PER)

DQN의 핵심 혁신 중 하나는 '경험 리플레이 버퍼 (Experience Replay Buffer)'였습니다. 이를 통해 샘플 간의 상관관계를 끊어내고 i.i.d. 가정을 만족시켰죠.

하지만 기본적인 DQN의 리플레이 버퍼는 **균일 샘플링 (uniform sampling)**을 사용합니다. 즉, 버퍼에 있는 100만 개의 경험을 모두 '동일한 확률'로 뽑습니다.

#### 균일 샘플링의 비효율성

이게 왜 비효율적일까요? 여러분이 시험공부를 한다고 생각해 봅시다. 리플레이 버퍼는 '문제집'이고, 학습은 '문제 풀기'입니다.

- **TD-error가 낮은 경험 ($||Q(s, a) - y||^2 \approx 0$):** 네트워크가 이미 정답을 잘 맞히는, "쉬운 문제"입니다.
- **TD-error가 높은 경험 ($||Q(s, a) - y||^2 \gg 0$):** 네트워크가 예측에 실패한, "어렵고 배울 게 많은 문제"입니다.

기존의 균일 샘플링은 이 '쉬운 문제'와 '어려운 문제'를 같은 빈도로 뽑아서 푸는 것입니다. 당연히 비효율적이죠. 우리는 '어려운 문제', 즉 **TD-error가 높은 경험**을 더 자주 뽑아서 학습 (replay)하고 싶습니다.

#### PER의 핵심 아이디어

이것이 바로 **Prioritized Experience Replay (PER)**의 핵심입니다.

- **Solution:** TD-error가 높은 트랜지션 (transition)을 더 높은 확률로 샘플링합니다.
- **어떻게?** 트랜지션을 버퍼에 저장할 때, 그 트랜지션의 TD-error 크기 (절댓값) $|\delta_i| = |Q(s_i, a_i) - y_i|$도 함께 저장합니다.
- 샘플링할 때는 이 $|\delta_i|$ 값에 비례하여 샘플링 확률 $P(i)$를 할당합니다.
  - $p_i = |\delta_i| + \epsilon$ ($\epsilon$은 0이 되는 것을 방지하는 작은 값)
  - $P(i) = \frac{p_i^\alpha}{\sum_k p_k^\alpha}$
  - 여기서 $\alpha$는 '우선순위화 (prioritization)'의 정도를 결정하는 하이퍼파라미터입니다. $\alpha=0$이면 균일 샘플링이 되고, $\alpha=1$이면 TD-error 크기에 정비례하여 샘플링합니다.

#### PER의 함정과 해결책

하지만 여기에는 두 가지 문제가 있습니다.

1. **과적합 (Overfitting):** TD-error가 극단적으로 높은 소수의 샘플들 (예: 버그나 노이즈)만 반복적으로 샘플링되면, 네트워크가 이 이상치 (outlier)에 과적합될 위험이 있습니다.
   - **Sutton & Barto 교과서 (2018)의 Section 8.4**에서는 '순위 기반 우선순위화 (rank-based prioritization)'를 대안으로 제시합니다. 이는 TD-error의 절대적 크기 대신, 'TD-error가 몇 등인지' 그 순위 (rank)를 기반으로 확률을 계산합니다. $P(i) \propto (1/\text{rank}(i))^\alpha$. 이는 이상치에 덜 민감하여 (robust) 더 안정적입니다.

2. **분포 편향 (Bias):** 우리가 의도적으로 샘플링 확률을 바꿨기 때문에, 이제 샘플링된 배치는 더 이상 원래의 데이터 분포를 따르지 않게 됩니다. 즉, **편향 (bias)**이 발생합니다.
   - 이 편향을 보정하기 위해, PER은 **'중요도 샘플링 가중치 (Importance-Sampling, IS weights)'**를 사용합니다.
   - $w_i = (\frac{1}{N} \cdot \frac{1}{P(i)})^\beta$
   - $N$은 버퍼 크기, $P(i)$는 이 샘플이 뽑힐 확률, $\beta$는 이 보정을 얼마나 강하게 적용할지 결정하는 하이퍼파라미터입니다.
   - 이 가중치 $w_i$를 손실 함수 (loss)에 곱해줍니다: $\text{Loss} = \sum_i w_i \cdot \delta_i^2$
   - 직관적으로, 자주 뽑히는 (P(i)가 높은) 샘플은 가중치 ($w_i$)를 낮게 주고, 드물게 뽑히는 샘플은 가중치를 높게 주어 편향을 상쇄하는 것입니다.

이런 추가적인 복잡성에도 불구하고, PER은 매우 효과적입니다. TD-error가 큰 샘플들을 집중적으로 학습하므로 **손실 (loss)이 훨씬 빠르게 줄어들고 (수렴 속도 향상)**, 전반적인 성능이 크게 향상됩니다.

### Distributional RL

이제 DQN 개선의 마지막 주제인 **Distributional DQN**입니다.

지금까지 우리가 다룬 모든 Q-러닝 계열은 $Q(s, a)$라는 **하나의 스칼라 값**을 학습했습니다.

$$Q(s, a) = \mathbb{E}[Z(s, a)]$$

여기서 $Q(s, a)$는 $(s, a)$에서 시작했을 때 미래에 받을 총 보상 (return)의 **'기댓값 (Expectation)'**입니다.

하지만 '기댓값'은 정보의 손실이 큽니다.

- **상황 A:** 행동 $a_1$은 50% 확률로 +20점, 50% 확률로 0점. → $\mathbb{E}[Z] = 10$
- **상황 B:** 행동 $a_2$는 100% 확률로 +10점. → $\mathbb{E}[Z] = 10$

표준 DQN에게 $a_1$과 $a_2$는 $Q=10$으로 동일한 가치를 가집니다. 하지만 여러분이라면 어떨까요? 안정적인 +10점을 원하나요 (위험 회피적), 아니면 +20점의 대박을 노리나요 (위험 추구적)?

**Distributional RL**의 아이디어는 이 '기댓값' 대신, 총 보상의 **'확률 분포 (Probability Distribution)' 자체**를 학습하자는 것입니다.

- $Z(s, a) \rightarrow \{ (p_1, z_1), (p_2, z_2), ..., (p_k, z_k) \}$
- 즉, $z_1$의 보상을 받을 확률이 $p_1$, $z_2$의 보상을 받을 확률이 $p_2$ ... 라는 '분포'를 통째로 학습합니다.

이 분포를 알면 기댓값은 물론, 분산 (위험도), 최댓값, 최솟값 등 훨씬 풍부한 정보를 알 수 있습니다.

이 아이디어를 구현한 대표적인 알고리즘이 "C51" (Bellemare et al., 2017)입니다.

1. **출력 (Output):** C51 네트워크는 Q-가치 (스칼라) 대신, 정해진 범위 (예: -10점 ~ +10점)를 51개의 '아톰 (atom)'으로 쪼개고, 각 아톰에 대한 확률 값을 출력합니다. 즉, Q-가치의 '히스토그램'을 예측합니다.
2. **타겟 (Target):** 1-스텝 타겟 분포 $R + \gamma Z(s', a')$ (이것도 분포겠죠)를 계산합니다.
3. **손실 (Loss):** 예측한 분포와 타겟 분포 사이의 차이, 즉 **'분포 간의 거리'** (보통 Cross-Entropy Loss)를 최소화하도록 네트워크를 학습시킵니다.

이 방식은 $Q$의 '평균'만 이동시키는 것이 아니라, 분포의 '모양' 자체를 학습시키기 때문에 훨씬 더 안정적이고 풍부한 신호를 제공하며, 실제 성능 향상도 매우 큽니다.

### Rainbow: 모든 개선안의 집합체

지금까지 우리는 DQN을 개선하기 위한 수많은 기법들을 배웠습니다:

- **Double DQN (DDQN)**
- **Prioritized Replay (PER)**
- **N-step returns**
- **Distributional DQN**
- (그 외 Dueling DDQN, Noisy DQN 등도 있습니다.)

2018년 Hessel 등이 발표한 **"Rainbow"**는 "이 좋은 기법들을 모두 합치면 어떻게 될까?"라는 질문에서 시작했습니다.

결과는 아타리 57개 게임의 '인간 점수 대비 중앙값 (Median human-normalized score)'으로 측정되었습니다:

- 맨 아래 오리지널 DQN
- DDQN, Prioritized 등이 그 위로 성능을 개선
- **Distributional DQN**이 그 자체로 엄청난 성능 향상
- 이 모든 것을 합친 **Rainbow**는 다른 모든 알고리즘을 압도하며, 인간 중앙값의 2배가 넘는 (200% 이상) 엄청난 성능을 달성했습니다.

이는 이 개선 기법들이 서로 다른 문제를 해결하며, 상호 보완적으로 (orthogonal) 잘 작동한다는 것을 보여준 중요한 결과입니다.

---

## 요약

### 학습 내용 정리

#### 1. 핵심 개념

**정책 경사 방법론 (Policy Gradient):**
- REINFORCE: On-policy 방식, 데이터 효율성 낮음
- 중요도 샘플링 (Importance Sampling): Off-policy 학습 가능, 하지만 비율이 불안정
- PPO: Clipping으로 정책 업데이트 제한, 안정적 학습

**가치 기반 방법론 (Value-based):**
- Q-Learning: 최적 Q-함수 학습 → 정책은 암묵적 도출
- Model-free: 환경 동역학 $p(s'|s,a)$ 불필요
- Off-policy: 임의의 정책으로 수집한 데이터 활용 가능

#### 2. 주요 알고리즘

**Q-Learning:**
- 벨만 방정식 (Bellman Equation):

$$Q(s_t, a_t) = \mathbb{E}[r_t + \max_{a_{t+1}} Q(s_{t+1}, a_{t+1})]$$

- 타겟: $y = r + \gamma \max_{a'} Q_{\phi'}(s', a')$
- 장점: Off-policy, 분산 낮음
- 단점: 딥러닝과 결합 시 불안정, 연속 행동 공간 처리 불가

**Deep Q-Network (DQN):**
- 리플레이 버퍼 (Replay Buffer): 샘플 상관관계 제거, i.i.d. 가정 만족
- 타겟 네트워크 (Target Network): Moving target 문제 해결, 학습 안정화
- $\epsilon$-greedy: 탐험-활용 균형

**DQN 개선 기법:**
- Double DQN: Q-가치 과대추정 방지 ($\phi$로 선택, $\phi'$로 평가)
- N-step Returns: 편향-분산 트레이드오프 조절
- PER: TD-error 큰 샘플 우선 학습
- Distributional RL: Q-가치 분포 자체 학습
- Rainbow: 모든 기법 통합, 최고 성능

#### 3. 중요 포인트


**정책 경사 vs Q-Learning 비교:**

| 항목 | 정책 경사 (PG) | Q-Learning |
|---|---|---|
| 학습 대상 | 정책 $\pi_\theta$ 직접 | Q-함수 $Q_\phi$ |
| 정책 도출 | 명시적 (Explicit) | 암묵적 (Implicit) |
| 행동 공간 | 이산/연속 모두 가능 | 이산만 가능 |
| 데이터 효율성 | 낮음 (On-policy) | 높음 (Off-policy) |
| 분산 | 높음 | 낮음 |
| 수렴 안정성 | 상대적 안정 | 불안정 (딥러닝 시) |

**DQN의 3대 핵심 기법:**
1. Replay Buffer → 샘플 독립성 확보
2. Target Network → 타겟 안정화
3. $\epsilon$-greedy → 충분한 탐험

**학습 성과:**
- 아타리 57개 게임에서 인간 수준 달성/초월
- Rainbow: 인간 중앙값의 200% 이상 성능

**주요 통찰:**
- Q-러닝은 환경 모델 없이도 최적 정책을 찾을 수 있는 강력한 방법론
- 딥러닝과의 결합 시 발생하는 불안정성은 영리한 공학적 기법으로 해결 가능
- 여러 개선 기법들은 상호 보완적으로 작동하여 시너지 효과 발생
- Q-러닝의 근본적 한계 (연속 행동 공간)는 Actor-Critic 방법론과의 결합으로 해결 가능 (다음 강의 주제)
