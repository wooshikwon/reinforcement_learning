# Lecture 05: PPO (Proximal Policy Optimization)

## 목차

- [강의 소개](#강의-소개)
- [Policy Gradient 복습](#policy-gradient-복습)
  - [정책 경사법의 기본 개념](#정책-경사법의-기본-개념)
  - [분산 문제와 해결 방법](#분산-문제와-해결-방법)
  - [분산 감소 기법 1: 인과성과 Reward-to-go](#분산-감소-기법-1-인과성과-reward-to-go)
  - [분산 감소 기법 2: 기저선](#분산-감소-기법-2-기저선)
  - [어드밴티지 함수](#어드밴티지-함수)
- [Actor-Critic 알고리즘](#actor-critic-알고리즘)
  - [REINFORCE 알고리즘](#reinforce-알고리즘)
  - [Actor-Critic 기본 개념](#actor-critic-기본-개념)
  - [샘플 효율성 비교](#샘플-효율성-비교)
  - [Actor-Critic의 한계](#actor-critic의-한계)
- [Actor-Critic 개선 방법](#actor-critic-개선-방법)
  - [N-step Returns](#n-step-returns)
  - [일반화된 어드밴티지 추정 (GAE)](#일반화된-어드밴티지-추정-gae)
  - [중요도 샘플링](#중요도-샘플링)
  - [중요도 샘플링의 문제점](#중요도-샘플링의-문제점)
- [PPO 알고리즘](#ppo-알고리즘)
  - [PPO의 핵심: Clipped Objective Function](#ppo의-핵심-clipped-objective-function)
  - [PPO 알고리즘 전체 흐름](#ppo-알고리즘-전체-흐름)
  - [PPO의 장단점](#ppo의-장단점)
  - [PPO 응용 사례](#ppo-응용-사례)
- [요약](#요약)

---

## 강의 소개

안녕하세요, 여러분. GCB6206 강화학습 7주차 수업을 시작하겠습니다. 저는 이 강의를 맡은 이영운입니다.

오늘 수업에서는 강화학습에서 매우 중요한 알고리즘 중 하나인 **PPO (Proximal Policy Optimization)**에 대해 배우게 될 텐데요. 그 근간이 되는 **액터-크리틱 (Actor-Critic)** 알고리즘부터 차근차근 밟아나가 보겠습니다.

### 강의 참고 자료

수업을 시작하기에 앞서, 우리 강의의 많은 슬라이드와 과제들은 스탠포드 대학의 CS224r, UC 버클리 대학의 CS285, 그리고 구글 딥마인드의 강의 자료들을 참고하여 만들어졌다는 점을 말씀드립니다. 특히 오늘 다룰 **정책 경사법 (Policy Gradient)**에 대해서는 Daniel Seita의 블로그 포스트가 좋은 참고 자료가 될 수 있으니 관심 있는 학생들은 한번 읽어보는 것을 추천합니다.

### 오늘 수업의 흐름

오늘 우리는 다음과 같은 순서로 수업을 진행할 겁니다.

1. **Recap**: 먼저 지난 시간에 배웠던 정책 경사법의 핵심 개념과 한계점을 다시 한번 짚어볼 것입니다.
2. **Actor-Critic Algorithms**: 그리고 그 한계를 극복하기 위해 등장한 액터-크리틱 알고리즘에 대해 배울 것입니다.
3. **Improving Actor-Critic**: 더 나아가 액터-크리틱 알고리즘의 성능을 어떻게 개선할 수 있는지, N-step return과 중요도 샘플링이라는 두 가지 주요 기법을 살펴볼 것입니다.
4. **PPO**: 마지막으로 이 모든 아이디어를 집대성한 Proximal Policy Optimization 알고리즘에 대해 배우게 됩니다.

자, 그럼 정책 경사법 복습부터 시작해볼까요?

---

## Policy Gradient 복습

### 정책 경사법의 기본 개념

강화학습의 궁극적인 목표가 무엇이었죠? 바로 **보상의 총합을 최대화하는 최적의 정책 $\pi_{\theta}(a_t|s_t)$를 찾는 것**이었습니다. 여기서 $\theta$는 정책을 결정하는 신경망의 파라미터를 의미합니다.

우리가 최대화하고자 하는 목적 함수 $J(\theta)$는 특정 정책 $\pi_{\theta}$를 따랐을 때 얻게 되는 궤적 (trajectory, $\tau$)에 대한 보상 총합의 기댓값으로 정의됩니다. 수식으로는 다음과 같이 표현할 수 있죠.

$$
J(\theta) = \mathbb{E}_{\tau \sim \pi_{\theta}(\tau)}[\sum_t r(\tau)]
$$

이 목적 함수를 $\theta$에 대해 미분하여 경사 상승법 (gradient ascent)을 통해 $J(\theta)$를 점진적으로 증가시키는 것이 바로 정책 경사법의 핵심 아이디어입니다. 이 목적 함수를 미분하면 다음과 같은 **정책 경사 정리 (Policy Gradient Theorem)**를 얻을 수 있습니다.

$$
\nabla_{\theta}J(\theta) = \mathbb{E}_{\tau \sim \pi_{\theta}(\tau)} [(\sum_{t=1}^{T} \nabla_{\theta} \log \pi_{\theta}(a_t|s_t)) (\sum_t r(s_t, a_t))]
$$

이 식의 의미를 한번 해석해볼까요? $\log \pi_{\theta}(a_t|s_t)$는 상태 $s_t$에서 행동 $a_t$를 할 로그 확률을 의미합니다. 여기에 미분 연산자 $\nabla_{\theta}$가 붙었으니, '어떤 방향으로 파라미터 $\theta$를 조정해야 상태 $s_t$에서 행동 $a_t$를 할 확률이 높아지는가'를 나타내는 벡터가 됩니다. 그리고 이 벡터에 그 궤적에서 받은 총 보상 $r(\tau)$을 곱해주는 것이죠. 즉, **총 보상이 컸던 궤적에서 수행했던 행동들의 확률은 높이고, 총 보상이 작았던 궤적의 행동들의 확률은 낮추는 방향**으로 정책을 업데이트하게 됩니다.

실제로는 기댓값을 정확히 계산할 수 없으므로, $N$개의 궤적을 샘플링하여 평균을 내는 방식으로 근사합니다.

$$
\nabla_{\theta}J(\theta) \approx \frac{1}{N} \sum_{i=1}^{N} (\sum_{t=1}^{T} \nabla_{\theta} \log \pi_{\theta}(a_t^i|s_t^i)) r(\tau^i)
$$

이 방법은 목적 함수를 직접 최적화한다는 큰 장점이 있지만, 치명적인 단점이 하나 있습니다. 바로 **분산 (variance)이 매우 높다**는 점이죠.

---

### 분산 문제와 해결 방법

#### 왜 분산이 중요한가?

분산이 높다는 것이 왜 문제일까요?

여기서 우리가 진짜 알고 싶은 값은 기댓값 $\mathbb{E}[X]$입니다. 하지만 우리는 이 값을 직접 알 수 없고, 샘플링을 통해 얻은 표본 평균 $\overline{X}$으로 추정해야 합니다.

- **High Variance 상황**: 샘플들이 매우 넓게 퍼져 있습니다. 이 경우, 우리가 우연히 특정 영역의 샘플들만 추출하게 되면, 표본 평균 $\overline{X}$는 실제 기댓값 $\mathbb{E}[X]$와는 아주 동떨어진 곳에 위치하게 될 수 있습니다.
- **Low Variance 상황**: 샘플들이 기댓값 주변에 옹기종기 모여있죠. 이 경우에는 어떻게 샘플링을 하더라도 표본 평균 $\overline{X}$이 실제 기댓값 $\mathbb{E}[X]$에서 크게 벗어나지 않을 가능성이 높습니다.

정책 경사법에서 우리가 샘플링을 통해 계산한 $\frac{1}{N}\sum(\dots)$ 값이 바로 이 표본 평균 $\overline{X}$에 해당하고, 우리가 진짜 나아가야 할 방향은 기댓값 $\mathbb{E}[\dots]$입니다. 만약 분산이 크다면, 우리가 계산한 경사도 (gradient)가 엉뚱한 방향을 가리킬 수 있고, 이는 학습을 매우 불안정하게 만들거나 심지어 학습이 전혀 진행되지 않게 만들 수 있습니다.

#### 분산 감소 전략

그렇다면 이 높은 분산을 어떻게 해결할 수 있을까요? 두 가지 해결책을 생각해 볼 수 있습니다.

1. **매우 큰 N 사용하기**: 샘플의 개수 $N$을 아주 아주 크게 하면 대수의 법칙 (Law of Large Numbers)에 따라 표본 평균이 기댓값에 수렴하게 됩니다. 하지만 이는 어마어마한 양의 데이터를 수집해야 하므로 매우 비효율적입니다.
2. **분산이 낮은 추정량 사용하기**: 기댓값은 동일하게 유지하면서, 분산 자체를 줄일 수 있는 더 똑똑한 추정량 (estimator)을 찾는 방법입니다.

우리는 당연히 두 번째 방법에 집중할 것입니다. 이제부터 분산을 줄이기 위한 몇 가지 중요한 아이디어들을 살펴보겠습니다.

---

### 분산 감소 기법 1: 인과성과 Reward-to-go

첫 번째 아이디어는 바로 **인과성 (Causality)**을 활용하는 것입니다.

생각해보면, 시간 $t$에 한 행동 $\pi(a_t|s_t)$는 그 시점 **이후**의 보상 ($r_t, r_{t+1}, \dots$)에는 영향을 줄 수 있지만, **이전**의 보상 ($r_1, \dots, r_{t-1}$)에는 아무런 영향을 미칠 수 없습니다. 너무나 당연한 사실이죠.

그런데 우리가 앞서 봤던 원래 정책 경사식은 시간 $t$의 행동을 평가하는데 과거의 보상까지 모두 포함된 전체 궤적 보상 $r(\tau) = \sum_{t'=1}^{T} r(s_{t'}, a_{t'})$를 사용하고 있습니다. 이는 불필요한 노이즈를 추가하여 분산을 높이는 원인이 됩니다.

원래 정책 경사식에서 총 보상 항을 과거 보상과 미래 보상으로 나눌 수 있습니다.

$$
\nabla_{\theta}J(\theta) = \mathbb{E}_{\tau \sim \pi_{\theta}(\tau)} [\sum_{t=1}^{T} \nabla_{\theta} \log \pi_{\theta}(a_t|s_t) (\sum_{t'=1}^{t-1} r(s_{t'}, a_{t'}) + \sum_{t'=t}^{T} r(s_{t'}, a_{t'}))]
$$

여기서 인과성 원리에 따라, $\nabla_{\theta} \log \pi_{\theta}(a_t|s_t)$와 과거 보상 $\sum_{t'=1}^{t-1} r(s_{t'}, a_{t'})$ 항은 서로 독립적입니다. 따라서 기댓값을 취하면 이 부분은 0이 되어 사라지게 됩니다. (조금 더 자세히 설명하자면, 특정 상태 $s_t$가 주어졌을 때, 과거 보상의 합은 상수 취급이 되고, $\mathbb{E}_{a_t \sim \pi_{\theta}(\cdot|s_t)}[\nabla_{\theta} \log \pi_{\theta}(a_t|s_t)]$는 0이 되기 때문입니다.)

결과적으로 우리는 시간 $t$의 행동을 평가할 때, 그 시점부터 끝까지의 보상, 즉 **"미래 보상 합 (Reward-to-go)"** $R_t(\tau) = \sum_{t'=t}^{T} r(s_{t'}, a_{t'})$ 만을 사용하면 됩니다.

$$
\nabla_{\theta}J(\theta) = \mathbb{E}_{\tau \sim \pi_{\theta}(\tau)} [\sum_{t=1}^{T} \nabla_{\theta} \log \pi_{\theta}(a_t|s_t) (\sum_{t'=t}^{T} r(s_{t'}, a_{t'}))]
$$

이렇게 수정된 추정량은 원래 추정량과 **기댓값은 정확히 동일하지만, 불필요한 과거 보상 항이 제거되었기 때문에 분산은 더 작습니다.** 이것이 분산을 줄이는 첫 번째 중요한 진전입니다.

---

### 분산 감소 기법 2: 기저선

분산을 더 줄일 수 있는 두 번째 아이디어는 **기저선 (Baseline)**을 도입하는 것입니다.

핵심 아이디어는 미래 보상 합에서 어떤 값 $b(s_t)$를 빼주는 것입니다. 이 때 $b(s_t)$는 현재 상태 $s_t$에만 의존하는 함수여야 합니다.

$$
\nabla_{\theta}J(\theta) = \mathbb{E}_{\tau \sim \pi_{\theta}(\tau)} [\sum_{t=1}^{T} \nabla_{\theta} \log \pi_{\theta}(a_t|s_t) (R_t(\tau) - b(s_t))]
$$

놀랍게도, 이렇게 $b(s_t)$를 빼주어도 정책 경사의 기댓값은 변하지 않습니다. 왜 그럴까요? 추가된 항인 $-\mathbb{E}[\sum_t \nabla_{\theta} \log \pi_{\theta}(a_t|s_t) b(s_t)]$를 살펴보면, 위에서 설명했던 것과 같은 이유로 이 항의 기댓값은 0이 되기 때문입니다.

기댓값은 그대로인데, 분산은 줄일 수 있는 '가능성'이 생겼습니다. 어떻게 그게 가능할까요?

#### 기저선이 분산을 줄이는 원리

분산의 정의는 $Var(X) = \mathbb{E}[X^2] - (\mathbb{E}[X])^2$ 이죠. 기저선을 사용하든 안하든 기댓값 $\mathbb{E}[X]$은 동일하기 때문에, 분산의 크기는 오직 $\mathbb{E}[X^2]$ 항에 의해 결정됩니다.

- **Original**: $\mathbb{E}[(\sum_t \nabla_{\theta} \log \pi_{\theta} R_t(\tau))^2]$
- **With Baseline**: $\mathbb{E}[(\sum_t \nabla_{\theta} \log \pi_{\theta} (R_t(\tau) - b(s_t)))^2]$

만약 우리가 $b(s_t)$를 잘 선택해서 괄호 안의 값 $(R_t(\tau) - b(s_t))$를 0에 가깝게 만들 수 있다면, 제곱한 값은 훨씬 더 작아질 것이고, 따라서 분산도 크게 줄어들 것입니다.

#### 최적의 기저선

그렇다면 $R_t(\tau)$의 값을 가장 잘 예측하는, 즉 $R_t(\tau)$의 기댓값에 가까운 $b(s_t)$는 무엇일까요? $R_t(\tau) = \sum_{t'=t}^{T} r(s_{t'}, a_{t'})$는 상태 $s_t$에서 시작했을 때 앞으로 받을 보상의 총합입니다. 이것의 기댓값이 바로 **상태 가치 함수 (State-value Function) $V^{\pi}(s_t)$**의 정의입니다!

$$
V^{\pi}(s_t) = \mathbb{E}_{\tau \sim \pi_{\theta}(\tau|s_t)}[\sum_{t'=t}^{T} r(s_{t'}, a_{t'})]
$$

따라서 **최적의 기저선은 바로 상태 가치 함수 $V^{\pi}(s_t)$** 입니다.

---

### 어드밴티지 함수

이제 우리는 정책 경사법의 최종 진화 형태에 도달했습니다. 기저선으로 상태 가치 함수 $V^{\pi_{\theta}}(s_t)$를 사용해봅시다.

$$
\nabla_{\theta}J(\theta) = \mathbb{E}_{\tau \sim \pi_{\theta}(\tau)} [\sum_{t=1}^{T} \nabla_{\theta} \log \pi_{\theta}(a_t|s_t) (R_t(\tau) - V^{\pi_{\theta}}(s_t))]
$$

여기서 $R_t(\tau) - V^{\pi_{\theta}}(s_t)$는 매우 중요한 의미를 갖습니다. $V^{\pi_{\theta}}(s_t)$는 상태 $s_t$에서 평균적으로 기대할 수 있는 보상입니다. $R_t(\tau)$는 실제로 받은 보상이죠. 따라서 이 값은 **"상태 $s_t$에서 행동 $a_t$를 한 것이 평균보다 얼마나 더 좋았는가 혹은 나빴는가"**를 나타냅니다.

- 만약 $R_t(\tau) > V^{\pi_{\theta}}(s_t)$ 이면, 이 값은 양수가 되고, 우리는 행동 $a_t$를 할 확률을 높이게 됩니다.
- 만약 $R_t(\tau) < V^{\pi_{\theta}}(s_t)$ 이면, 이 값은 음수가 되고, 행동 $a_t$를 할 확률을 낮추게 됩니다.

여기서 한 걸음 더 나아가, 샘플링된 하나의 궤적에 대한 보상 $R_t(\tau)$ 대신, 그것의 기댓값인 **행동 가치 함수 (Action-value Function) $Q^{\pi_{\theta}}(s_t, a_t)$**를 사용하면 더 안정적인 추정이 가능합니다.

$$
Q^{\pi_{\theta}}(s_t, a_t) = \mathbb{E}_{\tau \sim \pi_{\theta}(\tau|s_t, a_t)}[\sum_{t'=t}^{T} r(s_{t'}, a_{t'})]
$$

이렇게 되면 정책 경사식은 최종적으로 다음과 같이 정리됩니다.

$$
\nabla_{\theta}J(\theta) \approx \mathbb{E} [\sum_{t=1}^{T} \nabla_{\theta} \log \pi_{\theta}(a_t|s_t) (Q^{\pi_{\theta}}(s_t, a_t) - V^{\pi_{\theta}}(s_t))]
$$

이 때 $A^{\pi_{\theta}}(s_t, a_t) = Q^{\pi_{\theta}}(s_t, a_t) - V^{\pi_{\theta}}(s_t)$를 **어드밴티지 함수 (Advantage Function)**라고 부릅니다. 이 어드밴티지 함수를 사용하는 정책 경사법이 바로 **액터-크리틱 (Actor-Critic) 알고리즘**의 핵심입니다.

- **액터 (Actor)**: 정책 $\pi_{\theta}$를 의미하며, 어떤 행동을 할지 결정합니다.
- **크리틱 (Critic)**: 가치 함수 ($V^{\pi_{\theta}}$ 또는 $Q^{\pi_{\theta}}$)를 의미하며, 액터가 한 행동이 얼마나 좋았는지를 평가 (어드밴티지 계산)하여 피드백을 줍니다.

액터는 크리틱의 평가를 바탕으로 더 좋은 방향으로 정책을 업데이트하게 되는 구조입니다. 이 방식은 현재 강화학습에서 가장 널리 사용되는 패러다임 중 하나입니다.

#### 분산 감소 기법 총정리

지금까지 정책 경사법의 분산을 줄이기 위한 여정을 한번 정리해 보겠습니다.

우리는 $\nabla_{\theta} \log \pi_{\theta}$ 뒤에 곱해지는 항을 점차 발전시켜 왔습니다.

1. **총 보상 합**: $\sum_{t=1}^{T} r(s_t, a_t)$
   - 가장 기본적인 형태로, 분산이 매우 큽니다.
2. **미래 보상 합 (Reward-to-go)**: $\sum_{t'=t}^{T} r(s_{t'}, a_{t'})$
   - 인과성을 적용하여 불필요한 노이즈를 제거했습니다.
3. **어드밴티지 함수 (Advantage)**: $A^{\pi_{\theta}}(s_t, a_t) = Q^{\pi_{\theta}}(s_t, a_t) - V^{\pi_{\theta}}(s_t)$
   - 기저선을 도입하여, '평균보다 얼마나 더 나은 행동이었는가'라는 훨씬 정제된 신호를 사용하게 되었습니다.

---

## Actor-Critic 알고리즘

지금까지 정책 경사법의 문제점을 진단하고, 그 해결책으로 어드밴티지 함수의 개념까지 도출해냈습니다. 이로써 우리는 자연스럽게 **액터-크리틱 알고리즘**으로 넘어가게 되었습니다.

### REINFORCE 알고리즘

액터-크리틱을 이해하기 위해, 먼저 그 이전의 기본적인 정책 경사 알고리즘인 **REINFORCE**를 다시 한번 살펴보겠습니다.

REINFORCE 알고리즘의 작동 방식은 다음과 같습니다.

1. 현재 정책 $\pi_{\theta}$를 사용하여 처음부터 끝까지 완전한 궤적 (trajectory)들을 여러 개 샘플링합니다.
2. 각 궤적마다 총 보상을 계산합니다.
3. 이 정보를 사용하여 정책 경사를 계산하고,
4. 경사 상승법을 통해 정책 파라미터 $\theta$를 업데이트합니다.

여기서 가장 큰 비효율은 **매번 정책 업데이트를 할 때마다 새로운 궤적 전체를 필요로 한다**는 점입니다. 즉, 에피소드가 끝나기 전까지는 학습을 전혀 할 수 없고, 수집된 데이터를 단 한 번의 업데이트에만 사용하고 버리게 됩니다. 이를 **샘플 비효율적 (sample inefficient)**이라고 합니다.

---

### Actor-Critic 기본 개념

**액터-크리틱** 알고리즘은 REINFORCE의 샘플 비효율성 문제를 해결합니다.

작동 방식을 살펴볼까요?

1. 현재 정책 (액터) $\pi_{\theta}$를 사용하여 **정해진 N 스텝만큼만** 환경과 상호작용하며 데이터를 수집합니다. (전체 궤적이 아닙니다.)
2. 수집된 데이터 $(s, a, r, s')$를 사용하여 **가치 함수 (크리틱) $V_{\phi}$를 업데이트**합니다. 크리틱은 액터의 행동을 더 잘 평가하기 위해 스스로 학습하는 것이죠.
3. 업데이트된 크리틱을 사용하여 **어드밴티지 $A_{\phi}^{\pi_{\theta}}$를 계산**하고, 이를 이용해 정책 경사를 계산합니다.
4. 계산된 경사를 통해 **정책 (액터) 파라미터 $\theta$를 업데이트**합니다.

가장 큰 차이점은, 전체 에피소드가 끝날 때까지 기다릴 필요 없이, 소량의 데이터만으로도 크리틱을 학습시키고 액터를 업데이트할 수 있다는 점입니다. 이로 인해 훨씬 더 **샘플 효율적인 (sample efficient)** 학습이 가능해집니다.

---

### 샘플 효율성 비교

두 알고리즘의 샘플 효율성 차이를 구체적인 예시로 비교해봅시다.

- 하나의 에피소드 길이가 1000 스텝이라고 가정해봅시다.
- **REINFORCE**: 업데이트를 한번 하려면 1000 스텝짜리 에피소드 하나가 온전히 끝나야 합니다. 즉, **1000 스텝마다 정책 업데이트가 1번** 일어납니다.
- **액터-크리틱**: 만약 $N=10$으로 설정했다면, **10 스텝마다 데이터를 수집하여 정책 업데이트**를 할 수 있습니다. 1000 스텝 동안 무려 100번의 업데이트가 가능한 셈이죠.

결론적으로, 동일한 양의 샘플 (데이터)에 대해 액터-크리틱이 REINFORCE보다 훨씬 더 많은 정책 업데이트를 수행할 수 있습니다. 이는 학습 속도와 성능에 큰 차이를 만들어냅니다.

---

### Actor-Critic의 한계

지금까지 설명한 액터-크리틱 알고리즘은 REINFORCE에 비해 큰 발전을 이룬 것이 사실이지만, 여전히 해결해야 할 문제들이 남아있습니다.

1. **어드밴티지 추정의 부정확성**: 우리가 사용하는 어드밴티지 값 $A(s,a)$ 역시 추정치이기 때문에 여전히 편향 (bias)과 분산 (variance) 문제가 존재합니다. 이 추정치를 어떻게 더 정확하게 만들 수 있을까요? → 이에 대한 해답이 바로 **N-step returns**와 GAE (Generalized Advantage Estimation)입니다.

2. **데이터의 단발성 사용**: 액터-크리틱도 기본적으로는 **온-폴리시 (on-policy)** 알고리즘입니다. 즉, 현재 정책 $\pi_{\theta}$로 수집한 데이터는 오직 $\pi_{\theta}$를 업데이트하는 데에만 사용해야 합니다. 한번 업데이트하고 나면 정책이 $\pi_{\theta'}$로 바뀌기 때문에, 이전에 수집했던 데이터는 더 이상 유효하지 않게 되어 버려야 합니다. 데이터를 한 번의 그래디언트 업데이트에만 사용하고 버리는 것은 여전히 비효율적입니다. → 이 문제를 해결하기 위한 기법이 바로 **중요도 샘플링 (Importance Sampling)**입니다.

다음으로 이 두 가지 문제를 해결하고 액터-크리틱을 더욱 강력하게 만드는 기법들에 대해 자세히 알아보도록 하겠습니다.

---

## Actor-Critic 개선 방법

지난 시간에는 액터-크리틱 알고리즘의 기본적인 개념과, REINFORCE 알고리즘에 비해 샘플 효율성이 높다는 장점까지 살펴보았습니다. 하지만 마지막에 두 가지 한계점을 지적했었죠. 첫째는 '어드밴티지 추정의 부정확성', 둘째는 '데이터의 단발성 사용' 문제였습니다.

이번 시간에는 이 두 가지 문제를 해결하고 액터-크리틱을 한 단계 더 발전시키는 여정을 떠나보겠습니다.

### N-step Returns

첫 번째 문제, 즉 **어드밴티지 추정이 부정확하다**는 것을 해결하기 위한 아이디어부터 시작합시다.

어드밴티지 함수 $A^{\pi}(s_t, a_t)$는 $Q^{\pi}(s_t, a_t) - V^{\pi}(s_t)$로 정의되죠. 결국 Q함수를 얼마나 잘 추정하느냐가 관건입니다. Q함수를 추정하는 방식에는 크게 두 가지 극단적인 방법이 있습니다.

1. **몬테카를로 (Monte Carlo) 방식**: 에피소드가 끝날 때까지 기다렸다가, 실제로 받은 모든 미래 보상의 합 $\sum_{t'=t}^{T} r(s_{t'}, a_{t'})$을 Q값의 추정치로 사용합니다. 이 방법은 실제 결과값을 그대로 사용하기 때문에 **편향 (bias)은 없지만**, 딱 하나의 샘플 궤적에만 의존하기 때문에 어떤 궤적을 경험했느냐에 따라 추정값이 매우 크게 흔들립니다. 즉, **분산 (variance)이 매우 높습니다**.

2. **시간차 (Temporal Difference, TD) 방식**: 딱 한 스텝만 진행해보고 얻은 보상 $r(s_t, a_t)$에, 그 다음 상태의 가치 함수 추정치 $V^{\pi}(s_{t+1})$를 더해서 Q값을 추정합니다. 이를 '부트스트래핑 (bootstrapping)'이라고 하죠. 이 방법은 여러 샘플을 통해 학습된 안정적인 가치 함수 $V^{\pi}$를 사용하기 때문에 **분산은 낮지만**, 만약 $V^{\pi}$ 자체가 부정확하다면 그 오차가 계속 누적되므로 **편향이 높습니다**.

왼쪽 끝이 몬테카를로 (높은 분산), 오른쪽 끝이 TD (높은 편향)입니다. 그렇다면 이 둘 사이의 '스위트 스팟'은 없을까요?

그 해답이 바로 **N-step returns**입니다. N-step return은 실제 보상은 $n$ 스텝까지만 사용하고, 그 이후의 가치는 가치 함수 추정치 $V^{\pi}(s_{t+n})$로 대체하는 방식입니다.

$$
\hat{Q}_{n}^{\pi}(s_t, a_t) = \sum_{t'=t}^{t+n-1} r(s_{t'}, a_{t'}) + V^{\pi}(s_{t+n})
$$

하이퍼파라미터 '$n$'을 조절함으로써 우리는 편향과 분산 사이의 트레이드오프를 조절할 수 있습니다. 하지만 여기서 또 다른 문제가 생깁니다. 최적의 '$n$'은 문제마다 다른데, 이걸 어떻게 정할 수 있을까요?

---

### 일반화된 어드밴티지 추정 (GAE)

최적의 $n$을 찾는 문제에 대한 아주 영리한 해법이 바로 **일반화된 어드밴티지 추정 (Generalized Advantage Estimation, GAE)**입니다.

GAE의 핵심 아이디어는, "하나의 $n$만 선택하지 말고, 가능한 모든 n-step 어드밴티지를 전부 계산해서 지수 가중 평균을 내자!"는 것입니다.

$$
\hat{A}_{GAE}^{\pi}(s_t, a_t) = \sum_{n=1}^{\infty} w_n \hat{A}_{n}^{\pi}(s_t, a_t), \text{ 여기서 } w_n \propto \lambda^{n-1}
$$

여기서 $\lambda$ (람다)라는 새로운 하이퍼파라미터가 등장합니다. 이 $\lambda$ 값이 GAE의 성격을 결정합니다.

- **만약 $\lambda=0$ 이라면**: 가중치가 1-step 어드밴티지에만 집중되어, TD 방식과 같아집니다. 즉, $r_t + V^{\pi}(s_{t+1}) - V^{\pi}(s_t)$가 됩니다. 분산은 낮지만 편향이 높은 추정 방식이죠.
- **만약 $\lambda=1$ 이라면**: 모든 n-step 어드밴티지에 동일한 가중치를 주어, 몬테카를로 방식과 같아집니다. 즉, $\sum_{t'=t}^{\infty} r_{t'} - V^{\pi}(s_t)$가 됩니다. 편향은 없지만 분산이 높은 방식입니다.

보통 $\lambda$는 0.95와 같이 1에 가까운 값을 사용하여 편향을 줄이면서도 분산을 적절히 제어하는, 두 방식의 장점을 모두 취하는 전략을 사용합니다. GAE는 현재 대부분의 고성능 액터-크리틱 계열 알고리즘에서 표준처럼 사용되는 매우 중요한 기법입니다.

---

### 중요도 샘플링

자, 이제 첫 번째 문제였던 '어드밴티지 추정'을 GAE를 통해 해결했습니다. 이제 두 번째 문제인 '데이터의 단발성 사용'으로 넘어가 봅시다.

#### 데이터 재사용의 필요성

정책 경사 $\nabla_{\theta}J(\theta)$는 기댓값 $\mathbb{E}_{\tau \sim \pi_{\theta}(\tau)}[\dots]$ 형태로 정의됩니다. 이 기댓값은 현재 정책 $\pi_{\theta}$를 따르는 궤적 $\tau$에 대한 것입니다. 즉, 정책 경사를 계산하려면 **반드시 현재 정책으로 샘플링한 데이터**를 사용해야 합니다. 이를 **온-폴리시 (on-policy)**라고 합니다.

문제는, 우리가 경사 상승법 $\theta' \leftarrow \theta + \alpha \nabla_{\theta}J(\theta)$를 통해 정책을 **단 한 번이라도 업데이트**하면, 정책은 더 이상 $\pi_{\theta}$가 아니라 새로운 $\pi_{\theta'}$가 됩니다. 따라서 이전에 $\pi_{\theta}$로 수집했던 데이터는 새로운 정책 $\pi_{\theta'}$의 경사를 계산하는 데에는 원칙적으로 사용할 수 없게 됩니다. 힘들게 모은 데이터를 딱 한 번의 업데이트에만 쓰고 버려야 하는 것이죠. 이것이 바로 샘플 비효율성의 근본적인 원인입니다.

#### 중요도 샘플링의 원리

"이전 정책에서 뽑은 아까운 데이터를 재사용할 방법은 없을까?" 라는 질문에 대한 통계학적 해답이 바로 **중요도 샘플링 (Importance Sampling)**입니다.

중요도 샘플링의 핵심 수식을 봅시다.

$$
\mathbb{E}_{x \sim p(x)}[f(x)] = \int p(x)f(x)dx = \int q(x) \frac{p(x)}{q(x)} f(x)dx = \mathbb{E}_{x \sim q(x)}[\frac{p(x)}{q(x)}f(x)]
$$

이 수식이 의미하는 바는, 우리가 원래 알고 싶은 $p(x)$ 분포에서의 기댓값을, 다른 분포인 $q(x)$에서 샘플링한 데이터로 계산할 수 있다는 것입니다. 단, 샘플링 분포가 달라진 것을 보정해주기 위해 $\frac{p(x)}{q(x)}$라는 **중요도 가중치 (importance weight)**를 곱해줘야 합니다.

#### 정책 경사에 적용

이 원리를 우리의 정책 경사 문제에 적용해 봅시다. 우리가 샘플링한 분포는 과거 정책 $\pi_{\theta_{old}}$이고, 우리가 기댓값을 계산하고 싶은 분포는 현재 정책 $\pi_{\theta}$입니다. 따라서 $p(x) \rightarrow \pi_{\theta}(\tau)$, $q(x) \rightarrow \pi_{\theta_{old}}(\tau)$로 치환할 수 있습니다.

최종적으로 우리는 과거 정책 $\pi_{\theta_{old}}$로 샘플링한 데이터를 사용하면서, 현재 정책 $\pi_{\theta}$의 경사를 다음과 같이 추정할 수 있습니다.

$$
\nabla_{\theta}J(\theta) \approx \mathbb{E}_{\tau \sim \pi_{\theta_{old}}(\tau)} [(\frac{\pi_{\theta}(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}) \nabla_{\theta} \log \pi_{\theta}(a_t|s_t) A(s_t, a_t)]
$$

여기서 $\frac{\pi_{\theta}(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}$가 바로 중요도 가중치입니다. 이 덕분에 우리는 수집한 데이터를 버리지 않고 여러 번의 정책 업데이트에 재사용할 수 있게 되어 샘플 효율성을 크게 높일 수 있습니다.

#### 중요도 샘플링을 적용한 액터-크리틱

중요도 샘플링을 적용하면 액터-크리틱 알고리즘의 흐름이 이렇게 바뀝니다.

1. 일단 데이터를 $N$ 스텝만큼 수집합니다. ($\pi_{\theta}$ 사용)
2. 이 데이터로 어드밴티지를 계산합니다.
3. 현재 정책을 $\theta_{old}$로 저장해둡니다.
4. **For k=1, ..., K**: 수집된 **동일한 데이터**를 가지고 **여러 번 (K번)**의 경사 업데이트를 수행합니다. 단, 업데이트할 때마다 중요도 가중치 $\frac{\pi_{\theta^{(k)}}(a_i|s_i)}{\pi_{\theta_{old}}(a_i|s_i)}$를 곱해 분포의 차이를 보정해줍니다.
5. K번의 내부 업데이트가 끝나면, 최종적으로 업데이트된 정책을 새로운 $\theta$로 확정합니다.

이로써 데이터를 훨씬 더 효율적으로 '재활용'할 수 있게 되었습니다.

---

### 중요도 샘플링의 문제점

하지만 중요도 샘플링은 만능 해결책이 아닙니다. 치명적인 함정이 있죠.

중요도 가중치 $\frac{\pi_{\theta}}{\pi_{\theta_{old}}}$는 두 정책 분포의 비율입니다. 만약 새로운 정책 $\pi_{\theta}$가 과거 정책 $\pi_{\theta_{old}}$와 너무 많이 달라지게 되면, 이 비율 값이 매우 커지거나 0에 가까워질 수 있습니다. 이는 추정된 경사의 분산을 폭발적으로 증가시켜 학습을 매우 불안정하게 만듭니다.

따라서 중요도 샘플링을 안정적으로 사용하기 위한 핵심 전제 조건은 **"새로운 정책 $\pi_{\theta}$가 과거 정책 $\pi_{\theta_{old}}$로부터 너무 멀리 벗어나지 않도록 제어해야 한다"**는 것입니다.

이 문제를 해결하기 위한 두 가지 대표적인 알고리즘이 있습니다.

1. **신뢰 영역 정책 최적화 (Trust Region Policy Optimization, TRPO)**: 파라미터 공간이 아닌 정책 분포 공간에서, 두 정책의 KL 발산 (KL-divergence) 값이 특정 임계값 $\epsilon$을 넘지 않도록 직접적인 제약 조건을 걸어 최적화 문제를 풉니다. 아이디어는 훌륭하지만 계산이 매우 복잡하다는 단점이 있습니다.

2. **Proximal Policy Optimization (PPO)**: TRPO의 복잡한 제약 조건 최적화 대신, 목적 함수 자체를 수정하여 정책 변화를 간접적으로 제한하는 더 간단하고 실용적인 방법을 제안합니다. PPO는 정책 비율 자체를 특정 범위 ($1-\epsilon$ ~ $1+\epsilon$) 내로 '잘라내는 (clipping)' 방식을 사용합니다.

오늘 수업의 최종 목적지인 PPO에 거의 다 왔습니다.

---

## PPO 알고리즘

### PPO의 핵심: Clipped Objective Function

PPO의 가장 핵심적인 아이디어는 바로 이 **잘라내기 (clipping)** 기법에 있습니다.

여기서 $ratio_t(\theta) = \frac{\pi_{\theta}(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}$ 입니다.

#### Case 1: 어드밴티지가 양수일 때 ($A > 0$)

이것은 행동 $a_t$가 평균보다 좋은 행동이었다는 의미이므로, 우리는 $\pi_{\theta}(a_t|s_t)$의 확률을 높이고 싶습니다. 그러면 $ratio_t(\theta)$가 1보다 커지면서 목적 함수 값도 커지겠죠. 하지만 너무 과도하게 확률을 높이면 학습이 불안정해집니다.

그래서 PPO는 $ratio_t(\theta)$가 특정 상한선 ($1+\epsilon$)을 넘어서지 못하도록 '잘라냅니다'. 즉, $ratio_t(\theta)$가 $1+\epsilon$보다 커지더라도, 목적 함수 계산에는 $1+\epsilon$까지만 반영하여 과도한 업데이트를 막습니다.

#### Case 2: 어드밴티지가 음수일 때 ($A < 0$)

이것은 나쁜 행동이었다는 의미이므로, $\pi_{\theta}(a_t|s_t)$의 확률을 낮추고 싶습니다. 그러면 $ratio_t(\theta)$가 1보다 작아지면서 목적 함수 값 (음수이므로 절댓값은 커짐)도 커지게 됩니다. 하지만 너무 과도하게 확률을 낮추는 것도 문제가 될 수 있습니다.

그래서 PPO는 $ratio_t(\theta)$가 특정 하한선 ($1-\epsilon$) 밑으로 내려가지 못하도록 '잘라냅니다'. 즉, $ratio_t(\theta)$가 $1-\epsilon$보다 작아지더라도, 목적 함수 계산에는 $1-\epsilon$까지만 반영합니다.

#### PPO-Clip 목적 함수

이 두 가지 경우를 하나의 수식으로 합친 것이 바로 **PPO-Clip 목적 함수**입니다.

$$
J(\theta) = \mathbb{E} [\min(ratio_t(\theta) A_t, \text{clip}(ratio_t(\theta), 1-\epsilon, 1+\epsilon) A_t)]
$$

여기서 `min` 함수가 들어간 이유는, 잘라낸 버전의 목적 함수와 원래 목적 함수 중 더 **보수적인 (pessimistic) 값, 즉 더 작은 값을 선택**하기 위함입니다. 이를 통해 정책이 한 번에 너무 크게 변하는 것을 효과적으로 방지하여 학습의 안정성을 크게 높일 수 있습니다.

---

### PPO 알고리즘 전체 흐름

지금까지 배운 모든 조각들을 합쳐 PPO 알고리즘의 전체 그림을 완성해 보겠습니다.

1. 현재 정책 $\pi_{\theta}$로 $N$ 스텝만큼 데이터를 수집합니다.
2. 수집한 데이터로 **GAE**를 사용하여 어드밴티지 $A_{GAE}^{\pi_{\theta}}$를 계산합니다.
3. 현재 정책을 $\theta_{old}$로 저장합니다.
4. **For k=1, ..., K (여러 에포크 반복)**:
   수집된 **동일한 데이터 미니배치**를 사용하여, **PPO-Clip 목적 함수**의 경사를 계산하고 정책 $\theta$를 업데이트합니다.
5. K번의 내부 업데이트가 끝나면, 최종적으로 업데이트된 정책을 새로운 $\theta$로 확정하고, 다시 1번 과정으로 돌아가 새로운 데이터를 수집합니다.

PPO는 이처럼 GAE, 중요도 샘플링, 그리고 잘라내기 목적 함수라는 핵심 아이디어들을 결합하여 안정적이면서도 샘플 효율적인 학습을 가능하게 만든 매우 강력한 알고리즘입니다.

---

### PPO의 장단점

그렇다면 PPO가 왜 이렇게 인기가 많을까요?

#### 장점

- **구현이 비교적 간단합니다**: TRPO처럼 복잡한 2차 최적화나 제약 조건이 없어 구현하기가 훨씬 쉽습니다.
- **이산/연속 행동 공간 모두 지원**: 다양한 환경에 범용적으로 적용 가능합니다.
- **확장성**: 병렬 처리에 용이하여 대규모 학습이 가능합니다.
- **안정적인 성능**: 하이퍼파라미터에 민감하긴 하지만, 전반적으로 매우 안정적이고 좋은 성능을 보여줍니다.

#### 단점

- **하이퍼파라미터에 민감합니다**: 롤아웃 길이 ($N$), 배치 사이즈, 엔트로피 계수, 학습률, $\epsilon$, $\lambda$ 등 튜닝해야 할 하이퍼파라미터가 많고, 이 값들에 따라 성능이 크게 좌우될 수 있습니다.

---

### PPO 응용 사례

PPO의 안정성과 뛰어난 성능 덕분에, 복잡한 제어를 요구하는 다양한 분야에서 널리 사용되고 있습니다.

#### 로봇 공학

로봇 공학 분야에서의 활약이 두드러집니다. 정교한 조립 작업, 물체 조작, 동물의 움직임을 모방한 파쿠르, 험지에서의 인간형 로봇 보행 등 고차원 연속 행동 제어 문제에서 SOTA (State-of-the-art) 성능을 보여주고 있습니다.

#### 대규모 언어 모델 (LLM) 훈련

특히 주목할 만한 응용 사례는 바로 **ChatGPT와 같은 대규모 언어 모델을 훈련**시키는 데 사용된다는 점입니다. **인간 피드백 기반 강화학습 (RLHF, Reinforcement Learning from Human Feedback)**이라는 프로세스의 3단계에서, 인간의 선호도를 학습한 보상 모델 (Reward Model)을 환경처럼 사용하고, PPO를 통해 언어 모델의 정책 (다음 단어를 생성할 확률)을 이 보상을 최대화하는 방향으로 미세 조정 (fine-tuning)합니다. 즉, 여러분이 LLM과 대화할 때 더 유용하고 안전한 답변을 생성하도록 만드는 데 PPO가 핵심적인 역할을 하고 있는 것입니다.

---

### 샘플 효율성, 그 이상의 것을 향하여

PPO는 중요도 샘플링을 통해 이전 데이터를 재사용함으로써 샘플 효율성을 크게 개선했습니다. 하지만 정책이 $\pi_{old}$에서 너무 멀어지지 않도록 제약을 가하기 때문에, 여전히 **온-폴리시 계열의 알고리즘**으로 분류됩니다. 즉, 완전히 상관없는 과거의 다양한 정책들로부터 얻은 데이터를 자유롭게 사용하지는 못합니다.

강화학습의 또 다른 큰 축은, 이러한 제약 없이 경험 리플레이 버퍼 (experience replay buffer)에 저장된 방대한 양의 과거 데이터를 모두 활용하여 학습하는 **오프-폴리시 (off-policy) 알고리즘**입니다. 대표적으로 DQN, DDPG, SAC 같은 알고리즘들이 여기에 속합니다.

---

## 요약

### 학습 내용 정리

이번 강의에서는 정책 경사법에서 시작하여 PPO 알고리즘까지의 진화 과정을 체계적으로 학습했습니다.

#### 1. 핵심 개념

**정책 경사법 (Policy Gradient)**
- 목적: 보상의 총합을 최대화하는 정책 $\pi_{\theta}$ 찾기
- 목적 함수: $J(\theta) = \mathbb{E}_{\tau \sim \pi_{\theta}(\tau)}[\sum_t r(\tau)]$
- 정책 경사 정리: $\nabla_{\theta}J(\theta) = \mathbb{E}_{\tau \sim \pi_{\theta}(\tau)} [(\sum_{t=1}^{T} \nabla_{\theta} \log \pi_{\theta}(a_t|s_t)) (\sum_t r(s_t, a_t))]$
- 주요 문제: 높은 분산 (variance)

**분산 감소 기법**
1. **인과성 (Causality)**: 과거 보상 제거 → Reward-to-go 사용
2. **기저선 (Baseline)**: 상태 가치 함수 $V^{\pi}(s_t)$ 활용
3. **어드밴티지 함수 (Advantage Function)**: $A^{\pi}(s_t, a_t) = Q^{\pi}(s_t, a_t) - V^{\pi}(s_t)$

**액터-크리틱 (Actor-Critic)**
- 액터 (Actor): 정책 $\pi_{\theta}$ - 행동 선택
- 크리틱 (Critic): 가치 함수 $V^{\pi}$ - 행동 평가
- 장점: REINFORCE 대비 높은 샘플 효율성
- 한계: 어드밴티지 추정 부정확성, 데이터 단발성 사용

#### 2. Actor-Critic 개선 방법

**N-step Returns**
- 몬테카를로 (높은 분산) ↔ TD (높은 편향) 사이의 균형
- $\hat{Q}_{n}^{\pi}(s_t, a_t) = \sum_{t'=t}^{t+n-1} r(s_{t'}, a_{t'}) + V^{\pi}(s_{t+n})$

**GAE (Generalized Advantage Estimation)**
- 모든 n-step 어드밴티지의 지수 가중 평균
- 하이퍼파라미터 $\lambda$로 편향-분산 트레이드오프 조절
- $\lambda=0$: TD 방식, $\lambda=1$: 몬테카를로 방식
- 실무: $\lambda \approx 0.95$

**중요도 샘플링 (Importance Sampling)**
- 과거 정책의 데이터를 현재 정책 학습에 재사용
- 중요도 가중치: $\frac{\pi_{\theta}(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}$
- 문제점: 두 정책이 너무 달라지면 분산 폭발

#### 3. PPO (Proximal Policy Optimization)

**핵심 아이디어**
- Clipping을 통한 정책 업데이트 제한
- 정책 비율을 $[1-\epsilon, 1+\epsilon]$ 범위로 제한

**PPO-Clip 목적 함수**

$$
J(\theta) = \mathbb{E} [\min(ratio_t(\theta) A_t, \text{clip}(ratio_t(\theta), 1-\epsilon, 1+\epsilon) A_t)]
$$

여기서 $ratio_t(\theta) = \frac{\pi_{\theta}(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}$

**알고리즘 흐름**
1. 데이터 수집 ($N$ 스텝)
2. GAE로 어드밴티지 계산
3. $\theta_{old}$ 저장
4. K번의 미니배치 업데이트 (PPO-Clip 목적 함수 사용)
5. 반복

**장점**
- 구현 간단 (TRPO 대비)
- 이산/연속 행동 공간 지원
- 안정적 성능
- 높은 샘플 효율성
- 병렬 처리 용이

**단점**
- 하이퍼파라미터 튜닝 민감성 ($N$, batch size, learning rate, $\epsilon$, $\lambda$, entropy coefficient)

#### 4. 주요 응용 분야

**로봇 공학**
- 고차원 연속 제어 (조립, 조작, 보행)
- 복잡한 동작 학습 (파쿠르, 험지 이동)

**대규모 언어 모델**
- RLHF (Reinforcement Learning from Human Feedback)
- ChatGPT 등 LLM의 미세 조정
- 인간 선호도 학습 및 정책 최적화

#### 5. 알고리즘 비교

| 알고리즘 | 샘플 효율성 | 안정성 | 구현 복잡도 | 데이터 재사용 |
|---------|------------|--------|------------|--------------|
| REINFORCE | 낮음 | 낮음 | 간단 | 불가 |
| Actor-Critic | 중간 | 중간 | 중간 | 제한적 |
| TRPO | 높음 | 높음 | 복잡 | 가능 |
| **PPO** | **높음** | **높음** | **중간** | **가능** |

### 중요 포인트

1. **분산 감소의 중요성**: 정책 경사법의 성능은 분산을 얼마나 효과적으로 줄이느냐에 달려있습니다.

2. **어드밴티지 함수**: "평균 대비 얼마나 좋은 행동인가"를 측정하는 핵심 개념으로, 현대 강화학습의 기반입니다.

3. **샘플 효율성**: 데이터를 재사용하는 능력이 실용적인 강화학습 시스템의 핵심입니다.

4. **안정성과 효율성의 균형**: PPO는 복잡한 제약 최적화 없이 간단한 clipping으로 이 균형을 달성했습니다.

5. **실무 적용성**: PPO는 이론적 우수성뿐만 아니라 실제 다양한 도메인에서 검증된 실용적 알고리즘입니다.

### 다음 단계

PPO는 온-폴리시 알고리즘의 정점에 있지만, 완전히 다른 패러다임인 **오프-폴리시 (off-policy) 강화학습**도 존재합니다. DQN, DDPG, SAC 등의 알고리즘은 경험 리플레이 버퍼를 활용하여 완전히 다른 방식으로 샘플 효율성을 달성합니다. 다음 강의에서는 이러한 오프-폴리시 방법론을 탐구하게 될 것입니다.

---

이것으로 PPO에 대한 긴 여정을 마치겠습니다. 오늘 수업 내용이 강화학습의 핵심적인 아이디어들을 이해하는 데 큰 도움이 되었기를 바랍니다. 수고 많으셨습니다.
