# Lecture 03: 강화학습의 기초와 정책 경사도 (Policy Gradient)

## 목차

- [강의 소개](#강의-소개)
  - [강의 개요](#강의-개요)
- [강화학습 (Reinforcement Learning)](#강화학습-reinforcement-learning)
  - [강화학습의 필요성](#강화학습의-필요성)
  - [강화학습의 정의](#강화학습의-정의)
  - [마르코프 결정 과정 (MDP)](#마르코프-결정-과정-mdp)
  - [마르코프 과정의 예시](#마르코프-과정의-예시)
  - [마르코프 보상 과정 (MRP)](#마르코프-보상-과정-mrp)
  - [마르코프 결정 과정 (MDP)](#마르코프-결정-과정-mdp-1)
  - [미지의 MDP](#미지의-mdp)
  - [실제 코드에서의 MDP](#실제-코드에서의-mdp)
  - [부분 관찰 가능 MDP](#부분-관찰-가능-mdp)
  - [다양한 강화학습 예시](#다양한-강화학습-예시)
  
- [정책 경사도 (Policy Gradient)](#정책-경사도-policy-gradient)
  - [지도학습과의 비교](#지도학습과의-비교)
  - [목표 함수 정의](#목표-함수-정의)
  - [정책 경사도 유도](#정책-경사도-유도)
  - [궤적 확률의 미분](#궤적-확률의-미분)
  - [REINFORCE 알고리즘](#reinforce-알고리즘)
  - [정책 경사도 방법 요약](#정책-경사도-방법-요약)
- [분산 감소 (Variance Reduction)](#분산-감소-variance-reduction)
  - [높은 분산의 문제](#높은-분산의-문제)
  - [분산 감소 방법](#분산-감소-방법)
  - [개선된 추정량](#개선된-추정량)


---

## 강의 소개

### 강의 개요

오늘 우리가 무엇을 배울지 전체적인 지도를 한번 보고 가겠습니다.

1. **Quick recap**: 지난 시간에 배운 모방 학습에 대해 간단히 복습하며 강화학습의 필요성을 다시 한번 상기할 겁니다.
2. **Reinforcement learning**: 본격적으로 강화학습이 무엇인지, 어떤 수학적 틀 안에서 정의되는지 배울 겁니다.
3. **Policy gradient method**: 강화학습 문제를 푸는 가장 기본적인 알고리즘 중 하나인 정책 경사도 방법에 대해 알아볼 것입니다.
4. **Why variance matters in policy gradient?**: 마지막으로 정책 경사도 방법의 고질적인 문제인 '분산(variance)'이 왜 중요한지, 그리고 이것이 학습에 어떤 영향을 미치는지 살펴볼 예정입니다.

---

## 강화학습 (Reinforcement Learning)

### 강화학습의 필요성

지난 시간에 배운 모방 학습은 간단하고 강력한 프레임워크입니다. 하지만 모방 학습은 몇 가지 근본적인 한계를 가지고 있습니다.

- 어떤 문제들은 **전문가 시연 데이터를 모으는 것 자체가 매우 어렵거나 불가능**합니다. 예를 들어, 다리가 4개 달린 로봇을 걷게 하는 전문가 데이터를 사람이 일일이 만드는 것은 거의 불가능에 가깝습니다.

- 설령 전문가 데이터가 있다 해도, **학습된 정책은 절대로 전문가보다 더 잘할 수 없습니다.** 전문가의 한계가 곧 정책의 한계가 되는 것입니다.

- 가장 중요한 점은, 모방 학습은 에이전트가 스스로의 **경험이나 간접적인 피드백(보상 등)으로부터 학습하는 방법을 제공하지 않는다**는 것입니다.

"과연 에이전트는 누구의 도움도 없이, 스스로의 실수로부터 배우면서 성장할 수 없을까?"

이 질문에 대한 해답이 바로, 우리가 오늘 배울 **강화학습 (Reinforcement Learning)**입니다.

### 강화학습의 정의

자, 이제 오늘의 본론인 강화학습으로 넘어가겠습니다.

강화학습의 대가인 Sutton과 Barto는 그들의 저서에서 강화학습을 이렇게 정의했습니다.

> "강화학습이란, **수치적인 보상 신호를 최대화**하기 위해, 주어진 상황(situation)을 행동(action)에 어떻게 맵핑할 것인지를 배우는 것이다."

이 두 분은 강화학습 분야를 개척한 공로로 2024년 ACM 튜링상을 수상하기도 한, 이 분야의 살아있는 전설입니다.

위키피디아의 정의도 비슷합니다.

> "강화학습(RL)은 지능적인 에이전트가 동적인 환경 속에서 **누적 보상 (Cumulative reward)을 최대화**하기 위해 어떻게 행동해야 하는지를 다루는 학문이다."

핵심 키워드는 **'누적 보상 최대화'** 입니다. 모방 학습처럼 '정답 행동'을 가르쳐주는 것이 아니라, 에이전트가 한 행동에 대해 '보상'이라는 피드백을 줍니다. 에이전트는 이 보상을 더 많이 받기 위해 스스로 최적의 행동 방식을 찾아 나가야 합니다.

강화학습 과정을 정리하면 다음과 같습니다:

- 에이전트($\pi(a_t|s_t)$)가 환경에 행동($a_t$)을 가하면,
- 환경은 그 결과로 다음 상태($S_{t+1}$)와 함께 보상($r_t$)을 에이전트에게 돌려줍니다.
- 이 과정이 $(s_0, a_0, r_0), (s_1, a_1, r_1), ...$ 와 같이 반복되고,
- 강화학습의 목표는 이 보상들의 총합, 즉 $\sum r_t$ 를 최대화하는 정책 $\pi(a_t|s_t)$를 찾는 것입니다.

---

### 마르코프 결정 과정 (MDP)

그렇다면 이 강화학습 문제를 어떻게 수학적으로 엄밀하게 정의할 수 있을까요? 이때 사용되는 수학적 프레임워크가 바로 **'마르코프 결정 과정 (Markov Decision Process, MDP)'** 입니다. MDP는 강화학습의 환경(environment)과 과제(task, reward)를 공식적으로 정의하는 틀입니다.

MDP를 이해하기 위해 용어를 하나씩 뜯어보겠습니다.

#### 과정 (Process)

MDP에서의 '과정'은 **'확률 과정 (Random/Stochastic Process)'**을 의미합니다. 이것은 시간에 따라 변화하는 확률 변수들의 집합, 즉 **무작위적인 상태들의 시퀀스(sequence)**라고 생각할 수 있습니다.

#### 마르코프 (Markov)

그냥 확률 과정이 아니라 **'마르코프 속성 (Markov Property)'**을 따르는 특별한 과정입니다.

> **마르코프 속성이란, "미래는 과거와 현재에 독립적이며, 오직 현재에만 의존한다"는 성질입니다.**

수식으로는 $p(s_{t+1}|s_t) = p(s_{t+1}|s_1, ..., s_t)$ 와 같이 표현됩니다. 이게 무슨 뜻이냐면, 다음 상태 $s_{t+1}$을 예측하는 데 있어서, 과거의 모든 역사($s_1, ..., s_{t-1}$)는 알 필요 없고, 오직 현재 상태 $s_t$ 하나만으로 충분하다는 의미입니다. 현재 상태가 과거의 모든 중요한 정보를 압축해서 담고 있다는 뜻입니다.

따라서 **마르코프 과정 (Markov Process)** 또는 **마르코프 연쇄 (Markov Chain)**는 상태들의 집합 $S$와 상태 전이 확률 $p(s_{t+1}|s_t)$ 라는 두 가지 요소로 정의되는, '기억 없는(memoryless)' 확률 과정입니다.

---

### 마르코프 과정의 예시

말로만 들으면 어려우니 예시를 통해 이해해봅시다. 여기 학생의 하루를 나타내는 마르코프 과정이 있습니다. 상태는 'Study RL', 'HW1', 'Netflix', 'Submit' 네 가지입니다.

- 'Study RL' 상태에서는 0.5의 확률로 'HW1' 상태로 넘어가고, 0.5의 확률로 'Netflix'를 보는 상태로 빠집니다.
- 'HW1' 상태에서는 0.7의 확률로 과제를 'Submit'하고, 0.3의 확률로 다시 'Netflix'에 빠지네요.
- 'Netflix' 상태에서는 0.9라는 아주 높은 확률로 계속 'Netflix'를 보고, 0.1의 낮은 확률로 정신을 차리고 'Study RL'로 돌아갑니다.

이 확률들은 오직 현재 상태에 의해서만 결정됩니다. 예를 들어, 'HW1' 상태에서 'Submit'으로 갈 확률 0.7은 그 이전에 'Study RL'을 했는지, 'Netflix'를 봤는지와는 전혀 무관합니다. 이것이 바로 마르코프 속성입니다.

이 과정을 따라가면 "Study RL → HW1 → Submit" 같은 바람직한 에피소드도 나올 수 있고, "Netflix → Netflix → Netflix → ..." 와 같이 무한히 넷플릭스의 굴레에 빠지는 비극적인 에피소드도 나올 수 있습니다.

---

### 마르코프 보상 과정 (MRP)

자, 이제 이 마르코프 과정에 **'보상 (Reward)'** 개념을 추가해 봅시다. 그러면 **'마르코프 보상 과정 (Markov Reward Process, MRP)'**이 됩니다.

MRP는 다음과 같은 튜플(tuple)로 정의됩니다:

$$
\langle S, p(s_{t+1}|s_t), r(s_t), \gamma \rangle
$$

- $S$: 상태의 집합
- $p(s_{t+1}|s_t)$: 상태 전이 확률
- $r(s_t)$: **보상 함수 (Reward Function)**. 특정 상태 $s_t$에 도달했을 때 받는 즉각적인 보상입니다.
- $\gamma$: **할인율 (Discount Factor)**. $0$과 $1$ 사이의 값을 가집니다.

여기서 할인율 $\gamma$는 매우 중요한 개념입니다. 왜 미래의 보상을 현재 가치로 할인해야 할까요?

1. **수학적 안정성**: 'Netflix' 예시처럼 무한히 반복될 수 있는 과정에서 총 보상이 무한대로 발산하는 것을 막아줍니다.

2. **현실 반영**: 경제학적으로도 '오늘의 만 원이 내일의 만 원보다 가치있다'는 개념과 같습니다. 즉, 더 빨리 받는 보상이 더 좋다는 직관을 모델에 반영하는 것입니다. 또한, 불필요하게 경로를 우회하는 것을 막는 효과도 있습니다.

#### 마르코프 보상 과정의 예시

다시 아까의 학생 예시로 돌아가서, 각 상태에 보상을 부여해 봅시다.

- 'Study RL'과 'HW1' 같은 힘든 상태에 있으면 보상이 -1입니다.
- 'Netflix'를 보면 조금 덜 나쁜 -0.1의 보상을 받습니다.
- 마침내 과제를 'Submit'하면 +100이라는 아주 큰 보상을 받습니다.

이제 에피소드를 따라가면서 보상을 계산해볼 수 있습니다.

- "Study RL (-1) → HW1 (-1) → Submit (+100)" 경로의 (할인되지 않은) 총 보상은 98점이 됩니다.
- 만약 중간에 넷플릭스를 본다면, "Study RL (-1) → Netflix (-0.1) → Netflix (-0.1) → Study RL (-1) → HW1 (-1) → Submit (+100)" 과 같이 중간에 음수 보상이 누적되어 총 보상은 더 낮아지게 됩니다.

**Q. 최종 상태(terminal state)를 제외하고 모두 음수 보상을 주면, 과정의 경로가 짧아지게 될까?**

정답은 **"그렇다"** 입니다. 에이전트의 목표는 누적 보상을 최대화하는 것입니다. 매 스텝마다 음수 보상, 즉 벌점을 받는다면 에이전트는 최대한 빨리 이 벌점을 그만 받고 최종 목표의 큰 양수 보상을 받으려고 할 것입니다. 자연스럽게 최단 경로를 학습하게 되는 효과가 있습니다. 이것은 강화학습에서 보상을 설계(reward shaping)하는 아주 흔하고 효과적인 기법 중 하나입니다.

---

### 마르코프 결정 과정 (MDP)

앞서 다룬 마르코프 보상 과정(MRP)은 상태가 어떻게 변하고, 각 상태에서 어떤 보상을 받는지만을 다뤘습니다. 거기에는 에이전트의 '선택'이 끼어들 여지가 없었습니다. 이제 여기에 **'행동 (Action)'** 이라는 개념을 추가하여 에이전트가 능동적으로 환경에 영향을 미치는 상황을 모델링해 보겠습니다. 이것이 바로 MDP입니다.

MDP는 다음의 튜플(tuple)로 완벽하게 정의됩니다:

$$
\langle S, A, p(s_{t+1}|s_t, a_t), r(s_t, a_t), \gamma \rangle
$$

각 요소를 자세히 살펴볼까요?

- $S$: 유한한 **상태 (State)의 집합**입니다.
- $A$: 유한한 **행동 (Action)의 집합**입니다. 에이전트가 각 상태에서 할 수 있는 선택지들입니다.
- $p(s_{t+1}|s_t, a_t)$: **상태 전이 확률 (State transition probability)** 입니다. MRP와 가장 큰 차이점입니다. 다음 상태 $s_{t+1}$로 변할 확률이 현재 상태 $s_t$뿐만 아니라, 에이전트가 선택한 **행동 $a_t$** 에도 의존하게 됩니다.
- $r(s_t, a_t)$: **보상 함수 (Reward function)** 입니다. 마찬가지로, 에이전트가 받는 즉각적인 보상도 상태 $s_t$와 행동 $a_t$ 모두에 따라 결정됩니다. 어떤 상태에 있는 것만으로 보상을 받는 게 아니라, 그 상태에서 '어떤 행동을 했는가'가 보상을 결정하는 것이 더 일반적입니다.
- $\gamma$: 이전과 동일한 **할인율 (Discount factor)** 입니다. 미래의 보상을 현재 가치로 환산하는 역할을 합니다.

이처럼 MDP는 에이전트의 행동이 미래의 상태와 보상에 직접적인 영향을 미치는, 상호작용이 가능한 환경을 수학적으로 완벽하게 정의하는 틀입니다.

#### MDP의 예시

다시 한번 우리 학생의 예시로 돌아와서 이번엔 MDP로 이 상황을 모델링해 봅시다. 이전 MRP 예시와는 다르게, 이제 화살표에 '행동'이 명시되어 있습니다.

- **상태 $S$**: {"Study", "Netflix", "HW1", "Done"} 네 가지로 이전과 동일합니다.
- **행동 $A$**: {"Study RL", "Netflix", "Start HW1", "Submit"} 이 있습니다.

이제는 에이전트가 어떤 상태에 '놓이는' 것이 아니라, 상태에 따라 행동을 '선택'하게 됩니다. 예를 들어, 초기 'Study' 상태에서 에이전트가 'Study RL'이라는 행동을 선택하면 $R=-1$의 보상을 받고 'HW1' 상태로 가게 되고, 'Netflix'라는 행동을 선택하면 $R=-0.1$의 보상을 받고 'Netflix' 상태로 가는 식입니다. 모든 전이와 보상이 에이전트의 행동에 따라 결정되는 것을 볼 수 있습니다. 이것이 바로 MDP의 핵심입니다.

#### MDP와 정책 (Policy)

다시 이 그림으로 돌아와 보겠습니다. 이제 우리는 이 그림의 모든 구성요소를 완벽하게 이해할 수 있습니다.

- **환경 (Environment)**: 이 그림에서 오른쪽의 'environment'라고 표시된 부분, 즉 상태를 바꾸고($p(s_{t+1}|s_t, a_t)$) 보상을 주는($r(s_t, a_t)$) 규칙의 집합이 바로 **MDP** 그 자체입니다. MDP는 이 세상이 어떻게 돌아가는지에 대한 '게임의 법칙'을 정의합니다.

- **에이전트 (Agent)**: 왼쪽의 'agent'는 이 환경 속에서 살아가는 주인공입니다. 에이전트의 행동을 결정하는 것이 바로 **정책 (Policy)**, $\pi(a_t|s_t)$ 입니다. 정책은 에이전트의 '뇌' 또는 '전략'이라고 할 수 있으며, 현재 상태 $s_t$를 보고 어떤 행동 $a_t$를 할지를 결정하는 함수(또는 확률분포)입니다.

강화학습의 목표는 명확합니다. 주어진 환경(MDP) 속에서, 누적 보상의 합($\sum r_t$)을 최대로 만드는 최적의 에이전트, 즉 **최적의 정책 ($\pi^*$)**을 찾는 것입니다.

#### MDP의 완전한 정의

여기서 MDP의 정의를 조금 더 엄밀하게 완성해 보겠습니다. 실제로는 초기 상태가 항상 고정되어 있지 않고, 확률적으로 결정될 수 있습니다.

따라서 가장 완전한 형태의 MDP는 다음과 같이 정의됩니다:

$$
\langle S, A, p(s_0), p(s_{t+1}|s_t, a_t), r(s_t, a_t), \gamma \rangle
$$

여기서 추가된 $p(s_0)$는 **초기 상태 분포 (Initial state distribution)**를 의미합니다. 에피소드가 처음 시작될 때, 어떤 상태에서 출발할지에 대한 확률 분포입니다.

---

### 다양한 강화학습 예시

추상적인 개념들을 실제 세계에 적용시켜 볼까요?

- **바둑 (Go)**: 상태 $S$는 현재 바둑판의 돌 배치 상황이고, 행동 $A$는 다음 돌을 놓을 수 있는 위치입니다.

- **로봇 팔 제어**: 상태 $S$는 로봇의 각 관절 각도나 카메라 이미지 같은 센서 정보가 될 수 있고, 행동 $A$는 각 관절에 가하는 힘(토크)이나 집게(gripper)를 여닫는 명령 등이 될 수 있습니다.

이처럼 상태와 행동은 문제에 따라 매우 다양하고 복잡한 형태로 정의될 수 있습니다.

---

### 미지의 MDP

여기서 강화학습의 가장 중요하고 현실적인 가정이 등장합니다. 바로, 대부분의 실제 문제에서 우리는 **MDP의 상세한 내용을 알지 못한다**는 것입니다.

특히, 환경이 어떻게 변할지에 대한 규칙, 즉 **상태 전이 확률 $p(s_{t+1}|s_t, a_t)$을 모릅니다**.

바나나 던지는 로봇 예시를 봅시다.

- $s_t$: 로봇과 바나나의 위치 및 속도
- $a_t$: 로봇 팔의 관절 제어
- $r_t$: 바나나가 통에 들어가면 +1
- $p(s_{t+1}|s_t, a_t)$: ?

이 전이 확률은 다름 아닌 **현실 세계의 물리 법칙** 그 자체입니다. 공기 저항, 중력, 바나나의 불규칙한 모양, 로봇 모터의 미세한 오차 등 모든 것이 포함된 아주 복잡한 함수입니다. 우리가 이 물리 법칙을 완벽하게 수식으로 모델링하는 것은 불가능합니다.

이처럼 '모델을 모르는' 상황에서 학습하는 것이 바로 강화학습의 핵심입니다. 에이전트는 이 미지의 환경과 직접 **상호작용**하며, 수많은 시도와 실패를 통해 최적의 정책을 스스로 터득해야 합니다.

---

### 실제 코드에서의 MDP

이러한 '미지의 환경' 개념이 실제 강화학습 코드에서는 어떻게 구현되는지 봅시다. OpenAI의 `gym`이라는 유명한 라이브러리를 사용한 예시 코드입니다.

```python
env = gym.make("Ant-v4")
s = env.reset()  # s_0
a = agent(s).sample()
s_next, r, done, _ = env.step(a)
```

**코드 설명:**

1. `env = gym.make("Ant-v4")`: 'Ant-v4'라는 환경 객체를 만듭니다. 우리는 `env` 객체 내부의 코드를 들여다볼 수 없습니다. 즉, 우리(에이전트)에게 환경의 전이 확률과 보상 함수는 **블랙박스**입니다.

2. `s = env.reset()`: 환경을 초기 상태 $s_0$로 리셋합니다.

3. `a = agent(s).sample()`: 우리의 정책(agent)이 현재 상태 `s`를 보고 행동 `a`를 결정합니다.

4. `s_next, r, done, _ = env.step(a)`: 에이전트가 `env`에 행동 `a`를 전달하면, `env` 내부의 (우리가 모르는) 물리 엔진이 다음 상태 `s_next`와 보상 `r`, 그리고 에피소드의 종료 여부 `done`을 계산해서 반환해 줍니다.

여기서 핵심은 마지막 문장입니다. **"에이전트는 'env.step(a)' 안에서 무슨 일이 일어나는지 모른다!"** 이것이 바로 모델을 모르는 강화학습의 현실을 정확히 보여주는 코드입니다.

---

### 부분 관찰 가능 MDP

지금까지 우리는 MDP가 '완전 관찰 가능(fully observable)'하다고 가정했습니다. 즉, 현재 상태 $s_t$가 미래를 예측하는 데 필요한 모든 정보를 담고 있다고 본 것입니다.

하지만 현실 세계는 대부분 **완전하게 관찰 가능하지 않습니다**. 부엌에서 '큰 그릇을 찾아라'라는 임무를 받은 로봇을 생각해봅시다. 로봇이 현재 보고 있는 카메라 화면은 부엌의 일부일 뿐, 부엌 전체의 상태가 아닙니다. 그릇은 닫힌 찬장 안에 있을 수도 있습니다.

이처럼 관찰(observation)이 실제 상태(state)의 일부 정보만을 담고 있는 문제를 **부분 관찰 가능 마르코프 결정 과정 (Partially Observable MDP, POMDP)** 이라고 부르며, 이는 더 어려운 문제로 나중에 다루게 될 것입니다.

---

### 다양한 강화학습 예시

MDP 프레임워크가 얼마나 다양한 문제에 적용될 수 있는지 몇 가지 예시를 더 살펴보겠습니다.

#### Ant

다리가 여러 개 달린 개미 로봇을 걷게 하는 시뮬레이션입니다. 상태 $S$는 각 관절의 각도, 속도 등 27차원의 연속적인 벡터이고, 행동 $A$는 8개 관절에 가하는 토크 값입니다. 보상 함수는 넘어지지 않고($healthy reward$), 앞으로 빠르게 갈수록($forward reward$) 좋지만, 에너지를 많이 쓰면($control cost$) 감점하는 방식으로 정교하게 설계되었습니다.

#### 리그 오브 레전드

비디오 게임은 매우 복잡한 RL 환경입니다. 상태 $S$는 게임 화면 픽셀 전체나 사운드 같은 고차원 데이터이고, 행동 $A$는 키보드와 마우스 입력입니다. 전이 확률 $p$는 게임 엔진과 다른 유저들의 행동에 의해 결정됩니다.

#### 휴머노이드 로봇

실제 로봇을 제어하는 것은 시뮬레이션보다 훨씬 어렵습니다. 전이 확률 $p$가 미지의 실제 물리 법칙이기 때문입니다. 상태 $S$는 카메라 이미지와 관절 센서 값들이고, 행동 $A$는 모터에 직접 보내는 전류나 토크 신호입니다.

#### 챗봇

최근 ChatGPT 등으로 각광받는 대화형 AI도 강화학습으로 성능을 향상시킵니다. 상태 $S$는 지금까지의 대화 내용(텍스트)이고, 행동 $A$는 챗봇의 다음 답변(텍스트)입니다. 여기서 환경의 반응 $p$는 사용자가 어떤 다음 말을 할지인데, 이는 당연히 알 수 없습니다. 보상 $r$은 사용자가 챗봇의 답변에 대해 '좋아요/싫어요' 같은 피드백을 주는 것으로부터 얻습니다. 이를 **인간 피드백 기반 강화학습 (RLHF)** 이라고 합니다.

---

## 정책 경사도 (Policy Gradient)

자, 지금까지 우리는 강화학습 문제를 수학적으로 정의하는 틀인 MDP에 대해 아주 자세하게 배웠습니다. 이제 우리는 '상태', '행동', '정책', '보상', '전이 확률' 같은 용어들을 명확히 이해하게 되었습니다.

이제 진짜 질문으로 넘어갈 시간입니다. "그래서, 이 MDP 환경 속에서 누적 보상을 최대화하는 최적의 정책 $\pi$를 **어떻게 찾을 것인가?**"

이 질문에 답하는 첫 번째 알고리즘으로, 오늘 강의의 핵심인 **정책 경사도 (Policy Gradient) 방법**에 대해 배워보겠습니다.

---

### 지도학습과의 비교

정책 경사도 방법의 아이디어를 이해하기 위해, 이 문제를 우리가 익숙한 지도학습(Supervised Learning, SL) 문제처럼 한번 생각해 봅시다.

우리의 목표는 누적 보상($\sum r_t$)을 최대화하는 정책을 찾는 것입니다. 우리의 모델은 파라미터 $\theta$로 이루어진 신경망 정책, $\pi_{\theta}(a_t|s_t)$ 입니다. 이 신경망은 상태 $s_t$를 입력받아 행동 $a_t$에 대한 확률을 출력합니다.

지도학습에서는 (입력, 정답) 쌍으로 이루어진 데이터셋이 있고, 모델의 예측과 정답 사이의 오차를 '손실 함수(Loss function)'로 정의한 뒤, 이 손실을 줄이는 방향으로 경사 하강법(gradient descent)을 사용해 파라미터 $\theta$를 업데이트합니다.

하지만 강화학습에서는 '정답 행동'이 없습니다. 우리에게 주어진 데이터는 그저 에이전트가 환경과 상호작용한 기록, 즉 궤적($\tau = (s_0, a_0, r_0), (s_1, a_1, r_1), ...$) 뿐입니다. 이 데이터로부터 어떻게 손실 함수를 정의하고, 정책 신경망 $\pi_{\theta}$를 업데이트할 **경사도 (Gradient) $\nabla_{\theta}$**를 얻을 수 있을까요?

---

### 목표 함수 정의

이 문제를 해결하기 위해, 우리는 먼저 최적화하려는 목표를 명확한 **목표 함수 (Objective function)** $J(\theta)$로 정의해야 합니다.

우리의 목표는 '기대 누적 보상'을 최대화하는 것입니다. 즉, 현재 정책 $\pi_{\theta}$를 따랐을 때 얻게 될 궤적들에서 받을 보상 총합의 기댓값을 최대로 만드는 $\theta$를 찾는 것입니다.

수식으로 표현하면 다음과 같습니다:

$$
\theta^* = \arg\max_{\theta} J(\theta) = \arg\max_{\theta} \mathbb{E}_{\tau \sim \pi_{\theta}(\tau)} \left[ \sum_t r(s_t, a_t) \right]
$$

- $\tau = (s_1, a_1, ..., s_T, a_T, s_{T+1})$: 하나의 궤적(trajectory)을 의미합니다.
- $\pi_{\theta}(\tau)$: 현재 정책 $\pi_{\theta}$를 따랐을 때, 특정 궤적 $\tau$가 나타날 확률입니다.
- $\mathbb{E}_{\tau \sim \pi_{\theta}(\tau)}[\cdot]$: 정책 $\pi_{\theta}$가 만들어내는 모든 가능한 궤적에 대해 기댓값을 구한다는 의미입니다.

이 목표 함수를 $\theta$에 대해 미분해서 경사도(gradient)를 구하고, 경사 상승법(gradient ascent)을 통해 $J(\theta)$를 점진적으로 높여나가는 것이 바로 정책 경사도 방법의 핵심 아이디어입니다.

---

### 정책 경사도 유도

자, 우리의 목표 함수는 다음과 같습니다:

$$
J(\theta) = \mathbb{E}_{\tau \sim \pi_{\theta}(\tau)}[\sum_t r(s_t, a_t)] = \mathbb{E}_{\tau \sim \pi_{\theta}(\tau)}[r(\tau)]
$$

여기서 $r(\tau)$는 궤적 $\tau$의 보상 총합을 줄여 쓴 것입니다. 이 식을 $\theta$에 대해 미분하고 싶습니다. 즉, $\nabla_{\theta}J(\theta)$를 구해야 합니다.

가장 단순한 생각은 미분 기호 $\nabla_{\theta}$를 기댓값 안으로 집어넣는 것이겠죠. 하지만 여기서 큰 문제가 발생합니다. 바로 **기댓값을 계산하는 확률분포 자체가 $\theta$에 의존한다**는 점입니다. 즉, $\tau \sim \pi_{\theta}(\tau)$ 이기 때문에, 정책이 바뀌면 궤적의 분포도 바뀌게 됩니다. 따라서 단순히 미분 기호를 안으로 넣을 수가 없습니다.

이 문제를 해결하기 위해 강화학습에서 가장 중요하고 아름다운 수학적 트릭 중 하나인 **'로그-미분 트릭 (Log-derivative Trick)'** 또는 '가능도 비율(Likelihood Ratio)' 트릭을 사용합니다.

어떤 함수 $p(x)$의 미분은 다음과 같이 변형할 수 있습니다:

$$
\nabla p(x) = p(x) \frac{\nabla p(x)}{p(x)} = p(x) \nabla \log p(x)
$$

이 관계를 우리의 목표 함수 미분에 적용해 봅시다.

1. 먼저 기댓값을 적분 형태로 풀어 씁니다:

$$
J(\theta) = \int \pi_{\theta}(\tau) r(\tau) d\tau
$$

2. 이제 양변을 $\theta$로 미분합니다:

$$
\nabla_{\theta} J(\theta) = \nabla_{\theta} \int \pi_{\theta}(\tau) r(\tau) d\tau = \int \nabla_{\theta} \pi_{\theta}(\tau) r(\tau) d\tau
$$

3. 여기서 $\nabla_{\theta} \pi_{\theta}(\tau)$에 로그-미분 트릭을 적용합니다:

$$
\nabla_{\theta} J(\theta) = \int \pi_{\theta}(\tau) \nabla_{\theta} \log \pi_{\theta}(\tau) r(\tau) d\tau
$$

4. 마지막으로, 이 적분 형태를 다시 기댓값 형태로 바꿉니다:

$$
\nabla_{\theta} J(\theta) = \mathbb{E}_{\tau \sim \pi_{\theta}(\tau)}[\nabla_{\theta} \log \pi_{\theta}(\tau) r(\tau)]
$$

놀라운 결과가 나왔습니다! 원래는 분포 때문에 미분할 수 없었던 식이, 이제는 **원래의 분포 $\pi_{\theta}(\tau)$에 대한 기댓값** 형태로 바뀌었습니다. 이것이 왜 중요하냐면, 이제 우리는 이 기댓값을 **샘플링**을 통해 근사할 수 있게 되었기 때문입니다. 즉, 현재 정책 $\pi_{\theta}$를 사용해 여러 궤적 $\tau$를 생성해보고, 그 궤적들에서 나온 값을 평균 내어 경사도를 추정할 수 있는 길이 열린 것입니다.

---

### 궤적 확률의 미분

자, 그럼 방금 유도한 식의 핵심 부분인 $\nabla_{\theta} \log \pi_{\theta}(\tau)$는 구체적으로 어떻게 계산할 수 있을까요?

먼저 궤적 $\tau$가 나타날 확률 $\pi_{\theta}(\tau)$는 다음과 같이 구성됩니다:

$$
\pi_{\theta}(\tau) = p(s_1) \prod_{t=1}^{T} \pi_{\theta}(a_t|s_t) p(s_{t+1}|s_t, a_t)
$$

- $p(s_1)$: 초기 상태 분포
- $\pi_{\theta}(a_t|s_t)$: **정책**. 상태 $s_t$에서 행동 $a_t$를 할 확률. **이 부분만 $\theta$에 의존합니다.**
- $p(s_{t+1}|s_t, a_t)$: **환경의 동역학**. 상태 $s_t$, 행동 $a_t$ 이후 다음 상태 $s_{t+1}$이 나올 확률.

이제 양변에 로그를 취하면, 곱셈이 덧셈으로 바뀝니다:

$$
\log \pi_{\theta}(\tau) = \log p(s_1) + \sum_{t=1}^{T} \log \pi_{\theta}(a_t|s_t) + \sum_{t=1}^{T} \log p(s_{t+1}|s_t, a_t)
$$

이제 이 식을 우리가 궁금해하는 $\theta$에 대해 미분($\nabla_{\theta}$)해봅시다. $\log p(s_1)$와 $\log p(s_{t+1}|s_t, a_t)$ 항들은 $\theta$와 아무런 관련이 없으므로 미분하면 0이 되어 사라집니다. 그 결과, 아주 깔끔한 형태만 남게 됩니다:

$$
\nabla_{\theta} \log \pi_{\theta}(\tau) = \sum_{t=1}^{T} \nabla_{\theta} \log \pi_{\theta}(a_t|s_t)
$$

이것은 정책 경사도 방법의 또 다른 강력한 장점을 보여줍니다. 경사도를 계산하는 데 **환경의 동역학($p(s_{t+1}|s_t, a_t)$)에 대한 정보가 전혀 필요 없다**는 것입니다! 에이전트는 환경이 어떻게 돌아가는지 전혀 몰라도, 오직 자신의 정책과 그 결과로 받은 보상만으로 학습을 진행할 수 있습니다. 이를 **모델-프리 (Model-Free)** 방식이라고 부릅니다.

---

### REINFORCE 알고리즘

이제 앞선 두 슬라이드의 결과를 합쳐봅시다. 최종적인 정책 경사도(Policy Gradient) 공식은 다음과 같습니다:

$$
\nabla_{\theta} J(\theta) = \mathbb{E}_{\tau \sim \pi_{\theta}(\tau)} \left[ \left( \sum_{t=1}^{T} \nabla_{\theta} \log \pi_{\theta}(a_t|s_t) \right) \left( \sum_{t=1}^{T} r(s_t, a_t) \right) \right]
$$

이 수식의 의미를 해석해 볼까요?

- $(\sum r(s_t, a_t))$: 한 궤적의 **보상 총합**입니다. 이 값이 크면 좋은 궤적, 작으면 나쁜 궤적이라는 의미입니다.
- $(\sum \nabla_{\theta} \log \pi_{\theta}(a_t|s_t))$: 그 궤적에 포함된 모든 행동들에 대해, 해당 행동이 나올 **로그 확률을 높이는 방향**의 벡터입니다.

따라서 이 둘을 곱한다는 것은, **"좋은 궤적(보상이 높은)에 있었던 행동들의 확률은 높이고, 나쁜 궤적(보상이 낮은)에 있었던 행동들의 확률은 낮추도록 정책 $\theta$를 업데이트하라"** 는 아주 직관적인 의미를 갖습니다.

실제로는 이 기댓값을 정확히 계산할 수 없으므로, **몬테카를로 샘플링**으로 근사합니다. 즉, 현재 정책 $\pi_{\theta}$로 $N$개의 궤적을 샘플링하고, 그 결과를 평균 내어 경사도를 추정하는 것입니다.

이 원리를 그대로 알고리즘으로 만든 것이 바로 **REINFORCE 알고리즘**입니다.

1. **샘플링**: 현재 정책 $\pi_{\theta}(a_t|s_t)$를 사용해 환경과 상호작용하며 $N$개의 궤적 $\tau^1, \tau^2, ..., \tau^N$을 수집합니다.

2. **경사도 계산**: 수집된 궤적들을 이용해 경사도를 근사 계산합니다:

$$
\nabla_{\theta} J(\theta) \approx \frac{1}{N} \sum_{i=1}^{N} \left( \sum_{t=1}^{T} \nabla_{\theta} \log \pi_{\theta}(a_{i,t}|s_{i,t}) \right) \left( \sum_{t=1}^{T} r(s_{i,t}, a_{i,t}) \right)
$$

3. **파라미터 업데이트**: 경사 상승법을 이용해 정책 파라미터를 업데이트합니다. (손실을 최소화하는 것이 아니라 보상을 최대화하는 것이므로, 경사도를 더해줍니다.)

$$
\theta \leftarrow \theta + \alpha \nabla_{\theta} J(\theta)
$$

여기서 $\alpha$는 학습률(learning rate)입니다.

이 세 단계를 계속 반복하면 정책은 점진적으로 더 높은 보상을 받는 방향으로 개선됩니다.

#### REINFORCE 알고리즘 흐름도

이 REINFORCE 알고리즘의 전체적인 흐름을 그림으로 보면 더 명확하게 이해할 수 있습니다.

1. **정책 실행 (Roll-out policy)**: 현재의 정책 $\pi_{\theta}$를 가지고 실제 환경에서 에피소드를 여러 번(N번) 끝까지 진행시켜 봅니다. 이 과정에서 $(s_1, a_1, r_1), (s_2, a_2, r_2), ...$ 와 같은 데이터 궤적들이 수집됩니다.

2. **반환값 추정 및 모델 학습 (Estimate return / Fit model)**: 수집된 궤적들 각각에 대해 보상의 총합, 즉 반환값(return) $r(\tau^i)$을 계산합니다. 그리고 이 반환값과 로그 확률의 경사도를 이용해 최종 정책 경사도 $\nabla_{\theta}J(\theta)$를 계산합니다.

3. **정책 개선 (Improve policy)**: 계산된 경사도의 방향으로 정책의 파라미터 $\theta$를 조금 업데이트합니다.

그리고 개선된 정책을 가지고 다시 1번으로 돌아가 이 과정을 무한히 반복하는 것입니다.

---

### 정책 경사도 방법 요약

자, 지금까지 배운 정책 경사도 방법의 장단점을 정리해 봅시다.

#### 장점

- 우리가 진짜로 최적화하고 싶은 목표, 즉 '기대 누적 보상'을 직접 미분하여 경사 상승법을 적용하기 때문에 이론적으로 수렴성이 보장됩니다.
- 알고리즘의 원리가 "보상이 높았던 행동은 더 자주 하고, 보상이 낮았던 행동은 덜 하라"는 것으로 매우 직관적입니다.

#### 단점

- **높은 분산 (High Variance)**: 경사도를 샘플링을 통해 추정하기 때문에, 어떤 궤적이 샘플링되느냐에 따라 추정된 경사도의 값이 매우 크게 변동합니다. 운 좋게 보상 높은 궤적만 나오면 경사도가 너무 커지고, 운 나쁘게 보상 낮은 궤적만 나오면 경사도가 너무 작아지거나 방향이 틀어질 수 있습니다. 이는 학습을 매우 불안정하게 만듭니다.

- **On-policy 데이터 요구**: 경사도 유도 과정에서 현재 정책 $\pi_{\theta}$에 대한 기댓값을 사용했습니다. 이는 경사도를 계산할 때 사용되는 데이터가 반드시 **현재 정책 $\pi_{\theta}$로부터 수집**되어야 함을 의미합니다. 한번 업데이트하고 나면 이전 정책으로 수집한 데이터는 모두 버려야 하므로, 데이터 효율성이 매우 떨어집니다.

오늘 강의의 후반부는 바로 이 '높은 분산'이라는 치명적인 단점을 어떻게 해결할 것인지에 초점을 맞출 것입니다.

---

## 분산 감소 (Variance Reduction)

### 높은 분산의 문제

정책 경사도 학습이 불안정한 가장 큰 이유인 '분산' 문제에 대해 좀 더 깊이 파고들어 봅시다.

우리가 샘플링을 통해 추정하려는 진짜 정책 경사도를 $\mathbb{E}[X]$라고 합시다. 그리고 우리가 N개의 궤적 샘플을 뽑아 계산한 경사도 추정치를 $\bar{X}$라고 합시다.

- **분산이 높은 경우**: 샘플들이 진짜 평균 주변에 넓게 흩어져 있습니다. 만약 우리가 운 나쁘게 극단적인 샘플 몇 개만 뽑았다면, 우리의 경사도 추정치 $\bar{X}$는 실제 경사도 $\mathbb{E}[X]$와는 전혀 다른, 심지어 방향이 반대일 수도 있는 값을 가리키게 됩니다. 이런 잘못된 경사도로 파라미터를 업데이트하면 정책은 오히려 나빠지게 되겠죠. 학습이 지그재그로 진행되거나, 아예 수렴하지 못할 수도 있습니다.

- **분산이 낮은 경우**: 샘플들이 진짜 평균 주변에 옹기종기 모여 있습니다. 이 경우, 어떤 샘플을 뽑더라도 그 평균인 $\bar{X}$는 실제 평균 $\mathbb{E}[X]$와 매우 유사할 가능성이 높습니다. 따라서 우리는 더 안정적이고 빠르게 올바른 방향으로 정책을 개선해 나갈 수 있습니다.

결론적으로, 정책 경사도의 분산을 줄이는 것은 안정적이고 효율적인 학습을 위해 매우 중요합니다.

---

### 분산 감소 방법

그렇다면 이 높은 분산을 어떻게 줄일 수 있을까요? 두 가지 해결책이 있습니다.

#### 해결책 1: N을 아주 크게 한다 (더 많은 샘플 사용)

대수의 법칙에 따라, 샘플의 개수 N이 많아지면 샘플 평균은 실제 평균에 수렴합니다. 따라서 이론적으로는 궤적을 수백만 개씩 수집하면 분산 문제를 해결할 수 있습니다.

하지만 이는 현실적인 해결책이 아닙니다. 실제 로봇을 수백만 번 움직여 데이터를 수집하는 것은 시간과 비용 측면에서 거의 불가능하며, 그렇게 많은 데이터를 모아도 고작 단 한 번의 그래디언트 업데이트에만 사용하고 버려야 하므로 극도로 비효율적입니다.

#### 해결책 2: 분산이 낮은 추정량 (Estimator)을 사용한다

이것이 더 똑똑하고 실용적인 접근법입니다. 경사도 공식 자체를 수학적으로 변형하여, **기댓값은 동일하게 유지하면서 분산만 낮추는** 새로운 추정량을 만드는 것입니다.

---

### 개선된 추정량

자, 이제 정책 경사도 연구의 핵심이라고 할 수 있는, '어떻게 더 좋은 추정량을 만들 것인가'에 대한 여정을 시작하겠습니다.

우리의 원래 경사도 추정량은 $\nabla_{\theta} \log \pi_{\theta}(a_t|s_t)$에 어떤 '보상 값'을 곱하는 형태였습니다. 이 '보상 값'을 어떻게 바꾸느냐에 따라 분산이 크게 달라집니다.

#### 1. 기본: 전체 보상의 합 (Sum of rewards)

REINFORCE 알고리즘은 궤적 전체의 보상 합 $\sum_{t'=1}^{T} r(s_{t'}, a_{t'})$을 사용했습니다. 하지만 여기서 중요한 통찰이 있습니다. **시간 t에서의 행동 $a_t$는 그 이전에 받았던 보상 $r_1, ..., r_{t-1}$에 영향을 줄 수 없습니다.** 즉, 인과관계(causality)가 성립하지 않습니다. 그런데도 과거의 보상을 곱해주는 것은 아무 의미 없이 노이즈(분산)만 더하는 꼴이 됩니다.

#### 2. 개선 1: 미래 보상의 합 (Reward-to-go)

이 문제를 해결하기 위해, 행동 $a_t$를 평가할 때 **그 행동이 일어난 시점 t부터 끝까지의 보상 합**만 사용하는 것입니다. 이를 **'미래 보상 (Reward-to-go)'** 이라고 부릅니다.

$$
\hat{Q}_t = \sum_{t'=t}^{T} r(s_{t'}, a_{t'})
$$

이렇게 하면 인과관계를 만족시키면서 불필요한 노이즈를 제거하여 분산을 크게 줄일 수 있습니다.

#### 3. 개선 2: 더 나은 미래 보상 추정 (Q-function)

한 번의 궤적에서 얻은 미래 보상 합($\hat{Q}_t$)은 여전히 단 하나의 샘플일 뿐이라서 여전히 노이즈가 많습니다. 더 좋은 방법은, 상태 $s_t$에서 행동 $a_t$를 했을 때 앞으로 받을 것으로 **기대되는** 미래 보상의 총합, 즉 **Q-함수 $Q^{\pi}(s_t, a_t)$**를 사용하는 것입니다.

이 Q-함수를 신경망(보통 '비평가(Critic)'라고 부름)으로 근사하여 사용하면, 단일 샘플의 무작위성을 줄이고 더 안정적인 추정치를 얻을 수 있습니다. 이것이 바로 **액터-크리틱 (Actor-Critic) 알고리즘**의 시작입니다.

#### 4. 최종 개선: 어드밴티지 (Advantage)

여기서 한 걸음 더 나아갑니다. 어떤 상태에서 모든 행동들의 Q값이 전반적으로 높거나 낮을 수 있습니다. 예를 들어, 게임에서 거의 이긴 상태라면 어떤 행동을 하든 높은 보상이 기대되겠죠. 정말로 중요한 것은 이 행동 $a_t$가 그 상태에서 **평균적으로 가능한 다른 행동들보다 얼마나 더 좋은가** 입니다.

이 '상대적인 좋음'을 측정하는 것이 바로 **어드밴티지 함수 (Advantage Function)** 입니다.

$$
A^{\pi}(s_t, a_t) = Q^{\pi}(s_t, a_t) - V^{\pi}(s_t)
$$

- $V^{\pi}(s_t)$: 상태 가치 함수 (State Value Function). 상태 $s_t$가 평균적으로 얼마나 좋은지를 나타냅니다.

이 어드밴티지 값을 사용하면, 평균보다 좋은 행동은 양수 값을, 평균보다 나쁜 행동은 음수 값을 갖게 되어 경사도의 방향을 더 명확하고 안정적으로 알려줍니다. 이는 분산을 줄이는 매우 효과적인 **베이스라인 (Baseline)** 기법으로, 현대적인 정책 경사도 알고리즘의 표준적인 구성 요소가 되었습니다.



