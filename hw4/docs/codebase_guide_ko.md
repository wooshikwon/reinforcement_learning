# GCB6206 Homework 4 ì½”ë“œë² ì´ìŠ¤ êµ¬ì¡° ê°€ì´ë“œ
## Soft Actor-Critic êµ¬í˜„ ì´í•´ë¥¼ ìœ„í•œ ì´ˆë³´ì ê°€ì´ë“œ

---

## ëª©ì°¨

1. [ì†Œê°œ](#ì†Œê°œ)
2. [ê°•í™”í•™ìŠµ ê¸°ì´ˆ](#ê°•í™”í•™ìŠµ-ê¸°ì´ˆ)
3. [Soft Actor-Critic (SAC) ê°œìš”](#soft-actor-critic-sac-ê°œìš”)
4. [í”„ë¡œì íŠ¸ êµ¬ì¡°](#í”„ë¡œì íŠ¸-êµ¬ì¡°)
5. [í•µì‹¬ ì»´í¬ë„ŒíŠ¸ Deep Dive](#í•µì‹¬-ì»´í¬ë„ŒíŠ¸-deep-dive)
6. [ë°ì´í„° íë¦„ê³¼ Training Loop](#ë°ì´í„°-íë¦„ê³¼-training-loop)
7. [í•µì‹¬ ê°œë…ê³¼ ë””ìì¸ íŒ¨í„´](#í•µì‹¬-ê°œë…ê³¼-ë””ìì¸-íŒ¨í„´)
8. [ì½”ë“œë² ì´ìŠ¤ íƒìƒ‰ ë°©ë²•](#ì½”ë“œë² ì´ìŠ¤-íƒìƒ‰-ë°©ë²•)

---

## ì†Œê°œ

### ì´ í”„ë¡œì íŠ¸ëŠ” ë¬´ì—‡ì¸ê°€ìš”?

ì´ ì½”ë“œë² ì´ìŠ¤ëŠ” continuous control ì‘ì—…ì„ ìœ„í•œ ê°€ì¥ ì„±ê³µì ì¸ ë”¥ ê°•í™”í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ ì¤‘ í•˜ë‚˜ì¸ **Soft Actor-Critic (SAC)**ì„ êµ¬í˜„í•©ë‹ˆë‹¤. ë¡œë´‡ íŒ”ì´ ëª©í‘œì— ë„ë‹¬í•˜ë„ë¡ í›ˆë ¨í•˜ê±°ë‚˜, íœ´ë¨¸ë…¸ì´ë“œê°€ ê±·ë„ë¡ ê°€ë¥´ì¹˜ê±°ë‚˜, ì‹œë®¬ë ˆì´ì…˜ëœ ì¹˜íƒ€ê°€ ë‹¬ë¦¬ë„ë¡ ì œì–´í•˜ëŠ” ë“±ì˜ ì‘ì—…ì„ ìœ„í•´ agentë¥¼ í›ˆë ¨í•˜ë„ë¡ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤.

### ì´ ê°€ì´ë“œëŠ” ëˆ„êµ¬ë¥¼ ìœ„í•œ ê²ƒì¸ê°€ìš”?

- ê°•í™”í•™ìŠµ ì½”ë“œë¥¼ ì´í•´í•˜ê³  ì‹¶ì€ **Python ì´ˆë³´ì**
- ì´ë¡ ì  ê°œë…ì´ ì½”ë“œë¡œ ì–´ë–»ê²Œ ë³€í™˜ë˜ëŠ”ì§€ ë³´ê³  ì‹¶ì€ **RL ì´ˆë³´ì**
- ê³¼ì œë¥¼ ìˆ˜í–‰í•˜ëŠ” **í•™ìƒë“¤**
- í˜„ëŒ€ RL ì•Œê³ ë¦¬ì¦˜ì´ ì–´ë–»ê²Œ êµ¬í˜„ë˜ëŠ”ì§€ ê¶ê¸ˆí•œ **ëª¨ë“  ì‚¬ëŒ**

### ë¬´ì—‡ì„ ë°°ìš°ê²Œ ë˜ë‚˜ìš”:

1. RL ê°œë…(state, action, reward)ì´ ì½”ë“œë¡œ ë§¤í•‘ë˜ëŠ” ë°©ë²•
2. RL ì½”ë“œë² ì´ìŠ¤ì˜ êµ¬ì¡°ì™€ ì¡°ì§
3. Actor-critic ë°©ë²•ì—ì„œ ì‹ ê²½ë§ì´ ì‚¬ìš©ë˜ëŠ” ë°©ë²•
4. RL êµ¬í˜„ì„ ìœ„í•œ ëª¨ë²” ì‚¬ë¡€

---

## ê°•í™”í•™ìŠµ ê¸°ì´ˆ

### í° ê·¸ë¦¼

ë¡œë´‡ ê°œë¥¼ ê±·ë„ë¡ í›ˆë ¨í•œë‹¤ê³  ìƒìƒí•´ ë³´ì„¸ìš”:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Environment â”‚ â† ì‹œë®¬ë ˆì´ì…˜ëœ ì„¸ê³„ (ë¬¼ë¦¬, ì¤‘ë ¥ ë“±)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â†“ observation (state)
      â†‘ action
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Agent    â”‚ â† ë¬´ì—‡ì„ í• ì§€ ê²°ì •í•˜ëŠ” "ë‘ë‡Œ"
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**í•™ìŠµ ë£¨í”„:**
1. AgentëŠ” í˜„ì¬ **state**ë¥¼ ê´€ì°° (ê´€ì ˆ ê°ë„, ì†ë„)
2. AgentëŠ” **action**ì„ ì„ íƒ (ëª¨í„° í† í¬)
3. EnvironmentëŠ” ìƒˆë¡œìš´ **state**ì™€ **reward**ë¡œ ì‘ë‹µ
4. AgentëŠ” ì‹œê°„ì— ë”°ë¥¸ ì´ rewardë¥¼ ìµœëŒ€í™”í•˜ë„ë¡ í•™ìŠµ

### í•µì‹¬ RL ê°œë…

#### 1. Markov Decision Process (MDP)

MDPëŠ” RLì„ ìœ„í•œ ìˆ˜í•™ì  í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤:

- **State (s)**: Environmentì˜ ì™„ì „í•œ ì„¤ëª…
  - ì˜ˆ: [position, velocity, angle, angular_velocity]
- **Action (a)**: Agentê°€ í•  ìˆ˜ ìˆëŠ” ê²ƒ
  - ì´ì‚°: {left, right, jump}
  - ì—°ì†: [motor_1_torque, motor_2_torque, ...]
- **Reward (r)**: ìŠ¤ì¹¼ë¼ í”¼ë“œë°± ì‹ í˜¸
  - ì˜ˆ: ë˜‘ë°”ë¡œ ì„œ ìˆìœ¼ë©´ +1, ë„˜ì–´ì§€ë©´ -100
- **Transition**: p(s'|s,a) - ë‹¤ìŒ stateì˜ í™•ë¥ 
- **Policy (Ï€)**: Ï€(a|s) - agentì˜ ì „ëµ (state â†’ action)

#### 2. Value Function

**Q-function (Action-Value)**: Q(s, a) = state sì—ì„œ ì‹œì‘í•˜ì—¬ action aë¥¼ ì·¨í•œ í›„ policy Ï€ë¥¼ ë”°ë¥¼ ë•Œ ì˜ˆìƒë˜ëŠ” ì´ reward

```
Q(s, a) = E[r_0 + Î³r_1 + Î³Â²r_2 + Î³Â³r_3 + ...]
```

ì—¬ê¸°ì„œ Î³ (gamma)ëŠ” discount factorì…ë‹ˆë‹¤ (0 < Î³ < 1):
- Î³ = 0.99: ì¥ê¸° rewardë¥¼ ê³ ë ¤
- Î³ = 0.0: ì¦‰ê°ì ì¸ rewardë§Œ ê³ ë ¤

**Q-functionì„ ì‚¬ìš©í•˜ëŠ” ì´ìœ ?**
ëª¨ë“  actionì— ëŒ€í•œ Q(s, a)ë¥¼ ì•Œë©´ ìµœì„ ì˜ actionì„ ì„ íƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:
```
Ï€*(s) = argmax_a Q(s, a)
```

#### 3. Policy Gradient ë°©ë²•

Q-valueë¥¼ í•™ìŠµí•˜ëŠ” ëŒ€ì‹ , policy Ï€_Î¸(a|s)ë¥¼ ì§ì ‘ í•™ìŠµ:

**ì•„ì´ë””ì–´**: ì˜ˆìƒ rewardë¥¼ ì¦ê°€ì‹œí‚¤ë„ë¡ policy íŒŒë¼ë¯¸í„° Î¸ ì¡°ì •

```
âˆ‡_Î¸ J(Î¸) = E[âˆ‡_Î¸ log Ï€_Î¸(a|s) Â· Q(s,a)]
```

**ì§ê´€**: Action aê°€ ì¢‹ì€ Q-valueë¡œ ì´ì–´ì¡Œë‹¤ë©´, ë” ê°€ëŠ¥ì„± ìˆê²Œ ë§Œë“¤ê¸°!

#### 4. Actor-Critic ë°©ë²•

ë‘ ì ‘ê·¼ë²• ê²°í•©:

- **Actor (Policy)**: Ï€_Î¸(a|s) - ì–´ë–¤ actionì„ ì·¨í• ì§€ ê²°ì •
- **Critic (Value)**: Q_Ï†(s, a) - actionì´ ì–¼ë§ˆë‚˜ ì¢‹ì€ì§€ í‰ê°€

**ì¥ì :**
- ìˆœìˆ˜ policy gradientë³´ë‹¤ ë‚®ì€ ë¶„ì‚°
- ìˆœìˆ˜ value ë°©ë²•ë³´ë‹¤ íš¨ìœ¨ì 
- Continuous actionì— ì˜ ì‘ë™

---

## Soft Actor-Critic (SAC) ê°œìš”

### SACë¥¼ "Soft"í•˜ê²Œ ë§Œë“œëŠ” ê²ƒì€?

**í‘œì¤€ RL ëª©í‘œ**: ì´ reward ìµœëŒ€í™”
```
max E[âˆ‘ Î³^t r_t]
```

**SAC ëª©í‘œ**: Reward + entropy ìµœëŒ€í™”
```
max E[âˆ‘ Î³^t (r_t + Î±Â·H(Ï€(Â·|s_t)))]
```

ì—¬ê¸°ì„œ H(Ï€)ëŠ” entropyì…ë‹ˆë‹¤: H(Ï€) = -E[log Ï€(a|s)]

**ì™œ entropy?**
- **íƒìƒ‰**: ë‹¤ì–‘í•œ action ì‹œë„ë¥¼ ì¥ë ¤
- **ê°•ê±´ì„±**: ì‘ì—…ì„ í•´ê²°í•˜ëŠ” ì—¬ëŸ¬ ë°©ë²• í•™ìŠµ
- **ë¶•ê´´ ë°©ì§€**: ì¡°ê¸° ìˆ˜ë ´ ë°©ì§€

### SAC ì•Œê³ ë¦¬ì¦˜ êµ¬ì„±ìš”ì†Œ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           SAC Agent                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚   Actor    â”‚    â”‚   Critics    â”‚ â”‚
â”‚  â”‚  Ï€_Î¸(a|s)  â”‚    â”‚  Q_Ï†(s,a)    â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚        â”‚                  â”‚          â”‚
â”‚        â†“                  â†“          â”‚
â”‚   Sample action      Evaluate Q     â”‚
â”‚                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚   Target Critics (ì²œì²œíˆ    â”‚   â”‚
â”‚  â”‚   ì—…ë°ì´íŠ¸ë˜ëŠ” critic ë³µì‚¬ë³¸) â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**í•µì‹¬ íŠ¹ì§•:**
1. **Off-policy**: ê³¼ê±° ê²½í—˜ì—ì„œ í•™ìŠµ (replay buffer)
2. **Maximum entropy**: Entropy bonus ì¶”ê°€
3. **Twin critics**: ê³¼ëŒ€í‰ê°€ í¸í–¥ ê°ì†Œ
4. **Continuous action**: ì‹¤ìˆ˜ê°’ action ì²˜ë¦¬

### SAC í›ˆë ¨ ì•Œê³ ë¦¬ì¦˜

```
ë°˜ë³µ:
    1. Action ìƒ˜í”Œë§: a ~ Ï€_Î¸(Â·|s)
    2. Action ì‹¤í–‰, ê´€ì°° (s, a, r, s', done)
    3. Replay bufferì— ì €ì¥

    4. Replay bufferì—ì„œ batch ìƒ˜í”Œë§

    5. Critic ì—…ë°ì´íŠ¸:
       - Target ê³„ì‚°: y = r + Î³(Q'(s',a') + Î±Â·H(Ï€(Â·|s')))
       - ìµœì†Œí™”: (Q_Ï†(s,a) - y)Â²

    6. Actor ì—…ë°ì´íŠ¸:
       - ìµœëŒ€í™”: Q_Ï†(s, Ï€_Î¸(s)) + Î±Â·H(Ï€_Î¸(Â·|s))

    7. Target critic ì—…ë°ì´íŠ¸ (ì²œì²œíˆ)
```

---

## í”„ë¡œì íŠ¸ êµ¬ì¡°

### ë””ë ‰í† ë¦¬ ë ˆì´ì•„ì›ƒ

```
hw4/
â”œâ”€â”€ gcb6206/                      # ë©”ì¸ íŒ¨í‚¤ì§€
â”‚   â”œâ”€â”€ agents/                   # Agent êµ¬í˜„ì²´
â”‚   â”‚   â””â”€â”€ sac_agent.py         # SAC ì•Œê³ ë¦¬ì¦˜
â”‚   â”œâ”€â”€ networks/                 # ì‹ ê²½ë§ ì•„í‚¤í…ì²˜
â”‚   â”‚   â”œâ”€â”€ mlp_policy.py        # Actor network (policy)
â”‚   â”‚   â””â”€â”€ state_action_value_critic.py  # Critic network
â”‚   â”œâ”€â”€ infrastructure/           # ìœ í‹¸ë¦¬í‹°ì™€ í—¬í¼
â”‚   â”‚   â”œâ”€â”€ replay_buffer.py     # Experience replay
â”‚   â”‚   â”œâ”€â”€ logger.py            # TensorBoard ë¡œê¹…
â”‚   â”‚   â”œâ”€â”€ pytorch_util.py      # PyTorch í—¬í¼
â”‚   â”‚   â”œâ”€â”€ distributions.py     # ì»¤ìŠ¤í…€ ë¶„í¬
â”‚   â”‚   â””â”€â”€ utils.py             # ì¼ë°˜ ìœ í‹¸ë¦¬í‹°
â”‚   â”œâ”€â”€ env_configs/             # Environment ì„¤ì •
â”‚   â”‚   â”œâ”€â”€ sac_config.py        # SAC í•˜ì´í¼íŒŒë¼ë¯¸í„°
â”‚   â”‚   â””â”€â”€ schedule.py          # Learning rate ìŠ¤ì¼€ì¤„
â”‚   â””â”€â”€ scripts/                 # ì§„ì…ì 
â”‚       â”œâ”€â”€ run_hw4.py           # ë©”ì¸ í›ˆë ¨ ìŠ¤í¬ë¦½íŠ¸
â”‚       â””â”€â”€ scripting_utils.py   # Config ë¡œë”©
â”œâ”€â”€ experiments/                  # ì‹¤í—˜ ì„¤ì •
â”‚   â””â”€â”€ sac/                     # SAC ì‹¤í—˜ YAML íŒŒì¼
â”‚       â”œâ”€â”€ sanity_pendulum_1.yaml
â”‚       â”œâ”€â”€ halfcheetah_reinforce1.yaml
â”‚       â””â”€â”€ ...
â”œâ”€â”€ docs/                        # ë¬¸ì„œ
â”œâ”€â”€ requirements.txt             # Python ì˜ì¡´ì„±
â””â”€â”€ setup.py                     # íŒ¨í‚¤ì§€ ì„¤ì¹˜
```

### ì„¤ê³„ ì² í•™

**ê´€ì‹¬ì‚¬ì˜ ë¶„ë¦¬ (Separation of Concerns):**
- **agents/**: ì•Œê³ ë¦¬ì¦˜ ë¡œì§ (ë¬´ì—‡ì„ í•™ìŠµí• ì§€)
- **networks/**: ì‹ ê²½ë§ ì•„í‚¤í…ì²˜ (í•¨ìˆ˜ ê·¼ì‚¬ê¸°)
- **infrastructure/**: ì¬ì‚¬ìš© ê°€ëŠ¥í•œ ì»´í¬ë„ŒíŠ¸ (replay buffer, ë¡œê¹…)
- **env_configs/**: í•˜ì´í¼íŒŒë¼ë¯¸í„° (ì–´ë–»ê²Œ í•™ìŠµí• ì§€)
- **scripts/**: ì‹¤í–‰ ë¡œì§ (ì–¸ì œ ë¬´ì—‡ì„ í• ì§€)

---

## í•µì‹¬ ì»´í¬ë„ŒíŠ¸ Deep Dive

### 1. Agent: `gcb6206/agents/sac_agent.py`

**ëª©ì **: SAC í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ êµ¬í˜„

**í´ë˜ìŠ¤**: `SoftActorCritic`

#### í•µì‹¬ ì†ì„±

```python
class SoftActorCritic(nn.Module):
    def __init__(...):
        # Actor: Ï€_Î¸(a|s) - policy network
        self.actor = make_actor(observation_shape, action_dim)

        # Critics: Q_Ï†(s,a) - value networkë“¤ (ì—¬ëŸ¬ ê°œ ê°€ëŠ¥)
        self.critics = nn.ModuleList([
            make_critic(observation_shape, action_dim)
            for _ in range(num_critic_networks)
        ])

        # Target critics: Q_Ï†'(s,a) - ì²œì²œíˆ ì—…ë°ì´íŠ¸ë˜ëŠ” ë³µì‚¬ë³¸
        self.target_critics = nn.ModuleList([...])

        # Optimizer
        self.actor_optimizer = make_actor_optimizer(...)
        self.critic_optimizer = make_critic_optimizer(...)

        # í•˜ì´í¼íŒŒë¼ë¯¸í„°
        self.discount = discount  # Î³ (gamma)
        self.temperature = temperature  # Î± (alpha) for entropy
```

#### í•µì‹¬ ë©”ì„œë“œ

**`get_action(observation)`**
```python
def get_action(self, observation: np.ndarray) -> np.ndarray:
    """Environmentì—ì„œ ì‹¤í–‰í•  action ì„ íƒ"""
    # 1. numpy â†’ torch ë³€í™˜
    # 2. Policy distribution Ï€(Â·|s) ê°€ì ¸ì˜¤ê¸°
    # 3. Action ìƒ˜í”Œë§ a ~ Ï€(Â·|s)
    # 4. torch â†’ numpy ë³€í™˜
```

**`update_critic(obs, action, reward, next_obs, done)`**
```python
def update_critic(...):
    """Critic networkì— ëŒ€í•œ í•œ ë²ˆì˜ gradient step"""
    # 1. ë‹¤ìŒ action ìƒ˜í”Œë§: a' ~ Ï€(Â·|s')
    # 2. Target ê³„ì‚°: y = r + Î³(Q'(s',a') + Î±Â·H(Ï€(Â·|s')))
    # 3. Q-value ê³„ì‚°: Q(s,a)
    # 4. Loss ê³„ì‚°: MSE(Q, y)
    # 5. Backpropê³¼ ì—…ë°ì´íŠ¸
```

**`update_actor(obs)`**
```python
def update_actor(obs):
    """Actor networkì— ëŒ€í•œ í•œ ë²ˆì˜ gradient step"""
    # ë‘ ê°€ì§€ ë³€í˜•:
    # REINFORCE: âˆ‡ log Ï€(a|s) Â· Q(s,a)
    # REPARAMETRIZE: âˆ‡ Q(s, Ï€(s))
```

**`update(observations, actions, rewards, next_observations, dones, step)`**
```python
def update(...):
    """ë©”ì¸ ì—…ë°ì´íŠ¸: critic + actor + target network"""
    # 1. Critic ì—…ë°ì´íŠ¸ (ì—¬ëŸ¬ ë²ˆ)
    # 2. Actor ì—…ë°ì´íŠ¸ (í•œ ë²ˆ)
    # 3. Target network ì—…ë°ì´íŠ¸
```

---

### 2. Actor Network: `gcb6206/networks/mlp_policy.py`

**ëª©ì **: Actionì— ëŒ€í•œ í™•ë¥  ë¶„í¬ë¥¼ ì¶œë ¥í•˜ëŠ” ì‹ ê²½ë§

**í´ë˜ìŠ¤**: `MLPPolicy`

#### ì•„í‚¤í…ì²˜

```
ì…ë ¥: state (observation)
    â†“
[Linear Layer â†’ Activation] Ã— n_layers
    â†“
Output Layer
    â†“
Continuous actionì˜ ê²½ìš°:
    - Mean: Î¼(s)
    - Std: Ïƒ(s) (ì„ íƒì ìœ¼ë¡œ state ì˜ì¡´ì )
    â†“
Distribution: Ï€(a|s) = N(Î¼(s), Ïƒ(s))  ë˜ëŠ”  Tanh(N(Î¼(s), Ïƒ(s)))
```

#### í•µì‹¬ ì½”ë“œ

```python
def forward(self, obs: torch.FloatTensor) -> distributions.Distribution:
    """
    ì…ë ¥: observation batch [batch_size, obs_dim]
    ì¶œë ¥: ê° observationì— ëŒ€í•œ action distribution
    """
    if self.state_dependent_std:
        # Meanê³¼ std ëª¨ë‘ stateì— ì˜ì¡´
        mean, std = torch.chunk(self.net(obs), 2, dim=-1)
        std = F.softplus(std) + 1e-2  # ì–‘ìˆ˜ ë³´ì¥
    else:
        # Meanë§Œ stateì— ì˜ì¡´
        mean = self.net(obs)
        std = F.softplus(self.std) + 1e-2  # í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°

    if self.use_tanh:
        # Actionì„ [-1, 1]ë¡œ ì••ì¶•
        return make_tanh_transformed(mean, std)
    else:
        return make_multi_normal(mean, std)
```

**ì™œ Tanh?**
- ë§ì€ environmentê°€ [-1, 1] ë²”ìœ„ì˜ actionì„ ê¸°ëŒ€
- Tanh ë³€í™˜: a = tanh(Ã£) where Ã£ ~ N(Î¼, Ïƒ)
- ê·¹ë‹¨ì ì¸ action ë°©ì§€

---

### 3. Critic Network: `gcb6206/networks/state_action_value_critic.py`

**ëª©ì **: Q(s, a)ë¥¼ ì¶”ì •í•˜ëŠ” ì‹ ê²½ë§

**í´ë˜ìŠ¤**: `StateActionCritic`

#### ì•„í‚¤í…ì²˜

```
ì…ë ¥: ì—°ê²°ëœ [state, action]
    â†“
[Linear Layer â†’ Activation] Ã— n_layers
    â†“
Output Layer (1 ê°’)
    â†“
ì¶œë ¥: Q(s, a) - ìŠ¤ì¹¼ë¼ ê°’
```

#### í•µì‹¬ ì½”ë“œ

```python
class StateActionCritic(nn.Module):
    def __init__(self, ob_dim, ac_dim, n_layers, size):
        super().__init__()
        # MLP: (obs_dim + action_dim) â†’ hidden â†’ ... â†’ 1
        self.net = ptu.build_mlp(
            input_size=ob_dim + ac_dim,
            output_size=1,
            n_layers=n_layers,
            size=size,
        )

    def forward(self, obs, acs):
        # Stateì™€ action ì—°ê²°
        input = torch.cat([obs, acs], dim=-1)
        # Q-value ì¶œë ¥
        return self.net(input).squeeze(-1)
```

**ì„¤ê³„ ì°¸ê³ ì‚¬í•­**: ëª¨ë“  actionì— ëŒ€í•œ Q(s, a)ë¥¼ ì¶œë ¥í•˜ëŠ” DQNê³¼ ë‹¬ë¦¬, ì´ê²ƒì€ íŠ¹ì • (s, a) ìŒì— ëŒ€í•œ Q-valueë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤. ì´ëŠ” continuous action spaceì— í•„ìš”í•©ë‹ˆë‹¤.

---

### 4. Replay Buffer: `gcb6206/infrastructure/replay_buffer.py`

**ëª©ì **: Off-policy í•™ìŠµì„ ìœ„í•´ ê³¼ê±° ê²½í—˜ì„ ì €ì¥í•˜ê³  ìƒ˜í”Œë§

**í´ë˜ìŠ¤**: `ReplayBuffer`

#### ì™œ Replay Buffer?

**ë¬¸ì œ**: RL ë°ì´í„°ëŠ” ë†’ì€ ìƒê´€ê´€ê³„ë¥¼ ê°€ì§
- ì—°ì†ì ì¸ stateëŠ” ìœ ì‚¬í•¨
- ê³¼ì í•©ê³¼ ë¶ˆì•ˆì •ì„±ìœ¼ë¡œ ì´ì–´ì§

**í•´ê²°ì±…**: ê²½í—˜ì„ ì €ì¥í•˜ê³  ë¬´ì‘ìœ„ë¡œ ìƒ˜í”Œë§
- ìƒê´€ê´€ê³„ ì œê±°
- ë°ì´í„° íš¨ìœ¨ì  ì¬ì‚¬ìš©
- Off-policy í•™ìŠµ ê°€ëŠ¥

#### í•µì‹¬ ì—°ì‚°

```python
class ReplayBuffer:
    def __init__(self, capacity=1000000):
        self.max_size = capacity
        self.observations = None  # ì§€ì—° í• ë‹¹
        self.actions = None
        self.rewards = None
        self.next_observations = None
        self.dones = None

    def insert(self, observation, action, reward, next_observation, done):
        """í•˜ë‚˜ì˜ transition (s, a, r, s', done) ì¶”ê°€"""
        # Circular buffer: ê°€ë“ ì°¨ë©´ ê°€ì¥ ì˜¤ë˜ëœ ê²ƒ ë®ì–´ì”€
        idx = self.size % self.max_size
        self.observations[idx] = observation
        # ... ë‹¤ë¥¸ í•„ë“œ ì €ì¥
        self.size += 1

    def sample(self, batch_size):
        """ë¬´ì‘ìœ„ transition batch ìƒ˜í”Œë§"""
        indices = np.random.randint(0, self.size, size=batch_size)
        return {
            "observations": self.observations[indices],
            "actions": self.actions[indices],
            "rewards": self.rewards[indices],
            "next_observations": self.next_observations[indices],
            "dones": self.dones[indices],
        }
```

**ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±**: Numpy ë°°ì—´ ì‚¬ìš©, í•œ ë²ˆë§Œ í• ë‹¹

---

### 5. Configuration: `gcb6206/env_configs/sac_config.py`

**ëª©ì **: ëª¨ë“  í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ í•œ ê³³ì— ì •ì˜

**í•¨ìˆ˜**: `sac_config(env_name, **kwargs)`

#### í•µì‹¬ í•˜ì´í¼íŒŒë¼ë¯¸í„°

```python
def sac_config(
    env_name: str,

    # Network ì•„í‚¤í…ì²˜
    hidden_size: int = 128,
    num_layers: int = 3,

    # Learning rate
    actor_learning_rate: float = 3e-4,
    critic_learning_rate: float = 3e-4,

    # í›ˆë ¨
    total_steps: int = 300000,
    batch_size: int = 128,
    discount: float = 0.99,

    # íƒìƒ‰
    random_steps: int = 5000,  # ì‹œì‘ ì‹œ ë¬´ì‘ìœ„ action
    training_starts: int = 10000,  # ì´í›„ í›ˆë ¨ ì‹œì‘

    # Target network
    use_soft_target_update: bool = False,
    target_update_period: int = None,  # Hard update
    soft_target_update_rate: float = None,  # Soft update Ï„

    # Actor-critic
    actor_gradient_type: str = "reinforce",  # ë˜ëŠ” "reparametrize"
    num_actor_samples: int = 1,
    num_critic_updates: int = 1,
    num_critic_networks: int = 1,  # Double-Qì˜ ê²½ìš° 2

    # Entropy
    use_entropy_bonus: bool = True,
    temperature: float = 0.1,  # entropyì— ëŒ€í•œ Î±
):
    # ëª¨ë“  configê°€ í¬í•¨ëœ dict ë°˜í™˜
```

**Configuration íŒ¨í„´**:
- Pythonì˜ ê¸°ë³¸ configuration
- YAML íŒŒì¼ë¡œ ì¬ì •ì˜ (`experiments/sac/*.yaml`)
- ì‰¬ìš´ ì‹¤í—˜ ê°€ëŠ¥

---

### 6. Training Script: `gcb6206/scripts/run_hw4.py`

**ëª©ì **: ë©”ì¸ training loop, ëª¨ë“  ê²ƒì„ ì—°ê²°

#### Training Loop êµ¬ì¡°

```python
def run_training_loop(config, logger, args):
    # 1. ì„¤ì •
    env = config["make_env"]()
    agent = SoftActorCritic(...)
    replay_buffer = ReplayBuffer(...)

    observation, _ = env.reset()

    # 2. ë©”ì¸ ë£¨í”„
    for step in range(config["total_steps"]):

        # 3. ë°ì´í„° ìˆ˜ì§‘
        if step < config["random_steps"]:
            action = env.action_space.sample()  # ë¬´ì‘ìœ„
        else:
            action = agent.get_action(observation)  # Policyì—ì„œ

        next_observation, reward, done, truncated, info = env.step(action)
        replay_buffer.insert(observation, action, reward, next_observation, done)

        # 4. Agent í›ˆë ¨
        if step >= config["training_starts"]:
            batch = replay_buffer.sample(config["batch_size"])
            batch = ptu.from_numpy(batch)  # numpy â†’ torch
            update_info = agent.update(**batch, step=step)

            # í›ˆë ¨ í†µê³„ ë¡œê¹…
            if step % args.log_interval == 0:
                for k, v in update_info.items():
                    logger.log_scalar(v, k, step)

        # 5. í‰ê°€
        if step % args.eval_interval == 0:
            eval_returns = evaluate(agent, eval_env)
            logger.log_scalar(np.mean(eval_returns), "eval_return", step)

        # 6. ì™„ë£Œ ì‹œ ë¦¬ì…‹
        if done or truncated:
            observation, _ = env.reset()
        else:
            observation = next_observation
```

**í•µì‹¬ ë‹¨ê³„:**
1. **ë¬´ì‘ìœ„ íƒìƒ‰** (0 to random_steps): ì´ˆê¸° replay buffer êµ¬ì¶•
2. **í•™ìŠµ** (training_starts ì´í›„): Network ì—…ë°ì´íŠ¸
3. **í‰ê°€** (ì£¼ê¸°ì ): íƒìƒ‰ ë…¸ì´ì¦ˆ ì—†ì´ í…ŒìŠ¤íŠ¸

---

## ë°ì´í„° íë¦„ê³¼ Training Loop

### ì™„ì „í•œ ë°ì´í„° íë¦„ ë‹¤ì´ì–´ê·¸ë¨

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         Training Loop                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â†“                                      â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Environment  â”‚                     â”‚ Replay Bufferâ”‚
â”‚              â”‚                     â”‚              â”‚
â”‚ observation  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’â”‚ (s,a,r,s',d) â”‚
â”‚   â†“          â”‚                     â”‚              â”‚
â”‚ Agent.get    â”‚                     â”‚              â”‚
â”‚   _action()  â”‚                     â”‚ Sample batch â”‚
â”‚   â†“          â”‚                     â”‚      â†“       â”‚
â”‚ action       â”‚                     â””â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚   â†“          â”‚                            â”‚
â”‚ env.step()   â”‚                            â†“
â”‚   â†“          â”‚                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚(s',r,done)   â”‚                   â”‚ Agent.update() â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚                â”‚
                                   â”‚  Update critic â”‚
                                   â”‚  Update actor  â”‚
                                   â”‚  Update target â”‚
                                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ë‹¨ê³„ë³„ ì‹¤í–‰

**Step 1: Environment ìƒí˜¸ì‘ìš©**
```python
observation = env.reset()  # ì´ˆê¸° state ê°€ì ¸ì˜¤ê¸°
# observation shape: [obs_dim]
```

**Step 2: Action ì„ íƒ**
```python
action = agent.get_action(observation)
# get_action() ë‚´ë¶€:
#   obs_torch = ptu.from_numpy(observation)[None]  # [1, obs_dim]
#   dist = self.actor(obs_torch)  # Ï€(Â·|s) ê°€ì ¸ì˜¤ê¸°
#   action = dist.sample()  # a ~ Ï€(Â·|s) ìƒ˜í”Œë§
#   return ptu.to_numpy(action).squeeze(0)  # [action_dim]
```

**Step 3: Environment Step**
```python
next_obs, reward, done, truncated, info = env.step(action)
# next_obs: [obs_dim]
# reward: ìŠ¤ì¹¼ë¼
# done: bool
```

**Step 4: Replay Bufferì— ì €ì¥**
```python
replay_buffer.insert(observation, action, reward, next_obs, done)
```

**Step 5: ìƒ˜í”Œë§ ë° í›ˆë ¨**
```python
batch = replay_buffer.sample(batch_size)
# batch = {
#   "observations": [batch_size, obs_dim],
#   "actions": [batch_size, action_dim],
#   "rewards": [batch_size],
#   "next_observations": [batch_size, obs_dim],
#   "dones": [batch_size],
# }

batch = ptu.from_numpy(batch)  # torch tensorë¡œ ë³€í™˜

update_info = agent.update(**batch, step=step)
# í›ˆë ¨ ë©”íŠ¸ë¦­ì˜ dict ë°˜í™˜
```

### `agent.update()` ë‚´ë¶€

```python
def update(self, observations, actions, rewards, next_observations, dones, step):
    # 1. Critic ì—¬ëŸ¬ ë²ˆ ì—…ë°ì´íŠ¸
    for _ in range(self.num_critic_updates):
        critic_info = self.update_critic(
            observations, actions, rewards, next_observations, dones
        )

    # 2. Actor í•œ ë²ˆ ì—…ë°ì´íŠ¸
    actor_info = self.update_actor(observations)

    # 3. Target network ì—…ë°ì´íŠ¸
    if step % self.target_update_period == 0:  # Hard update
        self.update_target_critic()
    # ë˜ëŠ”
    self.soft_update_target_critic(tau=0.005)  # Soft update

    return {**actor_info, **critic_info}
```

### `update_critic()` ë‚´ë¶€

```python
def update_critic(self, obs, action, reward, next_obs, done):
    # 1. Target ê³„ì‚° (gradient ì—†ìŒ)
    with torch.no_grad():
        next_action_dist = self.actor(next_obs)
        next_action = next_action_dist.sample()
        next_q = self.target_critic(next_obs, next_action)

        if self.use_entropy_bonus:
            entropy = self.entropy(next_action_dist)
            next_q += self.temperature * entropy

        target = reward + self.discount * (1 - done) * next_q

    # 2. Q-value ì˜ˆì¸¡
    q_values = self.critic(obs, action)

    # 3. Loss ê³„ì‚° ë° ì—…ë°ì´íŠ¸
    loss = self.critic_loss(q_values, target)  # MSE

    self.critic_optimizer.zero_grad()
    loss.backward()
    self.critic_optimizer.step()
```

### `update_actor()` ë‚´ë¶€

**REINFORCE ë²„ì „:**
```python
def actor_loss_reinforce(self, obs):
    # 1. Policy distribution ê°€ì ¸ì˜¤ê¸°
    action_dist = self.actor(obs)

    # 2. Action ìƒ˜í”Œë§ (actionì— ëŒ€í•œ gradient ì—†ìŒ)
    with torch.no_grad():
        actions = action_dist.sample((num_samples,))
        q_values = self.critic(obs, actions)

    # 3. REINFORCE gradient ê³„ì‚°
    log_probs = action_dist.log_prob(actions)
    loss = -(log_probs * q_values).mean()

    return loss
```

**REPARAMETRIZE ë²„ì „:**
```python
def actor_loss_reparametrize(self, obs):
    # 1. Policy distribution ê°€ì ¸ì˜¤ê¸°
    action_dist = self.actor(obs)

    # 2. Reparameterizationìœ¼ë¡œ ìƒ˜í”Œë§ (gradient íë¦„!)
    action = action_dist.rsample()

    # 3. Q-value ê³„ì‚° (actionì„ í†µí•œ gradient íë¦„!)
    q_values = self.critic(obs, action)

    # 4. Loss (ìµœëŒ€í™”í•˜ë¯€ë¡œ ìŒìˆ˜)
    loss = -q_values.mean()

    return loss
```

---

## í•µì‹¬ ê°œë…ê³¼ ë””ìì¸ íŒ¨í„´

### 1. ê´€ì‹¬ì‚¬ì˜ ë¶„ë¦¬

**ì‹ ê²½ë§ (networks/)**: ìˆœìˆ˜ í•¨ìˆ˜ ê·¼ì‚¬
```python
class MLPPolicy:
    def forward(self, obs):
        # obs â†’ distribution
        # RL ë¡œì§ ì—†ìŒ, ë‹¨ì§€ ì‹ ê²½ë§
```

**Agent (agents/)**: RL ì•Œê³ ë¦¬ì¦˜ ë¡œì§
```python
class SoftActorCritic:
    def update_critic(self, ...):
        # Bootstrapping, target network, loss ê³„ì‚°
        # ìˆœìˆ˜ RL ë¡œì§, NNì€ networks/ì— ìœ„ì„
```

**Infrastructure (infrastructure/)**: ì¬ì‚¬ìš© ê°€ëŠ¥í•œ ìœ í‹¸ë¦¬í‹°
```python
class ReplayBuffer:
    # ì¼ë°˜ì ì¸ experience replay
    # ëª¨ë“  off-policy ì•Œê³ ë¦¬ì¦˜ì— ì‚¬ìš© ê°€ëŠ¥
```

### 2. Configuration ê´€ë¦¬

**ê³„ì¸µ êµ¬ì¡°:**
1. **ê¸°ë³¸ ê¸°ë³¸ê°’**: `sac_config.py`ì—
2. **ì‹¤í—˜ë³„**: YAML íŒŒì¼ì—
3. **ëª…ë ¹ì¤„**: argparseë¥¼ í†µí•´

**ì˜ˆì‹œ:**
```yaml
# experiments/sac/my_experiment.yaml
base_config: sac
env_name: HalfCheetah-v4
temperature: 0.2  # ê¸°ë³¸ê°’ ì¬ì •ì˜
```

```bash
python run_hw4.py -cfg experiments/sac/my_experiment.yaml --seed 42
```

### 3. ë¡œê¹…ê³¼ ëª¨ë‹ˆí„°ë§

**TensorBoard í†µí•©:**
```python
logger.log_scalar(value, name, step)
logger.log_scalar(q_values.mean().item(), "q_values", step)
```

**ê²°ê³¼ ë³´ê¸°:**
```bash
tensorboard --logdir data/
```

### 4. ëª¨ë“ˆí™”ëœ Network ìƒì„±

**Factory íŒ¨í„´:**
```python
def sac_config(...):
    def make_actor(obs_shape, action_dim):
        return MLPPolicy(
            ob_dim=obs_shape[0],
            ac_dim=action_dim,
            n_layers=num_layers,
            layer_size=hidden_size,
        )

    return {
        "agent_kwargs": {
            "make_actor": make_actor,
            # ...
        }
    }
```

**ì¥ì :**
- ì•„í‚¤í…ì²˜ êµì²´ ìš©ì´
- ì§€ì—°ëœ êµ¬ì„±
- Configuration ìœ ì—°ì„±

### 5. PyTorch ìœ í‹¸ë¦¬í‹°

**ë””ë°”ì´ìŠ¤ ê´€ë¦¬:**
```python
# pytorch_util.py
device = None  # ì „ì—­ device

def init_gpu(use_gpu=True, gpu_id=0):
    global device
    if torch.cuda.is_available() and use_gpu:
        device = torch.device("cuda:" + str(gpu_id))
    else:
        device = torch.device("cpu")

def from_numpy(data):
    """numpy â†’ torch ë³€í™˜, deviceë¡œ ì´ë™"""
    return torch.from_numpy(data).float().to(device)

def to_numpy(tensor):
    """torch â†’ numpy ë³€í™˜"""
    return tensor.to("cpu").detach().numpy()
```

**MLP ë¹Œë”:**
```python
def build_mlp(input_size, output_size, n_layers, size, activation="tanh"):
    layers = []
    for _ in range(n_layers):
        layers += [nn.Linear(in_size, size), activation]
        in_size = size
    layers.append(nn.Linear(in_size, output_size))
    return nn.Sequential(*layers)
```

### 6. Target Network ì—…ë°ì´íŠ¸

**Hard update (ì£¼ê¸°ì  ë³µì‚¬):**
```python
def update_target_critic(self):
    """ê°€ì¤‘ì¹˜ ì™„ì „ ë³µì‚¬"""
    for target_param, param in zip(target_critic.parameters(), critic.parameters()):
        target_param.data.copy_(param.data)
```

**Soft update (exponential moving average):**
```python
def soft_update_target_critic(self, tau=0.005):
    """Polyak averaging: Î¸' â† Î¸' + Ï„(Î¸ - Î¸')"""
    for target_param, param in zip(target_critic.parameters(), critic.parameters()):
        target_param.data.copy_(
            target_param.data * (1.0 - tau) + param.data * tau
        )
```

---

## ì½”ë“œë² ì´ìŠ¤ íƒìƒ‰ ë°©ë²•

### ì‹œì‘ì : ìƒˆë¡œìš´ ê¸°ëŠ¥ ì´í•´í•˜ê¸°

**ì§ˆë¬¸: "SACëŠ” ì–´ë–»ê²Œ actionì„ ì„ íƒí•˜ë‚˜ìš”?"**

1. **ì§„ì…ì ì—ì„œ ì‹œì‘**: `scripts/run_hw4.py`
   - Action ì„ íƒ ì°¾ê¸°: `action = agent.get_action(observation)`

2. **Agentë¡œ ë”°ë¼ê°€ê¸°**: `agents/sac_agent.py`
   - `get_action()` ë©”ì„œë“œ ì°¾ê¸°
   - `self.actor(observation)` í˜¸ì¶œ í™•ì¸

3. **Networkë¡œ ë”°ë¼ê°€ê¸°**: `networks/mlp_policy.py`
   - `forward()` ë©”ì„œë“œ ì°¾ê¸°
   - Distributionì„ ë°˜í™˜í•˜ëŠ” ê²ƒ ì´í•´

4. **ìœ í‹¸ë¦¬í‹° í™•ì¸**: `infrastructure/distributions.py`
   - ì»¤ìŠ¤í…€ distribution êµ¬í˜„ í™•ì¸

### ì½”ë“œë² ì´ìŠ¤ ì½ê¸°: ì¶”ì²œ ìˆœì„œ

**SAC ì´í•´ë¥¼ ìœ„í•´:**

1. ì‹œì‘: `scripts/run_hw4.py`
   - ì „ì²´ training loop
   - ì–¸ì œ ë¬´ìŠ¨ ì¼ì´ ì¼ì–´ë‚˜ëŠ”ì§€

2. ê·¸ ë‹¤ìŒ: `agents/sac_agent.py`
   - `__init__`: ì–´ë–¤ ì»´í¬ë„ŒíŠ¸ê°€ ì¡´ì¬í•˜ëŠ”ì§€
   - `get_action`: Actionì´ ì–´ë–»ê²Œ ì„ íƒë˜ëŠ”ì§€
   - `update`: ë©”ì¸ í•™ìŠµ ë¡œì§

3. ê·¸ ë‹¤ìŒ: `agents/sac_agent.py` (ìƒì„¸)
   - `update_critic`: Valueê°€ ì–´ë–»ê²Œ í•™ìŠµë˜ëŠ”ì§€
   - `update_actor`: Policyê°€ ì–´ë–»ê²Œ í•™ìŠµë˜ëŠ”ì§€

4. ê·¸ ë‹¤ìŒ: Network ì•„í‚¤í…ì²˜
   - `networks/mlp_policy.py`: Actor
   - `networks/state_action_value_critic.py`: Critic

5. ë§ˆì§€ë§‰: Infrastructure
   - `infrastructure/replay_buffer.py`: ë°ì´í„° ì €ì¥
   - `infrastructure/utils.py`: í—¬í¼ í•¨ìˆ˜

### ë””ë²„ê¹… íŒ

**Q-valueê°€ í­ë°œ/ì†Œë©¸í•˜ë‚˜ìš”?**
- í™•ì¸: `update_critic()` - Targetì´ ì˜¬ë°”ë¥´ê²Œ ê³„ì‚°ë˜ëŠ”ê°€?
- í™•ì¸: Target network ì—…ë°ì´íŠ¸ - ë°œìƒí•˜ê³  ìˆëŠ”ê°€?
- ë¡œê·¸: Q-value, target value, critic loss

**Policyê°€ ê°œì„ ë˜ì§€ ì•Šë‚˜ìš”?**
- í™•ì¸: `update_actor()` - Lossê°€ ê°ì†Œí•˜ëŠ”ê°€?
- í™•ì¸: Entropy - ë„ˆë¬´ ë†’ê±°ë‚˜ ë‚®ì€ê°€?
- ë¡œê·¸: Actor loss, entropy, policy std

**ì „í˜€ í•™ìŠµì´ ì•ˆ ë˜ë‚˜ìš”?**
- í™•ì¸: Replay bufferì— ì¶©ë¶„í•œ ìƒ˜í”Œì´ ìˆëŠ”ê°€
- í™•ì¸: `training_starts` íŒŒë¼ë¯¸í„°
- í™•ì¸: Learning rate
- ê²€ì¦: Gradientê°€ íë¥´ëŠ”ê°€ (grad norm ì¶œë ¥)

### ì¼ë°˜ì ì¸ ìˆ˜ì • íŒ¨í„´

**Network ì•„í‚¤í…ì²˜ ë³€ê²½:**
```python
# env_configs/sac_config.pyì—ì„œ
def make_actor(obs_shape, action_dim):
    return MLPPolicy(
        ...,
        n_layers=5,  # 3ì—ì„œ ë³€ê²½
        layer_size=256,  # 128ì—ì„œ ë³€ê²½
    )
```

**ìƒˆë¡œìš´ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¶”ê°€:**
```python
# sac_config.pyì—ì„œ
def sac_config(..., my_new_param=default_value):
    return {
        "agent_kwargs": {
            ...,
            "my_new_param": my_new_param,
        }
    }

# sac_agent.pyì—ì„œ
class SoftActorCritic:
    def __init__(self, ..., my_new_param):
        self.my_new_param = my_new_param
```

**ìƒˆë¡œìš´ ë¡œê¹… ì¶”ê°€:**
```python
# sac_agent.pyì—ì„œ
def update_critic(...):
    ...
    return {
        "critic_loss": loss.item(),
        "q_values": q_values.mean().item(),
        "my_new_metric": new_value.item(),  # ì´ê²ƒ ì¶”ê°€
    }
```

---

## ê³ ê¸‰ ì£¼ì œ

### 1. Reparameterization Trick

**ë¬¸ì œ**: ìƒ˜í”Œë§ì„ í†µí•œ backpropagation ë¶ˆê°€
```python
a ~ N(Î¼(s), Ïƒ(s))  # âˆ‡_Î¸ë¥¼ ì–´ë–»ê²Œ ì–»ë‚˜?
```

**í•´ê²°ì±…**: Reparameterize
```python
Îµ ~ N(0, 1)  # í‘œì¤€ ì •ê·œë¶„í¬
a = Î¼(s) + Ïƒ(s) Â· Îµ  # ì´ì œ Î¼ì™€ Ïƒë¥¼ í†µí•´ gradient íë¦„!
```

**ì½”ë“œì—ì„œ:**
```python
# .sample(): Gradient ì—†ìŒ
action = distribution.sample()

# .rsample(): Reparameterized, gradient íë¦„
action = distribution.rsample()
```

### 2. Bounded Actionì„ ìœ„í•œ Tanh ë³€í™˜

**ë¬¸ì œ**: EnvironmentëŠ” [-1, 1]ì˜ actionì„ ê¸°ëŒ€í•˜ì§€ë§Œ, Gaussianì€ ë¬´í•œ

**í•´ê²°ì±…**: Tanhë¥¼ í†µí•´ ì••ì¶•
```python
a_unbounded ~ N(Î¼, Ïƒ)
a = tanh(a_unbounded)  # ì´ì œ a âˆˆ (-1, 1)
```

**í™•ë¥ ì— ëŒ€í•œ ë³´ì •:**
```python
log Ï€(a|s) = log Ï€(a_unbounded|s) - log|da/da_unbounded|
            = log Ï€(a_unbounded|s) - log(1 - tanhÂ²(a_unbounded))
```

### 3. ë‹¤ì¤‘ Critic Network

**ì™œ?**
- Q-learningì—ì„œ ê³¼ëŒ€í‰ê°€ í¸í–¥
- ë‹¨ì¼ criticì€ ë‚™ê´€ì ì¸ ê²½í–¥

**í•´ê²°ì±…:**
- **Double-Q**: ë‘ critic, ê°ê° ë‹¤ë¥¸ ê²ƒì„ targetìœ¼ë¡œ ì‚¬ìš©
- **Clipped Double-Q**: Targetì— min(Q1, Q2) ì‚¬ìš©
- **Mean**: ì—¬ëŸ¬ criticì˜ í‰ê· 

**êµ¬í˜„:**
```python
self.critics = nn.ModuleList([
    make_critic(...) for _ in range(num_critic_networks)
])

def critic(self, obs, action):
    # ë°˜í™˜: [num_critics, batch_size]
    return torch.stack([critic(obs, action) for critic in self.critics])
```

### 4. Entropy-Regularized RL

**ëª©í‘œ:**
```
J(Ï€) = E[âˆ‘ Î³^t (r_t + Î± H(Ï€(Â·|s_t)))]
```

**íš¨ê³¼:**
- ì¡°ê¸° ìˆ˜ë ´ ë°©ì§€
- ê°•ê±´í•œ policy í•™ìŠµ
- ìë™ íƒìƒ‰

**Temperature (Î±):**
- ë†’ì€ Î±: ë” ë¬´ì‘ìœ„ (ë” ë§ì€ íƒìƒ‰)
- ë‚®ì€ Î±: ë” ê²°ì •ì  (ë” ë§ì€ í™œìš©)
- ìë™ìœ¼ë¡œ í•™ìŠµ ê°€ëŠ¥ (ì´ ê³¼ì œì—ì„œëŠ” ì•„ë‹˜)

---

## ìš©ì–´ì§‘

**Actor**: Action distributionì„ ì¶œë ¥í•˜ëŠ” policy network Ï€_Î¸(a|s)

**Critic**: ì˜ˆìƒ returnì„ ì¶”ì •í•˜ëŠ” value network Q_Ï†(s,a)

**Bellman Equation**: ì¬ê·€ ê´€ê³„: Q(s,a) = r + Î³E[Q(s',a')]

**Bootstrapping**: í•™ìŠµ targetì—ì„œ ë¯¸ë˜ valueì˜ ì¶”ì •ì¹˜ ì‚¬ìš©

**Discount Factor (Î³)**: ë¯¸ë˜ ëŒ€ ì¦‰ê° rewardë¥¼ ì–¼ë§ˆë‚˜ ì¤‘ì‹œí• ì§€

**Entropy**: ë¬´ì‘ìœ„ì„±ì˜ ì¸¡ì •: H(Ï€) = -E[log Ï€(a|s)]

**Episode**: ì‹œì‘ stateì—ì„œ ì¢…ë£Œ stateê¹Œì§€ì˜ ì™„ì „í•œ ì‹œí€€ìŠ¤

**Off-policy**: ë‹¤ë¥¸ policyê°€ ìƒì„±í•œ ë°ì´í„°ë¡œë¶€í„° í•™ìŠµ

**On-policy**: í˜„ì¬ policyê°€ ìƒì„±í•œ ë°ì´í„°ë¡œë¶€í„° í•™ìŠµ

**Policy**: Stateì—ì„œ actionìœ¼ë¡œì˜ ë§¤í•‘: Ï€(a|s)

**Replay Buffer**: ê³¼ê±° ê²½í—˜ì„ ìœ„í•œ ì €ì¥ì†Œ (s, a, r, s', done)

**Reparameterization**: Gradient íë¦„ì„ ìœ„í•œ a = Î¼ + ÏƒÂ·Îµ ê¸°ë²•

**Return**: Discounted rewardì˜ í•©: G_t = âˆ‘_{k=0}^âˆ Î³^k r_{t+k}

**Reward**: Environmentë¡œë¶€í„°ì˜ ìŠ¤ì¹¼ë¼ í”¼ë“œë°± ì‹ í˜¸

**State**: ì‹œê°„ tì—ì„œ environmentì˜ ì™„ì „í•œ ì„¤ëª…

**Target Network**: ì•ˆì •ì„±ì„ ìœ„í•´ ì²œì²œíˆ ì—…ë°ì´íŠ¸ë˜ëŠ” value networkì˜ ë³µì‚¬ë³¸

**Temperature (Î±)**: SACì—ì„œ entropy bonusì˜ ê°€ì¤‘ì¹˜

**Trajectory/Rollout**: (s, a, r) íŠœí”Œì˜ ì‹œí€€ìŠ¤

**Value Function**: ì˜ˆìƒ return: V(s) = E[G_t | s_t = s]

---

## ë‹¤ìŒ ë‹¨ê³„

### ì´í•´ë¥¼ ê¹Šê²Œ í•˜ë ¤ë©´:

1. **SAC ë…¼ë¬¸ ì½ê¸°**: https://arxiv.org/abs/1801.01290
2. **ë³€í˜• êµ¬í˜„í•˜ê¸°**: ë‹¤ì–‘í•œ ì•„í‚¤í…ì²˜, í•˜ì´í¼íŒŒë¼ë¯¸í„° ì‹œë„
3. **ì‹¤í—˜í•˜ê¸°**: ìƒˆë¡œìš´ environment ì‹œë„, í•™ìŠµëœ policy ì‹œê°í™”
4. **ì•Œê³ ë¦¬ì¦˜ ë¹„êµ**: TD3, PPO êµ¬í˜„í•˜ê³  ë¹„êµ

### ìë£Œ:

- **OpenAI Spinning Up**: https://spinningup.openai.com/
- **Berkeley CS 285**: http://rail.eecs.berkeley.edu/deeprlcourse/
- **PyTorch íŠœí† ë¦¬ì–¼**: https://pytorch.org/tutorials/
- **Gymnasium ë¬¸ì„œ**: https://gymnasium.farama.org/

---

## ìš”ì•½

ì´ ì½”ë“œë² ì´ìŠ¤ëŠ” ì™„ì „í•˜ê³  í”„ë¡œë•ì…˜ ìˆ˜ì¤€ì˜ SAC agentë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤. í•µì‹¬ ìš”ì :

1. **ëª¨ë“ˆí™” ì„¤ê³„**: ê´€ì‹¬ì‚¬ ë¶„ë¦¬ (ì•Œê³ ë¦¬ì¦˜, network, infrastructure)
2. **Configuration ì£¼ë„**: í•˜ì´í¼íŒŒë¼ë¯¸í„° ì‹¤í—˜ ìš©ì´
3. **ëª¨ë²” ì‚¬ë¡€**: Target network, replay buffer, entropy regularization
4. **í™•ì¥ ê°€ëŠ¥**: ìˆ˜ì • ë° í™•ì¥ ìš©ì´

ì´ ì½”ë“œë² ì´ìŠ¤ë¥¼ ì´í•´í•˜ë©´ ë‹¤ìŒì„ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤:
- **ì‹¤ìš©ì ì¸ RL êµ¬í˜„ ê¸°ìˆ **
- **ë”¥ëŸ¬ë‹ ì—”ì§€ë‹ˆì–´ë§ íŒ¨í„´**
- **ì—°êµ¬ ë° í”„ë¡œë•ì…˜ RLì„ ìœ„í•œ ê¸°ì´ˆ**

ì¦ê±°ìš´ í•™ìŠµ ë˜ì„¸ìš”! ğŸš€
